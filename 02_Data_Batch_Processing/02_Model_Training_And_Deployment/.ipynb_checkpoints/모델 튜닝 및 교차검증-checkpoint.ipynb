{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# airflow를 이용해서 테스트 진행\n",
    "\n",
    "#!airflow initdb\n",
    "!airflow test DAG_model_training emr_cluster_create 2019-08-01\n",
    "#!airflow test DAG_model_training emr_ssh_connect 2019-08-01\n",
    "#!airflow test DAG_model_training ip_forwarding 2019-08-01\n",
    "#!airflow test DAG_model_training traindata_s3_to_hdfs 2019-08-01\n",
    "#!airflow test DAG_model_training training_model 2019-08-01\n",
    "#!airflow test DAG_model_training trained_model_hdfs_to_s3 2019-08-01\n",
    "#!airflow test DAG_model_training trained_model_s3_to_server 2019-08-01\n",
    "#!airflow test DAG_model_training emr_cluster_close 2019-08-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/project1/hadoop/etc/hadoop-model\r\n"
     ]
    }
   ],
   "source": [
    "# 환경변수 변경(쥬피터 실행하는 콘솔에 복붙) \n",
    "# !! 쥬피터 내에서 실행하는 것은 새로고침하면 오류뜬다.\n",
    "export HADOOP_CONF_DIR='/home/ubuntu/project1/hadoop/etc/hadoop-model'\n",
    "export YARN_CONF_DIR='/home/ubuntu/project1/hadoop/etc/hadoop-model'\n",
    "\n",
    "!echo $HADOOP_CONF_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys, os, re, json, datetime, iso8601, timeit\n",
    "from tabulate import tabulate\n",
    "\n",
    "import pyspark\n",
    "import pyspark.sql\n",
    "\n",
    "# EMR 클러스터 사용할 때 설정\n",
    "#conf = pyspark.SparkConf().setAll([('spark.master', 'yarn'),(\"spark.driver.memory\", \"2g\"),('spark.executor.memory', '18g'), ('spark.executor.cores', '4'), ('spark.executor.instances', '4'),('spark.submit.deployMode','client')])\n",
    "\n",
    "# EMR 클러스터 사용 안 할때 설정\n",
    "conf = pyspark.SparkConf().setAll([('spark.driver.memory', '2g')])\n",
    "\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = pyspark.sql.SparkSession(sc).builder.getOrCreate()\n",
    "    \n",
    "# 로컬로 실행한다면    \n",
    "PROJECT_PATH = '../../02_Data_Batch_Processing'\n",
    "\n",
    "# emr에서 실행한다면\n",
    "#PROJECT_PATH = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.executor.instances', '4'),\n",
       " ('spark.repl.local.jars',\n",
       "  'file:///home/ubuntu/project1/lib/mongo-hadoop-spark-2.0.2.jar,file:///home/ubuntu/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.0.jar,file:///home/ubuntu/.ivy2/jars/org.apache.spark_spark-streaming-kinesis-asl_2.11-2.4.0.jar,file:///home/ubuntu/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar,file:///home/ubuntu/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar,file:///home/ubuntu/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar,file:///home/ubuntu/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar,file:///home/ubuntu/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar,file:///home/ubuntu/.ivy2/jars/com.101tec_zkclient-0.3.jar,file:///home/ubuntu/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar,file:///home/ubuntu/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar,file:///home/ubuntu/.ivy2/jars/log4j_log4j-1.2.17.jar,file:///home/ubuntu/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar,file:///home/ubuntu/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.1.jar,file:///home/ubuntu/.ivy2/jars/com.amazonaws_amazon-kinesis-client-1.8.10.jar,file:///home/ubuntu/.ivy2/jars/com.amazonaws_aws-java-sdk-sts-1.11.271.jar,file:///home/ubuntu/.ivy2/jars/com.amazonaws_aws-java-sdk-dynamodb-1.11.271.jar,file:///home/ubuntu/.ivy2/jars/com.amazonaws_aws-java-sdk-kinesis-1.11.271.jar,file:///home/ubuntu/.ivy2/jars/com.amazonaws_aws-java-sdk-cloudwatch-1.11.271.jar,file:///home/ubuntu/.ivy2/jars/com.google.protobuf_protobuf-java-2.5.0.jar,file:///home/ubuntu/.ivy2/jars/commons-lang_commons-lang-2.6.jar,file:///home/ubuntu/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar,file:///home/ubuntu/.ivy2/jars/com.amazonaws_aws-java-sdk-s3-1.11.271.jar,file:///home/ubuntu/.ivy2/jars/com.amazonaws_aws-java-sdk-core-1.11.271.jar,file:///home/ubuntu/.ivy2/jars/com.amazonaws_jmespath-java-1.11.271.jar,file:///home/ubuntu/.ivy2/jars/com.amazonaws_aws-java-sdk-kms-1.11.271.jar,file:///home/ubuntu/.ivy2/jars/org.apache.httpcomponents_httpclient-4.5.6.jar,file:///home/ubuntu/.ivy2/jars/software.amazon.ion_ion-java-1.0.2.jar,file:///home/ubuntu/.ivy2/jars/com.fasterxml.jackson.core_jackson-databind-2.6.7.1.jar,file:///home/ubuntu/.ivy2/jars/com.fasterxml.jackson.dataformat_jackson-dataformat-cbor-2.6.7.jar,file:///home/ubuntu/.ivy2/jars/joda-time_joda-time-2.9.3.jar,file:///home/ubuntu/.ivy2/jars/org.apache.httpcomponents_httpcore-4.4.10.jar,file:///home/ubuntu/.ivy2/jars/commons-codec_commons-codec-1.10.jar,file:///home/ubuntu/.ivy2/jars/com.fasterxml.jackson.core_jackson-annotations-2.6.7.jar,file:///home/ubuntu/.ivy2/jars/com.fasterxml.jackson.core_jackson-core-2.6.7.jar'),\n",
       " ('spark.executor.cores', '5'),\n",
       " ('spark.io.compression.codec', 'org.apache.spark.io.SnappyCompressionCodec'),\n",
       " ('spark.ui.proxyBase', '/proxy/application_1564014105570_0009'),\n",
       " ('spark.submit.pyFiles',\n",
       "  '/home/ubuntu/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.0.jar,/home/ubuntu/.ivy2/jars/org.apache.spark_spark-streaming-kinesis-asl_2.11-2.4.0.jar,/home/ubuntu/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar,/home/ubuntu/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar,/home/ubuntu/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar,/home/ubuntu/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar,/home/ubuntu/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar,/home/ubuntu/.ivy2/jars/com.101tec_zkclient-0.3.jar,/home/ubuntu/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar,/home/ubuntu/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar,/home/ubuntu/.ivy2/jars/log4j_log4j-1.2.17.jar,/home/ubuntu/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar,/home/ubuntu/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.1.jar,/home/ubuntu/.ivy2/jars/com.amazonaws_amazon-kinesis-client-1.8.10.jar,/home/ubuntu/.ivy2/jars/com.amazonaws_aws-java-sdk-sts-1.11.271.jar,/home/ubuntu/.ivy2/jars/com.amazonaws_aws-java-sdk-dynamodb-1.11.271.jar,/home/ubuntu/.ivy2/jars/com.amazonaws_aws-java-sdk-kinesis-1.11.271.jar,/home/ubuntu/.ivy2/jars/com.amazonaws_aws-java-sdk-cloudwatch-1.11.271.jar,/home/ubuntu/.ivy2/jars/com.google.protobuf_protobuf-java-2.5.0.jar,/home/ubuntu/.ivy2/jars/commons-lang_commons-lang-2.6.jar,/home/ubuntu/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar,/home/ubuntu/.ivy2/jars/com.amazonaws_aws-java-sdk-s3-1.11.271.jar,/home/ubuntu/.ivy2/jars/com.amazonaws_aws-java-sdk-core-1.11.271.jar,/home/ubuntu/.ivy2/jars/com.amazonaws_jmespath-java-1.11.271.jar,/home/ubuntu/.ivy2/jars/com.amazonaws_aws-java-sdk-kms-1.11.271.jar,/home/ubuntu/.ivy2/jars/org.apache.httpcomponents_httpclient-4.5.6.jar,/home/ubuntu/.ivy2/jars/software.amazon.ion_ion-java-1.0.2.jar,/home/ubuntu/.ivy2/jars/com.fasterxml.jackson.core_jackson-databind-2.6.7.1.jar,/home/ubuntu/.ivy2/jars/com.fasterxml.jackson.dataformat_jackson-dataformat-cbor-2.6.7.jar,/home/ubuntu/.ivy2/jars/joda-time_joda-time-2.9.3.jar,/home/ubuntu/.ivy2/jars/org.apache.httpcomponents_httpcore-4.4.10.jar,/home/ubuntu/.ivy2/jars/commons-codec_commons-codec-1.10.jar,/home/ubuntu/.ivy2/jars/com.fasterxml.jackson.core_jackson-annotations-2.6.7.jar,/home/ubuntu/.ivy2/jars/com.fasterxml.jackson.core_jackson-core-2.6.7.jar'),\n",
       " ('spark.yarn.dist.pyFiles',\n",
       "  'file:///home/ubuntu/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.0.jar,file:///home/ubuntu/.ivy2/jars/org.apache.spark_spark-streaming-kinesis-asl_2.11-2.4.0.jar,file:///home/ubuntu/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar,file:///home/ubuntu/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar,file:///home/ubuntu/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar,file:///home/ubuntu/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar,file:///home/ubuntu/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar,file:///home/ubuntu/.ivy2/jars/com.101tec_zkclient-0.3.jar,file:///home/ubuntu/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar,file:///home/ubuntu/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar,file:///home/ubuntu/.ivy2/jars/log4j_log4j-1.2.17.jar,file:///home/ubuntu/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar,file:///home/ubuntu/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.1.jar,file:///home/ubuntu/.ivy2/jars/com.amazonaws_amazon-kinesis-client-1.8.10.jar,file:///home/ubuntu/.ivy2/jars/com.amazonaws_aws-java-sdk-sts-1.11.271.jar,file:///home/ubuntu/.ivy2/jars/com.amazonaws_aws-java-sdk-dynamodb-1.11.271.jar,file:///home/ubuntu/.ivy2/jars/com.amazonaws_aws-java-sdk-kinesis-1.11.271.jar,file:///home/ubuntu/.ivy2/jars/com.amazonaws_aws-java-sdk-cloudwatch-1.11.271.jar,file:///home/ubuntu/.ivy2/jars/com.google.protobuf_protobuf-java-2.5.0.jar,file:///home/ubuntu/.ivy2/jars/commons-lang_commons-lang-2.6.jar,file:///home/ubuntu/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar,file:///home/ubuntu/.ivy2/jars/com.amazonaws_aws-java-sdk-s3-1.11.271.jar,file:///home/ubuntu/.ivy2/jars/com.amazonaws_aws-java-sdk-core-1.11.271.jar,file:///home/ubuntu/.ivy2/jars/com.amazonaws_jmespath-java-1.11.271.jar,file:///home/ubuntu/.ivy2/jars/com.amazonaws_aws-java-sdk-kms-1.11.271.jar,file:///home/ubuntu/.ivy2/jars/org.apache.httpcomponents_httpclient-4.5.6.jar,file:///home/ubuntu/.ivy2/jars/software.amazon.ion_ion-java-1.0.2.jar,file:///home/ubuntu/.ivy2/jars/com.fasterxml.jackson.core_jackson-databind-2.6.7.1.jar,file:///home/ubuntu/.ivy2/jars/com.fasterxml.jackson.dataformat_jackson-dataformat-cbor-2.6.7.jar,file:///home/ubuntu/.ivy2/jars/joda-time_joda-time-2.9.3.jar,file:///home/ubuntu/.ivy2/jars/org.apache.httpcomponents_httpcore-4.4.10.jar,file:///home/ubuntu/.ivy2/jars/commons-codec_commons-codec-1.10.jar,file:///home/ubuntu/.ivy2/jars/com.fasterxml.jackson.core_jackson-annotations-2.6.7.jar,file:///home/ubuntu/.ivy2/jars/com.fasterxml.jackson.core_jackson-core-2.6.7.jar'),\n",
       " ('spark.driver.memory', '1g'),\n",
       " ('spark.app.name', 'pyspark-shell'),\n",
       " ('spark.jars.packages',\n",
       "  'org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.0,org.apache.spark:spark-streaming-kinesis-asl_2.11:2.4.0'),\n",
       " ('spark.master', 'yarn'),\n",
       " ('spark.jars', 'file:/home/ubuntu/project1/lib/mongo-hadoop-spark-2.0.2.jar'),\n",
       " ('spark.executor.memory', '18g'),\n",
       " ('spark.yarn.isPython', 'true'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.yarn.dist.jars',\n",
       "  'file:///home/ubuntu/project1/lib/mongo-hadoop-spark-2.0.2.jar,file:///home/ubuntu/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.0.jar,file:///home/ubuntu/.ivy2/jars/org.apache.spark_spark-streaming-kinesis-asl_2.11-2.4.0.jar,file:///home/ubuntu/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar,file:///home/ubuntu/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar,file:///home/ubuntu/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar,file:///home/ubuntu/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar,file:///home/ubuntu/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar,file:///home/ubuntu/.ivy2/jars/com.101tec_zkclient-0.3.jar,file:///home/ubuntu/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar,file:///home/ubuntu/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar,file:///home/ubuntu/.ivy2/jars/log4j_log4j-1.2.17.jar,file:///home/ubuntu/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar,file:///home/ubuntu/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.1.jar,file:///home/ubuntu/.ivy2/jars/com.amazonaws_amazon-kinesis-client-1.8.10.jar,file:///home/ubuntu/.ivy2/jars/com.amazonaws_aws-java-sdk-sts-1.11.271.jar,file:///home/ubuntu/.ivy2/jars/com.amazonaws_aws-java-sdk-dynamodb-1.11.271.jar,file:///home/ubuntu/.ivy2/jars/com.amazonaws_aws-java-sdk-kinesis-1.11.271.jar,file:///home/ubuntu/.ivy2/jars/com.amazonaws_aws-java-sdk-cloudwatch-1.11.271.jar,file:///home/ubuntu/.ivy2/jars/com.google.protobuf_protobuf-java-2.5.0.jar,file:///home/ubuntu/.ivy2/jars/commons-lang_commons-lang-2.6.jar,file:///home/ubuntu/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar,file:///home/ubuntu/.ivy2/jars/com.amazonaws_aws-java-sdk-s3-1.11.271.jar,file:///home/ubuntu/.ivy2/jars/com.amazonaws_aws-java-sdk-core-1.11.271.jar,file:///home/ubuntu/.ivy2/jars/com.amazonaws_jmespath-java-1.11.271.jar,file:///home/ubuntu/.ivy2/jars/com.amazonaws_aws-java-sdk-kms-1.11.271.jar,file:///home/ubuntu/.ivy2/jars/org.apache.httpcomponents_httpclient-4.5.6.jar,file:///home/ubuntu/.ivy2/jars/software.amazon.ion_ion-java-1.0.2.jar,file:///home/ubuntu/.ivy2/jars/com.fasterxml.jackson.core_jackson-databind-2.6.7.1.jar,file:///home/ubuntu/.ivy2/jars/com.fasterxml.jackson.dataformat_jackson-dataformat-cbor-2.6.7.jar,file:///home/ubuntu/.ivy2/jars/joda-time_joda-time-2.9.3.jar,file:///home/ubuntu/.ivy2/jars/org.apache.httpcomponents_httpcore-4.4.10.jar,file:///home/ubuntu/.ivy2/jars/commons-codec_commons-codec-1.10.jar,file:///home/ubuntu/.ivy2/jars/com.fasterxml.jackson.core_jackson-annotations-2.6.7.jar,file:///home/ubuntu/.ivy2/jars/com.fasterxml.jackson.core_jackson-core-2.6.7.jar')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pyspark 설정 보기\n",
    "pyspark.SparkConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Year='2015', Quarter='2', Month='5', DayofMonth='9', DayOfWeek='6', FlightDate='2015-05-09', Carrier='WN', TailNum='N8311Q', FlightNum='1839', Origin='MCO', OriginCityName='Orlando, FL', OriginState='FL', Dest='SJU', DestCityName='San Juan, PR', DestState='PR', DepTime='1118', DepDelay=-2.0, DepDelayMinutes=0, TaxiOut=8.0, TaxiIn=9.0, WheelsOff='1126', WheelsOn='1352', ArrTime='1401', ArrDelay=-4.0, ArrDelayMinutes=0.0, Cancelled=0, Diverted=0, ActualElapsedTime=163.0, AirTime=146.0, Flights=1, Distance=1189.0, CarrierDelay=None, WeatherDelay=None, NASDelay=None, SecurityDelay=None, LateAircraftDelay=None, CRSDepTime='1120', CRSArrTime='1405')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType, IntegerType, FloatType, DoubleType, DateType, TimestampType\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "SEED = 2019\n",
    "\n",
    "# 원본 데이터(널값 제거 안함)\n",
    "input_path = \"{}/data/Raw_Data.parquet\".format(PROJECT_PATH)\n",
    "\n",
    "# 컬럼 필터한 데이터(널값 제거) \n",
    "#input_path = \"{}/data/Refined_Data.parquet\".format(PROJECT_PATH)\n",
    "\n",
    "data_set = spark.read.parquet(input_path)\n",
    "\n",
    "# 데이터 크기를 줄이기 위한 샘플 채취 \n",
    "data_set = data_set.sample(False, 0.1)\n",
    "\n",
    "data_set.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "582692"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 개수 확인\n",
    "data_set.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# null 값 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('TailNum', 1455), ('DepTime', 8684), ('DepDelay', 8684), ('DepDelayMinutes', 8684), ('TaxiOut', 8954), ('TaxiIn', 9294), ('WheelsOff', 8954), ('WheelsOn', 9294), ('ArrTime', 9294), ('ArrDelay', 10602), ('ArrDelayMinutes', 10602), ('ActualElapsedTime', 10602), ('AirTime', 10602), ('CarrierDelay', 476032), ('WeatherDelay', 476032), ('NASDelay', 476032), ('SecurityDelay', 476032), ('LateAircraftDelay', 476032)]\n",
      "지우기 전: 582692\n",
      "지운 후: 106660\n"
     ]
    }
   ],
   "source": [
    "# 널값 검사\n",
    "null_counts = [(column, data_set.where(data_set[column].isNull()).count()) for column in data_set.columns]\n",
    "cols_with_nulls = list(filter(lambda x: x[1] > 0, null_counts))\n",
    "print(cols_with_nulls)\n",
    "\n",
    "# 넓값 지우기 전\n",
    "print(\"지우기 전:\",data_set.count())\n",
    "\n",
    "# 만약 널값이 있다면 제거 한다.\n",
    "if list(cols_with_nulls):    \n",
    "    data_set = data_set.na.drop()\n",
    "\n",
    "print(\"지운 후:\",data_set.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 훈련 데이터 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-------------------+-------+----------+---------+---------+--------+----+--------+-------------------+---------+------+\n",
      "|ArrDelay|         CRSArrTime|         CRSDepTime|Carrier|DayOfMonth|DayOfWeek|DayOfYear|DepDelay|Dest|Distance|         FlightDate|FlightNum|Origin|\n",
      "+--------+-------------------+-------------------+-------+----------+---------+---------+--------+----+--------+-------------------+---------+------+\n",
      "|    72.0|2015-05-09 13:45:00|2015-05-09 11:55:00|     WN|         9|        6|      129|    79.0| MDW|  1105.0|2015-05-09 00:00:00|      973|   RSW|\n",
      "|    45.0|2015-05-09 19:35:00|2015-05-09 17:40:00|     WN|         9|        6|      129|     9.0| DEN|  1506.0|2015-05-09 00:00:00|      962|   TPA|\n",
      "+--------+-------------------+-------------------+-------+----------+---------+---------+--------+----+--------+-------------------+---------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 테이블 등록\n",
    "data_set.registerTempTable(\"data_set\")\n",
    "\n",
    "# 모델 훈련에 쓰일 데이터 생성\n",
    "training_yet_data = spark.sql(\"\"\"\n",
    "SELECT\n",
    "  FlightNum,\n",
    "  FlightDate,\n",
    "  DayOfWeek,\n",
    "  DayofMonth AS DayOfMonth,\n",
    "  CONCAT(Month, '-',  DayofMonth) AS DayOfYear,\n",
    "  Carrier,\n",
    "  Origin,\n",
    "  Dest,\n",
    "  Distance,\n",
    "  DepDelay,\n",
    "  ArrDelay,\n",
    "FROM data_set\n",
    "\"\"\")\n",
    "\n",
    "sys.path.append(\"../../02_Data_Batch_Processing/lib\")\n",
    "import date_util\n",
    "\n",
    "# SparkContext에 모듈 등록\n",
    "sc.addPyFile('../../02_Data_Batch_Processing/lib/date_util.py')\n",
    "\n",
    "# date_util.alter_feature_datetimes : 날짜 파싱\n",
    "training_yet_data=training_yet_data.rdd.map(date_util.alter_feature_datetimes).toDF()\n",
    "training_yet_data.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 훈련 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 항공편 번호를 운항 경로로 대체하기\n",
    "from pyspark.sql.functions import lit, concat\n",
    "\n",
    "features_with_route = training_yet_data.withColumn(\n",
    "  'Route',\n",
    "  concat(\n",
    "    training_yet_data.Origin,\n",
    "    lit('-'),\n",
    "    training_yet_data.Dest\n",
    "  )\n",
    ")\n",
    "\n",
    "#### Bucketizer:목표변수 분류 클래스 나누기 ####\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "\n",
    "splits = [-float(\"inf\"), -15.0, 0, 30.0, float(\"inf\")]\n",
    "bucketizer = Bucketizer(\n",
    "  splits=splits,\n",
    "  inputCol=\"ArrDelay\", #원시 목표변수\n",
    "  outputCol=\"ArrDelayBucket\" #클래스 나뉜 목표변수\n",
    ")\n",
    "\n",
    "# Bucketizer로 데이터 변환\n",
    "ml_bucketized_features = bucketizer.transform(features_with_route)\n",
    "\n",
    "#### StringIndexer : String 타입의 범주 값을 해당 값의 정수 번호로 변환 ####\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "for column in [\"Carrier\", \"Origin\", \"Dest\", \"Route\"]:\n",
    "    string_indexer = StringIndexer(\n",
    "    inputCol=column,\n",
    "    outputCol=column + \"_index\"\n",
    "    )\n",
    "    string_indexer_model = string_indexer.fit(ml_bucketized_features)\n",
    "    ml_bucketized_features = string_indexer_model.transform(ml_bucketized_features)\n",
    "\n",
    "    ml_bucketized_features = ml_bucketized_features.drop(column)\n",
    "\n",
    "\n",
    "#### VectorAssembler: 데이터를 벡터화 하기 ####\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "numeric_columns = [\"DepDelay\", \"Distance\",\n",
    "    \"DayOfMonth\", \"DayOfWeek\",\n",
    "    \"DayOfYear\"]\n",
    "index_columns = [\"Carrier_index\", \"Origin_index\",\n",
    "                   \"Dest_index\", \"Route_index\"]\n",
    "vector_assembler = VectorAssembler(\n",
    "  inputCols=numeric_columns + index_columns,\n",
    "  outputCol=\"Features_vec\"\n",
    ")\n",
    "final_vectorized_features = vector_assembler.transform(ml_bucketized_features)\n",
    "\n",
    "# 필요없는 컬럼 제거\n",
    "for column in index_columns:\n",
    "    final_vectorized_features = final_vectorized_features.drop(column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 훈련하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하이퍼 파라미터 수정 x = 약 59%\n",
    "\n",
    "\n",
    "수정(깊이 10, 개수 5) = 약 61% / 걸린시간 약 7분\n",
    "\n",
    "EMR(m1,d2)+수정(깊이 20, 개수 10) = 애러 / 걸린시간 2.7시간 / 이유 : 메모리 부족\n",
    "\n",
    "EMR(m1,d2)+수정(깊이 10, 개수 5) = 약 61% / 걸린시간 약 4분 \n",
    "\n",
    "EMR(m1,d2)+수정(깊이 15, 개수 8) = 약 63% / 걸린시간 약 26분\n",
    "\n",
    "EMR(m1,d2)+수정(깊이 15, 개수 20) = 약 63% / 걸린시간 약 63분\n",
    "\n",
    "EMR(m1,d2)+수정(깊이 15, 개수 20)+원본의10% = 약 62% / 걸린시간 약 22분\n",
    "\n",
    "EMR(m1,d2,j2)+수정(깊이 15, 개수 20)+원본의10% = 약 62% / 걸린시간 약 17분\n",
    "\n",
    "EMR(m1,d2,j2)+수정(깊이 18, 개수 20)+원본의10% = 약 59% / 걸린시간 약 39분 !깊이 올렸지만 오히려 떨어짐\n",
    "\n",
    "EMR(m1,d2,j2)+수정(깊이 20, 개수 5)+원본의10% + 전체실행자수 4->5 = 약 59% / 걸린시간 약 16분\n",
    "\n",
    "EMR(m1,d2,j2)+수정(깊이 20, 개수 10)+원본의10% + 전체실행자수 4->5 = 약 61% / 걸린시간 약 31분\n",
    "\n",
    "EMR(m1,d2,j2)+수정(깊이 30, 개수 5)+원본의10% + 전체실행자수 4->5 = 약 57% / 걸린시간 약 40분\n",
    "\n",
    "EMR(m1,d2,j2)+수정(깊이 10, 개수 5)+원본의10% + 전체실행자수 4->5 = 약 60% / 걸린시간 약 1분\n",
    "\n",
    "EMR(m1,d2,j2)+수정(깊이 10, 개수 5)+원본의100% + 전체실행자수 4->5 = 약 60% / 걸린시간 약 1분\n",
    "\n",
    "EMR(m1,d2,j2)+수정(깊이 10, 개수 5)+원본의100% + 전체실행자수 4->5 = 약 61% / 걸린시간 약 4분\n",
    "\n",
    "EMR(m1,d2,j2)+수정(깊이 10, 개수 5)+원본의100% + 전체실행자수 4->5 = 약 61% / 걸린시간 약 3분\n",
    "\n",
    "1. 클러스터 추가 할 수록 속도는 빨라짐.(다만 처리해야하는 양이 많이 않을 때는 비슷 함. 각 노드를 확인해 보니 양이 적을 때는 놀고 있는 노드가 있었음(cpu %를 보고 판단)\n",
    "\n",
    "2. 깊이를 늘리면 실행시간은 '제곱'으로 느려지는 것 같음 ( 깊이 15 -> 18로 늘렸을 때, 실행시간은 약 2배 늘어남 )\n",
    "\n",
    "3. 개수를 늘리면 실행시간은 '배수'로  느려지는 것 같음 (예 개수 5 -> 10으로 2배 늘렸을 때 실행시간도 약 2배 늘어남)\n",
    "\n",
    "4. 일정 수준 데이터 수가 충족되면(아니면 비슷한 데이터가 많아서 거품낀 데이터 일 수도) 모델 성능에 크게 영향을 안줌 (예 원본의 100%와 10%로 훈련했을 때 정확도는 크게 차이없음.)\n",
    "\n",
    "5. 깊이는 높아질 수록 '과적합'되는 것같음 (깊이 10, 개수 5일때(정확도 61와 깊이 30, 개수 5 일때 비교 했을 때 '전자'가 정확도가 높았음)\n",
    "\n",
    "6. 개수는 일정수준에 도달하면 더 이상 성능이 증가하지 않음(8개에서 20개로 늘렸는데 정확도는 거이 차이가 없었음.)\n",
    "\n",
    "7. 실행자수 4에서 5로 늘리고 노드도 추가했지만  걸린시간은 비슷했음.. ( 1번과 같은 이유 인듯)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ArrDelay: double, CRSArrTime: timestamp, CRSDepTime: timestamp, DayOfMonth: bigint, DayOfWeek: bigint, DayOfYear: bigint, DepDelay: double, Distance: double, FlightDate: timestamp, FlightNum: string, ArrDelayBucket: double, Features_vec: vector]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 훈련(8:2)\n",
    "training_data, test_data = final_vectorized_features.randomSplit([0.8, 0.2],seed=SEED)\n",
    "training_data.cache()\n",
    "test_data.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 : 랜덤포레스트\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(\n",
    "  featuresCol=\"Features_vec\",\n",
    "  labelCol=\"ArrDelayBucket\",\n",
    "  numTrees=10,\n",
    "  maxBins=4657,\n",
    "  maxDepth=15,\n",
    "  maxMemoryInMB=1048,\n",
    "  seed = SEED,\n",
    "  #checkpointInterval = 10,\n",
    "  cacheNodeIds = True,\n",
    "  featureSubsetStrategy=\"auto\"\n",
    "    \n",
    ")\n",
    "# cache_node_ids = TRUE\n",
    "# checkpoint_interval = 10\n",
    "# featureSubsetStrategy=\"auto\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5918552185500023\n"
     ]
    }
   ],
   "source": [
    "# 훈련시작(시간 체크)\n",
    "start = timeit.default_timer()\n",
    "\n",
    "model = rfc.fit(training_data)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print((stop - start)/60) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-------------------+----------+---------+---------+--------+--------+-------------------+---------+--------------+--------------------+--------------------+--------------------+----------+\n",
      "|ArrDelay|         CRSArrTime|         CRSDepTime|DayOfMonth|DayOfWeek|DayOfYear|DepDelay|Distance|         FlightDate|FlightNum|ArrDelayBucket|        Features_vec|       rawPrediction|         probability|prediction|\n",
      "+--------+-------------------+-------------------+----------+---------+---------+--------+--------+-------------------+---------+--------------+--------------------+--------------------+--------------------+----------+\n",
      "|   -82.0|2015-01-21 20:49:00|2015-01-21 17:20:00|        21|        3|       21|   -15.0|  2402.0|2015-01-21 00:00:00|       11|           0.0|[-15.0,2402.0,21....|[2.65697552471158...|[0.53139510494231...|       0.0|\n",
      "|   -80.0|2015-11-17 19:57:00|2015-11-17 17:00:00|        17|        2|      321|   -13.0|  2248.0|2015-11-17 00:00:00|      211|           0.0|[-13.0,2248.0,17....|[2.55063906984087...|[0.51012781396817...|       0.0|\n",
      "|   -78.0|2015-03-09 13:42:00|2015-03-09 09:00:00|         9|        1|       68|    11.0|  2704.0|2015-03-09 00:00:00|      351|           0.0|[11.0,2704.0,9.0,...|[1.03961354656218...|[0.20792270931243...|       2.0|\n",
      "|   -76.0|2016-01-01 00:16:00|2015-12-31 18:05:00|        31|        4|      365|    -5.0|  1576.0|2015-12-31 00:00:00|      741|           0.0|[-5.0,1576.0,31.0...|[2.00980548704963...|[0.40196109740992...|       0.0|\n",
      "|   -74.0|2015-03-15 12:57:00|2015-03-15 09:35:00|        15|        7|       74|    -2.0|  2475.0|2015-03-15 00:00:00|      223|           0.0|[-2.0,2475.0,15.0...|[2.31155388304779...|[0.46231077660955...|       0.0|\n",
      "+--------+-------------------+-------------------+----------+---------+---------+--------+--------+-------------------+---------+--------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model using test data\n",
    "predictions = model.transform(test_data)\n",
    "predictions.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.6105440467272318\n"
     ]
    }
   ],
   "source": [
    "# 훈련된 모델에 테스트 데이터로 검증하기\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# 정확도 점수\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\",labelCol=\"ArrDelayBucket\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy = {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 = 0.6100615729464689\n"
     ]
    }
   ],
   "source": [
    "# f1점수\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\",labelCol=\"ArrDelayBucket\", metricName=\"f1\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"f1 = {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 교차 검증\n",
    "\n",
    "데이터 전처리하고 난 뒤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ArrDelay: double, CRSArrTime: timestamp, CRSDepTime: timestamp, DayOfMonth: bigint, DayOfWeek: bigint, DayOfYear: bigint, DepDelay: double, Distance: double, FlightDate: timestamp, FlightNum: string, ArrDelayBucket: double, Features_vec: vector]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 훈련(8:2)\n",
    "from numpy import *\n",
    "random.seed(SEED)\n",
    "\n",
    "training_data, test_data = final_vectorized_features.randomSplit([0.8, 0.2])\n",
    "training_data.cache()\n",
    "test_data.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.455859561233334\n"
     ]
    }
   ],
   "source": [
    "# k겹 교차 검즘\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# 반복 횟수\n",
    "numFolds = 5 \n",
    "\n",
    "# 모델 정의(랜덤 포레스트)\n",
    "rf = RandomForestClassifier(    \n",
    "    featuresCol=\"Features_vec\",\n",
    "    labelCol=\"ArrDelayBucket\",\n",
    "    predictionCol=\"Prediction\",\n",
    "    maxBins=4657,\n",
    "    maxMemoryInMB=1024)\n",
    "\n",
    "# 모델 평가 정의\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "            labelCol=\"ArrDelayBucket\",\n",
    "            predictionCol=\"Prediction\") \n",
    "\n",
    "# 파이프 라인 및 모델 파라미터 정의\n",
    "pipeline = Pipeline(stages=[rf]) # stages 배열 순서대로 나중에 bestModel에서 속성들을 찾을 수 있다.\n",
    "paramGrid = ParamGridBuilder().addGrid(rf.numTrees, [10,15,20]).addGrid(rf.maxDepth, [5,7,10]).build()\n",
    "\n",
    "# 교차 검증 정의\n",
    "crossval = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=numFolds)\n",
    "\n",
    "# 훈련시작(시간 체크)\n",
    "start = timeit.default_timer()\n",
    "\n",
    "model = crossval.fit(trainingData)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print('걸림시간: ',(stop - start)/60) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 베스트 모델 다루기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineModel_5933924c7a53\n",
      "RandomForestClassificationModel (uid=RandomForestClassifier_ef9d13b13603) with 10 trees\n",
      "(9,[0,1,2,3,4,5,6,7,8],[0.856746453954314,0.02405667307481816,0.0018408220086847758,0.0011827426965437596,0.007994231896378986,0.014629792815877821,0.019966781962298414,0.01446072462429139,0.059121776966792584])\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# 모델 중 가장 좋은 성능의 모델 가져오기\n",
    "bestPipeline = model.bestModel\n",
    "print(bestPipeline)\n",
    "\n",
    "# stages 반환값은 배열. -1 이라는 것은 가장 마지막 열 len-1 과 같다. # 그리고 stages는 위에서 만든 pipline의 매개변수이다.\n",
    "# 파이프라인의 랜덤 포레스트 모델 가져오기\n",
    "bestModel = bestPipeline.stages[0]\n",
    "print(bestModel)\n",
    "\n",
    "# 특징 중요도 \n",
    "importances = bestModel.featureImportances\n",
    "print(importances)\n",
    "\n",
    "# 모델의 트리개수(아래 두 코드는 같은 결과)\n",
    "print(bestModel.getNumTrees)\n",
    "print(model.bestModel.stages[-1]._java_obj.getNumTrees())\n",
    "\n",
    "# 모델의 트리 깊이(아래 두 코드는 같은 결과)\n",
    "print(bestModel.getOrDefault('maxDepth'))\n",
    "print(model.bestModel.stages[len(model.bestModel.stages)-1]._java_obj.getMaxDepth())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.6104931941486331\n"
     ]
    }
   ],
   "source": [
    "# 배스트 모델의 정확도 확인\n",
    "prediction = model.bestModel.transform(test_data)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"Prediction\",labelCol=\"ArrDelayBucket\", metricName=\"accuracy\")\n",
    "accuracy = {\"Accuracy\":evaluator.evaluate(prediction)}\n",
    "\n",
    "print(\"Accuracy = {}\".format(accuracy[\"Accuracy\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 특징 중요도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "정렬된 특징 중요도\n",
      "-------------------\n",
      "Name             Importance\n",
      "-------------  ------------\n",
      "DepDelay         0.858286\n",
      "Route_index      0.0668056\n",
      "Origin_index     0.0217321\n",
      "Distance         0.0194487\n",
      "Dest_index       0.0123101\n",
      "Carrier_index    0.0113784\n",
      "DayOfYear        0.00731536\n",
      "DayOfMonth       0.00160242\n",
      "DayOfWeek        0.00112085\n"
     ]
    }
   ],
   "source": [
    "# 빈 리스트 생성\n",
    "feature_importances = defaultdict(list)\n",
    "\n",
    "# 컬럼명 가져오기\n",
    "feature_names = vector_assembler.getInputCols()\n",
    "# 특징 중요도\n",
    "feature_importance_list = bestModel.featureImportances\n",
    "\n",
    "# zip 함수로 컬럼명과 특징 중요도록 묶어준다.\n",
    "for feature_name, feature_importance in zip(feature_names, feature_importance_list):\n",
    "    feature_importances[feature_name].append(feature_importance)\n",
    "\n",
    "# 리스트로 묶여있는 특징 중요도를 하나의 값,그리고 float 형식으로 변환\n",
    "feature_importance_entry = defaultdict(float)\n",
    "for feature_name, value_list in feature_importances.items():\n",
    "  average_importance = sum(value_list) / len(value_list)\n",
    "  feature_importance_entry[feature_name] = average_importance\n",
    "\n",
    "# 정렬하기\n",
    "import operator\n",
    "sorted_feature_importances = sorted(\n",
    "  feature_importance_entry.items(), # 정렬할 데이터\n",
    "  key=operator.itemgetter(1), # 정렬할 키(0:컬럼이름,1:특징 중요도)\n",
    "  reverse=True # 내림차순\n",
    ")\n",
    "\n",
    "print(\"\\n정렬된 특징 중요도\")\n",
    "print(\"-------------------\")\n",
    "print(tabulate(sorted_feature_importances, headers=['Name', 'Importance'])) # tabulate: 시각적을 보기 좋게 테이블화 해주는 모듈\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 이전 결과와 비교(정확도,특징 중요도)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Accuracy': 0.6104931941486331,\n",
       " 'DepDelay': 0.858286342821538,\n",
       " 'Route_index': 0.06680557136144623,\n",
       " 'Origin_index': 0.021732143046945946,\n",
       " 'Distance': 0.01944874148171331,\n",
       " 'Dest_index': 0.012310135720946623,\n",
       " 'Carrier_index': 0.011378438035375332,\n",
       " 'DayOfYear': 0.007315360251440876,\n",
       " 'DayOfMonth': 0.0016024150398984331,\n",
       " 'DayOfWeek': 0.0011208522406953548}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 현제 정확도, 특징 중요도 점수 가져와서 합치기\n",
    "score_log_entry = {result_name: accuracy[result_name] for result_name in result_names}\n",
    "for feature_name, importance in sorted_feature_importances:\n",
    "    score_log_entry[feature_name] = importance\n",
    "\n",
    "score_log_entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이전기록 있음\n",
      "\n",
      "Experiment Report\n",
      "-----------------\n",
      "Metric           Score\n",
      "-------------  -------\n",
      "Accuracy             0\n",
      "DepDelay             0\n",
      "Distance             0\n",
      "DayOfMonth           0\n",
      "DayOfWeek            0\n",
      "DayOfYear            0\n",
      "Carrier_index        0\n",
      "Origin_index         0\n",
      "Dest_index           0\n",
      "Route_index          0\n"
     ]
    }
   ],
   "source": [
    "import pickle, copy\n",
    "\n",
    "# 정확도, 특징 중요도를 합친 리스트 만들기\n",
    "result_names = list(accuracy.keys()) # 정확도 말고 f1 같은것도 넣을 수 있다.\n",
    "feature_names = vector_assembler.getInputCols()\n",
    "\n",
    "metric_names=[]\n",
    "for result_name in result_names:\n",
    "    metric_names.append(result_name) \n",
    "\n",
    "for feature_name in feature_names:\n",
    "    metric_names.append(feature_name)\n",
    "\n",
    "# 이전 결과값 가져오기\n",
    "try:\n",
    "  score_log_filename = \"./models/score_log.pickle\"\n",
    "  score_log = pickle.load(open(score_log_filename, \"rb\"))\n",
    "  if not isinstance(score_log, list):\n",
    "    score_log = []\n",
    "except IOError:\n",
    "  score_log = []\n",
    "\n",
    "# 현제 정확도, 특징 중요도 점수 가져와서 합치기\n",
    "score_log_entry = copy.deepcopy(accuracy)\n",
    "for feature_name, importance in sorted_feature_importances:\n",
    "    score_log_entry[feature_name] = importance\n",
    "\n",
    "# 이전 결과값이 있다면 이전 결과값 반환, 이전 결과값이 없다면 현재 결과값 반환.\n",
    "try:\n",
    "  last_log = score_log[-1]\n",
    "  print(\"이전기록 있음\")\n",
    "except (IndexError, TypeError, AttributeError):\n",
    "  last_log = score_log_entry\n",
    "  print(\"이전기록 없음\")\n",
    "\n",
    "experiment_report = []\n",
    "for metric_name in metric_names:\n",
    "  run_delta = score_log_entry[metric_name] - last_log[metric_name]\n",
    "  experiment_report.append((metric_name, run_delta))\n",
    "\n",
    "print(\"\\n이전 기록과 차이\")\n",
    "print(\"-----------------\")\n",
    "print(tabulate(experiment_report, headers=[\"Metric\", \"Score\"]))\n",
    "\n",
    "# 이전 결과값에 현재 결과값을 붙친다.\n",
    "score_log.append(score_log_entry)\n",
    "\n",
    "# 객체로 저장.\n",
    "pickle.dump(score_log, open(score_log_filename, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Accuracy': 0.6104931941486331,\n",
       "  'DepDelay': 0.858286342821538,\n",
       "  'Route_index': 0.06680557136144623,\n",
       "  'Origin_index': 0.021732143046945946,\n",
       "  'Distance': 0.01944874148171331,\n",
       "  'Dest_index': 0.012310135720946623,\n",
       "  'Carrier_index': 0.011378438035375332,\n",
       "  'DayOfYear': 0.007315360251440876,\n",
       "  'DayOfMonth': 0.0016024150398984331,\n",
       "  'DayOfWeek': 0.0011208522406953548},\n",
       " {'Accuracy': 0.6104931941486331,\n",
       "  'DepDelay': 0.858286342821538,\n",
       "  'Route_index': 0.06680557136144623,\n",
       "  'Origin_index': 0.021732143046945946,\n",
       "  'Distance': 0.01944874148171331,\n",
       "  'Dest_index': 0.012310135720946623,\n",
       "  'Carrier_index': 0.011378438035375332,\n",
       "  'DayOfYear': 0.007315360251440876,\n",
       "  'DayOfMonth': 0.0016024150398984331,\n",
       "  'DayOfWeek': 0.0011208522406953548}]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 이전 데이터는 덮어쓰지 않고 추가한다.\n",
    "score_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEED 적용 범위 \n",
    "\n",
    "데이터 훈련/테스트로 분할 할 때는 seed 적용 x, 모델 파라미터 seed 적용 o\n",
    "데이터 훈련/테스트로 분할 할 때는 seed 적용 o, 모델 파라미터 seed 적용 o\n",
    "=> 두 훈련 결과는 다름 \n",
    "\n",
    "데이터 훈련/테스트로 분할 할 때는 seed 적용 o, 모델 파라미터 seed 적용 x\n",
    "데이터 훈련/테스트로 분할 할 때는 seed 적용 o, 모델 파라미터 seed 적용 x\n",
    "=> 두 훈련 결과는 같음\n",
    "\n",
    "결론 훈련 데이터의 seed가 다르면 결과도 다르게 나오는데, 모델 파라미터 seed는 달라도 결과 같음.\n",
    "\n",
    "그리고\n",
    "\n",
    "통합적으로 seed를 걸수 있는데\n",
    "from numpy import *\n",
    "random.seed(SEED)\n",
    "\n",
    "위 코드를 시작하는 부분에 넣으면 그 뒤로 나오는 랜덤함수를 호출하는 부분은 \n",
    "\n",
    "설정한 시드값을 기준으로 출력된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.6124855450818236\n"
     ]
    }
   ],
   "source": [
    "prediction = model.bestModel.transform(test_data)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"Prediction\",labelCol=\"ArrDelayBucket\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(prediction)\n",
    "# 깊이 10 개수 10 시드(데이터x, 모델 0)\n",
    "print(\"Accuracy = {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.6095072829573539\n"
     ]
    }
   ],
   "source": [
    "prediction = model.bestModel.transform(test_data)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"Prediction\",labelCol=\"ArrDelayBucket\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(prediction)\n",
    "# 깊이 10 개수 10 시드(데이터0, 모델 0)\n",
    "print(\"Accuracy = {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.6101470614012025\n"
     ]
    }
   ],
   "source": [
    "prediction = model.bestModel.transform(test_data)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"Prediction\",labelCol=\"ArrDelayBucket\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(prediction)\n",
    "# 깊이 10 개수 10 시드(데이터0, 모델 x) 시도1 약 10분걸림\n",
    "print(\"Accuracy = {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.6101470614012025\n"
     ]
    }
   ],
   "source": [
    "prediction = model.bestModel.transform(test_data)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"Prediction\",labelCol=\"ArrDelayBucket\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(prediction)\n",
    "# 깊이 10 개수 10 시드(데이터0, 모델 x) 시도2 약 10분\n",
    "print(\"Accuracy = {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.6098067987282955\n"
     ]
    }
   ],
   "source": [
    "prediction = model.bestModel.transform(test_data)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"Prediction\",labelCol=\"ArrDelayBucket\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(prediction)\n",
    "# 깊이 10 개수 10 시드(데이터x, 모델 x, numpy으로 지정) 시도1 약 10분\n",
    "print(\"Accuracy = {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.6098067987282955\n"
     ]
    }
   ],
   "source": [
    "prediction = model.bestModel.transform(test_data)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"Prediction\",labelCol=\"ArrDelayBucket\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(prediction)\n",
    "# 깊이 10 개수 10 시드(데이터x, 모델 x, numpy으로 지정) 시도2 약 10분\n",
    "print(\"Accuracy = {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.6081872373349276\n"
     ]
    }
   ],
   "source": [
    "prediction = model.bestModel.transform(test_data)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"Prediction\",labelCol=\"ArrDelayBucket\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(prediction)\n",
    "# 깊이 10 개수 10 시드(데이터x, 모델 x, numpy으로 지정(2019+1)) 시도1 약 10분\n",
    "print(\"Accuracy = {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.6081872373349276\n"
     ]
    }
   ],
   "source": [
    "prediction = model.bestModel.transform(test_data)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"Prediction\",labelCol=\"ArrDelayBucket\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(prediction)\n",
    "# 깊이 10 개수 10 시드(데이터x, 모델 x, numpy으로 지정(2019+1)) 시도2 약 10분\n",
    "print(\"Accuracy = {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------end---------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(id=4, text='spark i j k', probability=DenseVector([0.2581, 0.7419]), prediction=1.0)\n",
      "Row(id=5, text='l m n', probability=DenseVector([0.9186, 0.0814]), prediction=0.0)\n",
      "Row(id=6, text='mapreduce spark', probability=DenseVector([0.432, 0.568]), prediction=1.0)\n",
      "Row(id=7, text='apache hadoop', probability=DenseVector([0.6766, 0.3234]), prediction=0.0)\n"
     ]
    }
   ],
   "source": [
    "# spark 공식 홈피 모델 튜닝 예제\n",
    "# https://spark.apache.org/docs/2.4.0/ml-tuning.html#model-selection-aka-hyperparameter-tuning\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "# Prepare training documents, which are labeled.\n",
    "training = spark.createDataFrame([\n",
    "    (0, \"a b c d e spark\", 1.0),\n",
    "    (1, \"b d\", 0.0),\n",
    "    (2, \"spark f g h\", 1.0),\n",
    "    (3, \"hadoop mapreduce\", 0.0),\n",
    "    (4, \"b spark who\", 1.0),\n",
    "    (5, \"g d a y\", 0.0),\n",
    "    (6, \"spark fly\", 1.0),\n",
    "    (7, \"was mapreduce\", 0.0),\n",
    "    (8, \"e spark program\", 1.0),\n",
    "    (9, \"a e c l\", 0.0),\n",
    "    (10, \"spark compile\", 1.0),\n",
    "    (11, \"hadoop software\", 0.0)\n",
    "], [\"id\", \"text\", \"label\"])\n",
    "\n",
    "# Configure an ML pipeline, which consists of tree stages: tokenizer, hashingTF, and lr.\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
    "\n",
    "# We now treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n",
    "# This will allow us to jointly choose parameters for all Pipeline stages.\n",
    "# A CrossValidator requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n",
    "# We use a ParamGridBuilder to construct a grid of parameters to search over.\n",
    "# With 3 values for hashingTF.numFeatures and 2 values for lr.regParam,\n",
    "# this grid will have 3 x 2 = 6 parameter settings for CrossValidator to choose from.\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(hashingTF.numFeatures, [10, 100, 1000]) \\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=2)  # use 3+ folds in practice\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cvModel = crossval.fit(training)\n",
    "\n",
    "# Prepare test documents, which are unlabeled.\n",
    "test = spark.createDataFrame([\n",
    "    (4, \"spark i j k\"),\n",
    "    (5, \"l m n\"),\n",
    "    (6, \"mapreduce spark\"),\n",
    "    (7, \"apache hadoop\")\n",
    "], [\"id\", \"text\"])\n",
    "\n",
    "# Make predictions on test documents. cvModel uses the best model found (lrModel).\n",
    "prediction = cvModel.transform(test)\n",
    "selected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\")\n",
    "for row in selected.collect():\n",
    "    print(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
