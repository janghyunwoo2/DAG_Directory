{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# airflow 스케줄러 실행\n",
    "# putty에서 직접 실행\n",
    "!airflow scheduler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.6/site-packages/airflow/configuration.py:590: DeprecationWarning: You have two airflow.cfg files: /home/ubuntu/airflow/airflow.cfg and /home/ubuntu/project1/airflow/airflow.cfg. Airflow used to look at ~/airflow/airflow.cfg, even when AIRFLOW_HOME was set to a different value. Airflow will now only read /home/ubuntu/project1/airflow/airflow.cfg, and you should remove the other file\n",
      "  category=DeprecationWarning,\n",
      "[2019-08-08 06:07:22,867] {__init__.py:51} INFO - Using executor SequentialExecutor\n",
      "  ____________       _____________\n",
      " ____    |__( )_________  __/__  /________      __\n",
      "____  /| |_  /__  ___/_  /_ __  /_  __ \\_ | /| / /\n",
      "___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /\n",
      " _/_/  |_/_/  /_/    /_/    /_/  \\____/____/|__/\n",
      "[2019-08-08 06:07:23,211] {__init__.py:305} INFO - Filling up the DagBag from /home/ubuntu/project1/airflow/dags\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:387: DeprecationWarning: Importing 'SSHHook' directly from 'airflow.contrib.hooks' has been deprecated. Please import from 'airflow.contrib.hooks.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.\n",
      "  DeprecationWarning)\n",
      "[2019-08-08 06:07:23,251] {__init__.py:416} ERROR - Failed to import: /home/ubuntu/project1/airflow/dags/data_refine_DAG.py\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.6/site-packages/airflow/models/__init__.py\", line 413, in process_file\n",
      "    m = imp.load_source(mod_name, filepath)\n",
      "  File \"/usr/lib/python3.6/imp.py\", line 172, in load_source\n",
      "    module = _load(spec)\n",
      "  File \"<frozen importlib._bootstrap>\", line 684, in _load\n",
      "  File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/home/ubuntu/project1/airflow/dags/data_refine_DAG.py\", line 99, in <module>\n",
      "    dag=dag\n",
      "  File \"/home/ubuntu/.local/lib/python3.6/site-packages/airflow/utils/decorators.py\", line 98, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.6/site-packages/airflow/operators/bash_operator.py\", line 70, in __init__\n",
      "    super(BashOperator, self).__init__(*args, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.6/site-packages/airflow/utils/decorators.py\", line 98, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.6/site-packages/airflow/models/__init__.py\", line 2166, in __init__\n",
      "    validate_key(task_id)\n",
      "  File \"/home/ubuntu/.local/lib/python3.6/site-packages/airflow/utils/helpers.py\", line 61, in validate_key\n",
      "    \"dots and underscores exclusively\".format(**locals()))\n",
      "airflow.exceptions.AirflowException: The key () has to be made of alphanumeric characters, dashes, dots and underscores exclusively\n",
      "Running the Gunicorn Server with:\n",
      "Workers: 4 sync\n",
      "Host: 0.0.0.0:8080\n",
      "Timeout: 120\n",
      "Logfiles: - -\n",
      "=================================================================            \n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/airflow/configuration.py:590: DeprecationWarning: You have two airflow.cfg files: /home/ubuntu/airflow/airflow.cfg and /home/ubuntu/project1/airflow/airflow.cfg. Airflow used to look at ~/airflow/airflow.cfg, even when AIRFLOW_HOME was set to a different value. Airflow will now only read /home/ubuntu/project1/airflow/airflow.cfg, and you should remove the other file\n",
      "  category=DeprecationWarning,\n",
      "[2019-08-08 06:07:24 +0000] [4426] [INFO] Starting gunicorn 19.9.0\n",
      "[2019-08-08 06:07:24 +0000] [4426] [INFO] Listening at: http://0.0.0.0:8080 (4426)\n",
      "[2019-08-08 06:07:24 +0000] [4426] [INFO] Using worker: sync\n",
      "[2019-08-08 06:07:24 +0000] [4431] [INFO] Booting worker with pid: 4431\n",
      "[2019-08-08 06:07:24 +0000] [4432] [INFO] Booting worker with pid: 4432\n",
      "[2019-08-08 06:07:24,256] {__init__.py:51} INFO - Using executor SequentialExecutor\n",
      "[2019-08-08 06:07:24 +0000] [4433] [INFO] Booting worker with pid: 4433\n",
      "[2019-08-08 06:07:24,310] {__init__.py:51} INFO - Using executor SequentialExecutor\n",
      "[2019-08-08 06:07:24 +0000] [4435] [INFO] Booting worker with pid: 4435\n",
      "[2019-08-08 06:07:24,538] {__init__.py:51} INFO - Using executor SequentialExecutor\n",
      "[2019-08-08 06:07:24,616] {__init__.py:51} INFO - Using executor SequentialExecutor\n",
      "[2019-08-08 06:07:24,674] {__init__.py:305} INFO - Filling up the DagBag from /home/ubuntu/project1/airflow/dags\n",
      "[2019-08-08 06:07:24,680] {__init__.py:305} INFO - Filling up the DagBag from /home/ubuntu/project1/airflow/dags\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:387: DeprecationWarning: Importing 'SSHHook' directly from 'airflow.contrib.hooks' has been deprecated. Please import from 'airflow.contrib.hooks.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.\n",
      "  DeprecationWarning)\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:387: DeprecationWarning: Importing 'SSHHook' directly from 'airflow.contrib.hooks' has been deprecated. Please import from 'airflow.contrib.hooks.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.\n",
      "  DeprecationWarning)\n",
      "[2019-08-08 06:07:24,804] {__init__.py:416} ERROR - Failed to import: /home/ubuntu/project1/airflow/dags/data_refine_DAG.py\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.6/site-packages/airflow/models/__init__.py\", line 413, in process_file\n",
      "    m = imp.load_source(mod_name, filepath)\n",
      "  File \"/usr/lib/python3.6/imp.py\", line 172, in load_source\n",
      "    module = _load(spec)\n",
      "  File \"<frozen importlib._bootstrap>\", line 684, in _load\n",
      "  File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/home/ubuntu/project1/airflow/dags/data_refine_DAG.py\", line 99, in <module>\n",
      "    dag=dag\n",
      "  File \"/home/ubuntu/.local/lib/python3.6/site-packages/airflow/utils/decorators.py\", line 98, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.6/site-packages/airflow/operators/bash_operator.py\", line 70, in __init__\n",
      "    super(BashOperator, self).__init__(*args, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.6/site-packages/airflow/utils/decorators.py\", line 98, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.6/site-packages/airflow/models/__init__.py\", line 2166, in __init__\n",
      "    validate_key(task_id)\n",
      "  File \"/home/ubuntu/.local/lib/python3.6/site-packages/airflow/utils/helpers.py\", line 61, in validate_key\n",
      "    \"dots and underscores exclusively\".format(**locals()))\n",
      "airflow.exceptions.AirflowException: The key () has to be made of alphanumeric characters, dashes, dots and underscores exclusively\n",
      "[2019-08-08 06:07:24,829] {__init__.py:416} ERROR - Failed to import: /home/ubuntu/project1/airflow/dags/data_refine_DAG.py\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.6/site-packages/airflow/models/__init__.py\", line 413, in process_file\n",
      "    m = imp.load_source(mod_name, filepath)\n",
      "  File \"/usr/lib/python3.6/imp.py\", line 172, in load_source\n",
      "    module = _load(spec)\n",
      "  File \"<frozen importlib._bootstrap>\", line 684, in _load\n",
      "  File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/home/ubuntu/project1/airflow/dags/data_refine_DAG.py\", line 99, in <module>\n",
      "    dag=dag\n",
      "  File \"/home/ubuntu/.local/lib/python3.6/site-packages/airflow/utils/decorators.py\", line 98, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.6/site-packages/airflow/operators/bash_operator.py\", line 70, in __init__\n",
      "    super(BashOperator, self).__init__(*args, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.6/site-packages/airflow/utils/decorators.py\", line 98, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.6/site-packages/airflow/models/__init__.py\", line 2166, in __init__\n",
      "    validate_key(task_id)\n",
      "  File \"/home/ubuntu/.local/lib/python3.6/site-packages/airflow/utils/helpers.py\", line 61, in validate_key\n",
      "    \"dots and underscores exclusively\".format(**locals()))\n",
      "airflow.exceptions.AirflowException: The key () has to be made of alphanumeric characters, dashes, dots and underscores exclusively\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-08-08 06:07:24,965] {__init__.py:305} INFO - Filling up the DagBag from /home/ubuntu/project1/airflow/dags\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:387: DeprecationWarning: Importing 'SSHHook' directly from 'airflow.contrib.hooks' has been deprecated. Please import from 'airflow.contrib.hooks.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.\n",
      "  DeprecationWarning)\n",
      "[2019-08-08 06:07:25,036] {__init__.py:305} INFO - Filling up the DagBag from /home/ubuntu/project1/airflow/dags\n",
      "[2019-08-08 06:07:25,057] {__init__.py:416} ERROR - Failed to import: /home/ubuntu/project1/airflow/dags/data_refine_DAG.py\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.6/site-packages/airflow/models/__init__.py\", line 413, in process_file\n",
      "    m = imp.load_source(mod_name, filepath)\n",
      "  File \"/usr/lib/python3.6/imp.py\", line 172, in load_source\n",
      "    module = _load(spec)\n",
      "  File \"<frozen importlib._bootstrap>\", line 684, in _load\n",
      "  File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/home/ubuntu/project1/airflow/dags/data_refine_DAG.py\", line 99, in <module>\n",
      "    dag=dag\n",
      "  File \"/home/ubuntu/.local/lib/python3.6/site-packages/airflow/utils/decorators.py\", line 98, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.6/site-packages/airflow/operators/bash_operator.py\", line 70, in __init__\n",
      "    super(BashOperator, self).__init__(*args, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.6/site-packages/airflow/utils/decorators.py\", line 98, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.6/site-packages/airflow/models/__init__.py\", line 2166, in __init__\n",
      "    validate_key(task_id)\n",
      "  File \"/home/ubuntu/.local/lib/python3.6/site-packages/airflow/utils/helpers.py\", line 61, in validate_key\n",
      "    \"dots and underscores exclusively\".format(**locals()))\n",
      "airflow.exceptions.AirflowException: The key () has to be made of alphanumeric characters, dashes, dots and underscores exclusively\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:387: DeprecationWarning: Importing 'SSHHook' directly from 'airflow.contrib.hooks' has been deprecated. Please import from 'airflow.contrib.hooks.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.\n",
      "  DeprecationWarning)\n",
      "[2019-08-08 06:07:25,093] {__init__.py:416} ERROR - Failed to import: /home/ubuntu/project1/airflow/dags/data_refine_DAG.py\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.6/site-packages/airflow/models/__init__.py\", line 413, in process_file\n",
      "    m = imp.load_source(mod_name, filepath)\n",
      "  File \"/usr/lib/python3.6/imp.py\", line 172, in load_source\n",
      "    module = _load(spec)\n",
      "  File \"<frozen importlib._bootstrap>\", line 684, in _load\n",
      "  File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/home/ubuntu/project1/airflow/dags/data_refine_DAG.py\", line 99, in <module>\n",
      "    dag=dag\n",
      "  File \"/home/ubuntu/.local/lib/python3.6/site-packages/airflow/utils/decorators.py\", line 98, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.6/site-packages/airflow/operators/bash_operator.py\", line 70, in __init__\n",
      "    super(BashOperator, self).__init__(*args, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.6/site-packages/airflow/utils/decorators.py\", line 98, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.6/site-packages/airflow/models/__init__.py\", line 2166, in __init__\n",
      "    validate_key(task_id)\n",
      "  File \"/home/ubuntu/.local/lib/python3.6/site-packages/airflow/utils/helpers.py\", line 61, in validate_key\n",
      "    \"dots and underscores exclusively\".format(**locals()))\n",
      "airflow.exceptions.AirflowException: The key () has to be made of alphanumeric characters, dashes, dots and underscores exclusively\n",
      "112.150.121.158 - - [08/Aug/2019:06:07:42 +0000] \"GET / HTTP/1.1\" 302 221 \"-\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36\"\n",
      "112.150.121.158 - - [08/Aug/2019:06:07:42 +0000] \"GET /admin/ HTTP/1.1\" 200 44267 \"-\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36\"\n",
      "112.150.121.158 - - [08/Aug/2019:06:07:42 +0000] \"GET /admin/admin/bootstrap/bootstrap3/swatch/default/bootstrap.min.css?v=3.3.5 HTTP/1.1\" 200 0 \"http://ec2-13-209-183-234.ap-northeast-2.compute.amazonaws.com:8080/admin/\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36\"\n",
      "112.150.121.158 - - [08/Aug/2019:06:07:42 +0000] \"GET /admin/admin/admin/css/bootstrap3/admin.css?v=1.1.1 HTTP/1.1\" 200 0 \"http://ec2-13-209-183-234.ap-northeast-2.compute.amazonaws.com:8080/admin/\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36\"\n",
      "112.150.121.158 - - [08/Aug/2019:06:07:42 +0000] \"GET /admin/admin/admin/css/bootstrap3/submenu.css HTTP/1.1\" 200 0 \"http://ec2-13-209-183-234.ap-northeast-2.compute.amazonaws.com:8080/admin/\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36\"\n",
      "112.150.121.158 - - [08/Aug/2019:06:07:42 +0000] \"GET /static/main.css HTTP/1.1\" 200 0 \"http://ec2-13-209-183-234.ap-northeast-2.compute.amazonaws.com:8080/admin/\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36\"\n",
      "112.150.121.158 - - [08/Aug/2019:06:07:42 +0000] \"GET /admin/admin/bootstrap/bootstrap3/css/bootstrap-theme.min.css?v=3.3.5 HTTP/1.1\" 200 0 \"http://ec2-13-209-183-234.ap-northeast-2.compute.amazonaws.com:8080/admin/\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36\"\n",
      "112.150.121.158 - - [08/Aug/2019:06:07:42 +0000] \"GET /static/bootstrap-theme.css HTTP/1.1\" 200 0 \"http://ec2-13-209-183-234.ap-northeast-2.compute.amazonaws.com:8080/admin/\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36\"\n",
      "112.150.121.158 - - [08/Aug/2019:06:07:42 +0000] \"GET /static/bootstrap-toggle.min.css HTTP/1.1\" 200 0 \"http://ec2-13-209-183-234.ap-northeast-2.compute.amazonaws.com:8080/admin/\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36\"\n",
      "112.150.121.158 - - [08/Aug/2019:06:07:42 +0000] \"GET /static/dataTables.bootstrap.css HTTP/1.1\" 200 0 \"http://ec2-13-209-183-234.ap-northeast-2.compute.amazonaws.com:8080/admin/\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36\"\n",
      "112.150.121.158 - - [08/Aug/2019:06:07:42 +0000] \"GET /admin/admin/vendor/jquery.min.js?v=2.1.4 HTTP/1.1\" 200 0 \"http://ec2-13-209-183-234.ap-northeast-2.compute.amazonaws.com:8080/admin/\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36\"\n",
      "112.150.121.158 - - [08/Aug/2019:06:07:42 +0000] \"GET /admin/admin/bootstrap/bootstrap3/js/bootstrap.min.js?v=3.3.5 HTTP/1.1\" 200 0 \"http://ec2-13-209-183-234.ap-northeast-2.compute.amazonaws.com:8080/admin/\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36\"\n",
      "112.150.121.158 - - [08/Aug/2019:06:07:42 +0000] \"GET /admin/admin/vendor/moment.min.js?v=2.9.0 HTTP/1.1\" 200 0 \"http://ec2-13-209-183-234.ap-northeast-2.compute.amazonaws.com:8080/admin/\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36\"\n",
      "112.150.121.158 - - [08/Aug/2019:06:07:42 +0000] \"GET /static/jqClock.min.js HTTP/1.1\" 200 0 \"http://ec2-13-209-183-234.ap-northeast-2.compute.amazonaws.com:8080/admin/\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36\"\n",
      "112.150.121.158 - - [08/Aug/2019:06:07:42 +0000] \"GET /admin/admin/vendor/select2/select2.min.js?v=3.5.2 HTTP/1.1\" 200 0 \"http://ec2-13-209-183-234.ap-northeast-2.compute.amazonaws.com:8080/admin/\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112.150.121.158 - - [08/Aug/2019:06:07:42 +0000] \"GET /static/bootstrap-toggle.min.js HTTP/1.1\" 200 0 \"http://ec2-13-209-183-234.ap-northeast-2.compute.amazonaws.com:8080/admin/\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36\"\n",
      "112.150.121.158 - - [08/Aug/2019:06:07:42 +0000] \"GET /static/jquery.dataTables.min.js HTTP/1.1\" 200 0 \"http://ec2-13-209-183-234.ap-northeast-2.compute.amazonaws.com:8080/admin/\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36\"\n",
      "112.150.121.158 - - [08/Aug/2019:06:07:42 +0000] \"GET /static/d3.v3.min.js HTTP/1.1\" 200 0 \"http://ec2-13-209-183-234.ap-northeast-2.compute.amazonaws.com:8080/admin/\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36\"\n",
      "112.150.121.158 - - [08/Aug/2019:06:07:42 +0000] \"GET /static/bootstrap3-typeahead.min.js HTTP/1.1\" 200 0 \"http://ec2-13-209-183-234.ap-northeast-2.compute.amazonaws.com:8080/admin/\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36\"\n",
      "112.150.121.158 - - [08/Aug/2019:06:07:42 +0000] \"GET /static/pin_100.png HTTP/1.1\" 200 0 \"http://ec2-13-209-183-234.ap-northeast-2.compute.amazonaws.com:8080/admin/\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36\"\n",
      "112.150.121.158 - - [08/Aug/2019:06:07:42 +0000] \"GET /static/loading.gif HTTP/1.1\" 200 0 \"http://ec2-13-209-183-234.ap-northeast-2.compute.amazonaws.com:8080/admin/\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36\"\n",
      "112.150.121.158 - - [08/Aug/2019:06:07:43 +0000] \"GET /admin/admin/bootstrap/bootstrap3/swatch/fonts/glyphicons-halflings-regular.woff2 HTTP/1.1\" 200 0 \"http://ec2-13-209-183-234.ap-northeast-2.compute.amazonaws.com:8080/admin/admin/bootstrap/bootstrap3/swatch/default/bootstrap.min.css?v=3.3.5\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36\"\n",
      "112.150.121.158 - - [08/Aug/2019:06:07:43 +0000] \"GET /admin/airflow/dag_stats HTTP/1.1\" 200 11778 \"http://ec2-13-209-183-234.ap-northeast-2.compute.amazonaws.com:8080/admin/\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36\"\n",
      "112.150.121.158 - - [08/Aug/2019:06:07:43 +0000] \"GET /admin/airflow/blocked HTTP/1.1\" 200 422 \"http://ec2-13-209-183-234.ap-northeast-2.compute.amazonaws.com:8080/admin/\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36\"\n",
      "112.150.121.158 - - [08/Aug/2019:06:07:43 +0000] \"GET /admin/airflow/task_stats HTTP/1.1\" 200 37978 \"http://ec2-13-209-183-234.ap-northeast-2.compute.amazonaws.com:8080/admin/\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36\"\n",
      "112.150.121.158 - - [08/Aug/2019:06:07:43 +0000] \"GET /static/pin_30.png HTTP/1.1\" 200 0 \"http://ec2-13-209-183-234.ap-northeast-2.compute.amazonaws.com:8080/admin/\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36\"\n",
      "112.150.121.158 - - [08/Aug/2019:06:07:48 +0000] \"GET /admin/connection/ HTTP/1.1\" 200 108610 \"http://ec2-13-209-183-234.ap-northeast-2.compute.amazonaws.com:8080/admin/\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36\"\n",
      "112.150.121.158 - - [08/Aug/2019:06:07:48 +0000] \"GET /admin/admin/vendor/select2/select2.css?v=3.5.2 HTTP/1.1\" 200 0 \"http://ec2-13-209-183-234.ap-northeast-2.compute.amazonaws.com:8080/admin/connection/\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36\"\n",
      "112.150.121.158 - - [08/Aug/2019:06:07:48 +0000] \"GET /admin/admin/vendor/select2/select2-bootstrap3.css?v=1.4.6 HTTP/1.1\" 200 0 \"http://ec2-13-209-183-234.ap-northeast-2.compute.amazonaws.com:8080/admin/connection/\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36\"\n",
      "112.150.121.158 - - [08/Aug/2019:06:07:48 +0000] \"GET /admin/admin/vendor/bootstrap-daterangepicker/daterangepicker-bs3.css?v=1.3.22 HTTP/1.1\" 200 0 \"http://ec2-13-209-183-234.ap-northeast-2.compute.amazonaws.com:8080/admin/connection/\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36\"\n",
      "112.150.121.158 - - [08/Aug/2019:06:07:48 +0000] \"GET /admin/admin/admin/js/form.js?v=1.0.1 HTTP/1.1\" 200 0 \"http://ec2-13-209-183-234.ap-northeast-2.compute.amazonaws.com:8080/admin/connection/\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36\"\n",
      "112.150.121.158 - - [08/Aug/2019:06:07:48 +0000] \"GET /admin/admin/admin/js/filters.js?v=1.0.0 HTTP/1.1\" 200 0 \"http://ec2-13-209-183-234.ap-northeast-2.compute.amazonaws.com:8080/admin/connection/\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36\"\n",
      "112.150.121.158 - - [08/Aug/2019:06:07:48 +0000] \"GET /admin/admin/vendor/bootstrap-daterangepicker/daterangepicker.js?v=1.3.22 HTTP/1.1\" 200 0 \"http://ec2-13-209-183-234.ap-northeast-2.compute.amazonaws.com:8080/admin/connection/\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36\"\n",
      "112.150.121.158 - - [08/Aug/2019:06:07:48 +0000] \"GET /admin/admin/admin/js/actions.js?v=1.0.0 HTTP/1.1\" 200 0 \"http://ec2-13-209-183-234.ap-northeast-2.compute.amazonaws.com:8080/admin/connection/\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36\"\n",
      "[2019-08-08 06:07:55 +0000] [4426] [INFO] Handling signal: ttin\n",
      "[2019-08-08 06:07:55 +0000] [4439] [INFO] Booting worker with pid: 4439\n",
      "[2019-08-08 06:07:55,538] {__init__.py:51} INFO - Using executor SequentialExecutor\n",
      "[2019-08-08 06:07:55,715] {__init__.py:305} INFO - Filling up the DagBag from /home/ubuntu/project1/airflow/dags\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:387: DeprecationWarning: Importing 'SSHHook' directly from 'airflow.contrib.hooks' has been deprecated. Please import from 'airflow.contrib.hooks.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.\n",
      "  DeprecationWarning)\n",
      "[2019-08-08 06:07:55,772] {__init__.py:416} ERROR - Failed to import: /home/ubuntu/project1/airflow/dags/data_refine_DAG.py\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.6/site-packages/airflow/models/__init__.py\", line 413, in process_file\n",
      "    m = imp.load_source(mod_name, filepath)\n",
      "  File \"/usr/lib/python3.6/imp.py\", line 172, in load_source\n",
      "    module = _load(spec)\n",
      "  File \"<frozen importlib._bootstrap>\", line 684, in _load\n",
      "  File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/home/ubuntu/project1/airflow/dags/data_refine_DAG.py\", line 99, in <module>\n",
      "    dag=dag\n",
      "  File \"/home/ubuntu/.local/lib/python3.6/site-packages/airflow/utils/decorators.py\", line 98, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.6/site-packages/airflow/operators/bash_operator.py\", line 70, in __init__\n",
      "    super(BashOperator, self).__init__(*args, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.6/site-packages/airflow/utils/decorators.py\", line 98, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.6/site-packages/airflow/models/__init__.py\", line 2166, in __init__\n",
      "    validate_key(task_id)\n",
      "  File \"/home/ubuntu/.local/lib/python3.6/site-packages/airflow/utils/helpers.py\", line 61, in validate_key\n",
      "    \"dots and underscores exclusively\".format(**locals()))\n",
      "airflow.exceptions.AirflowException: The key () has to be made of alphanumeric characters, dashes, dots and underscores exclusively\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112.150.121.158 - - [08/Aug/2019:06:07:56 +0000] \"GET /admin/connection/edit/?id=38&url=%2Fadmin%2Fconnection%2F HTTP/1.1\" 200 17816 \"http://ec2-13-209-183-234.ap-northeast-2.compute.amazonaws.com:8080/admin/connection/\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36\"\n",
      "112.150.121.158 - - [08/Aug/2019:06:07:56 +0000] \"GET /static/connection_form.js HTTP/1.1\" 200 0 \"http://ec2-13-209-183-234.ap-northeast-2.compute.amazonaws.com:8080/admin/connection/edit/?id=38&url=%2Fadmin%2Fconnection%2F\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36\"\n",
      "112.150.121.158 - - [08/Aug/2019:06:07:56 +0000] \"GET /admin/admin/vendor/select2/select2.png HTTP/1.1\" 200 0 \"http://ec2-13-209-183-234.ap-northeast-2.compute.amazonaws.com:8080/admin/admin/vendor/select2/select2.css?v=3.5.2\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36\"\n",
      "[2019-08-08 06:07:56 +0000] [4426] [INFO] Handling signal: ttou\n",
      "[2019-08-08 06:07:56 +0000] [4431] [INFO] Worker exiting (pid: 4431)\n",
      "112.150.121.158 - - [08/Aug/2019:06:08:25 +0000] \"GET /admin/ HTTP/1.1\" 200 44267 \"http://ec2-13-209-183-234.ap-northeast-2.compute.amazonaws.com:8080/admin/connection/edit/?id=38&url=%2Fadmin%2Fconnection%2F\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36\"\n",
      "112.150.121.158 - - [08/Aug/2019:06:08:25 +0000] \"GET /admin/airflow/blocked HTTP/1.1\" 200 422 \"http://ec2-13-209-183-234.ap-northeast-2.compute.amazonaws.com:8080/admin/\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36\"\n",
      "112.150.121.158 - - [08/Aug/2019:06:08:25 +0000] \"GET /admin/airflow/dag_stats HTTP/1.1\" 200 11778 \"http://ec2-13-209-183-234.ap-northeast-2.compute.amazonaws.com:8080/admin/\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36\"\n",
      "112.150.121.158 - - [08/Aug/2019:06:08:25 +0000] \"GET /admin/airflow/task_stats HTTP/1.1\" 200 37978 \"http://ec2-13-209-183-234.ap-northeast-2.compute.amazonaws.com:8080/admin/\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36\"\n",
      "[2019-08-08 06:08:26 +0000] [4426] [INFO] Handling signal: ttin\n",
      "[2019-08-08 06:08:26 +0000] [4452] [INFO] Booting worker with pid: 4452\n",
      "[2019-08-08 06:08:26,880] {__init__.py:51} INFO - Using executor SequentialExecutor\n",
      "[2019-08-08 06:08:27,060] {__init__.py:305} INFO - Filling up the DagBag from /home/ubuntu/project1/airflow/dags\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:387: DeprecationWarning: Importing 'SSHHook' directly from 'airflow.contrib.hooks' has been deprecated. Please import from 'airflow.contrib.hooks.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.\n",
      "  DeprecationWarning)\n",
      "[2019-08-08 06:08:27,117] {__init__.py:416} ERROR - Failed to import: /home/ubuntu/project1/airflow/dags/data_refine_DAG.py\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.6/site-packages/airflow/models/__init__.py\", line 413, in process_file\n",
      "    m = imp.load_source(mod_name, filepath)\n",
      "  File \"/usr/lib/python3.6/imp.py\", line 172, in load_source\n",
      "    module = _load(spec)\n",
      "  File \"<frozen importlib._bootstrap>\", line 684, in _load\n",
      "  File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/home/ubuntu/project1/airflow/dags/data_refine_DAG.py\", line 99, in <module>\n",
      "    dag=dag\n",
      "  File \"/home/ubuntu/.local/lib/python3.6/site-packages/airflow/utils/decorators.py\", line 98, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.6/site-packages/airflow/operators/bash_operator.py\", line 70, in __init__\n",
      "    super(BashOperator, self).__init__(*args, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.6/site-packages/airflow/utils/decorators.py\", line 98, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.6/site-packages/airflow/models/__init__.py\", line 2166, in __init__\n",
      "    validate_key(task_id)\n",
      "  File \"/home/ubuntu/.local/lib/python3.6/site-packages/airflow/utils/helpers.py\", line 61, in validate_key\n",
      "    \"dots and underscores exclusively\".format(**locals()))\n",
      "airflow.exceptions.AirflowException: The key () has to be made of alphanumeric characters, dashes, dots and underscores exclusively\n",
      "[2019-08-08 06:08:27 +0000] [4426] [INFO] Handling signal: ttou\n",
      "[2019-08-08 06:08:27 +0000] [4432] [INFO] Worker exiting (pid: 4432)\n",
      "112.150.121.158 - - [08/Aug/2019:06:08:29 +0000] \"GET /admin/connection/ HTTP/1.1\" 200 108610 \"http://ec2-13-209-183-234.ap-northeast-2.compute.amazonaws.com:8080/admin/\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36\"\n",
      "[2019-08-08 06:08:58 +0000] [4426] [INFO] Handling signal: ttin\n",
      "[2019-08-08 06:08:58 +0000] [4455] [INFO] Booting worker with pid: 4455\n",
      "[2019-08-08 06:08:58,218] {__init__.py:51} INFO - Using executor SequentialExecutor\n",
      "[2019-08-08 06:08:58,398] {__init__.py:305} INFO - Filling up the DagBag from /home/ubuntu/project1/airflow/dags\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:387: DeprecationWarning: Importing 'SSHHook' directly from 'airflow.contrib.hooks' has been deprecated. Please import from 'airflow.contrib.hooks.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.\n",
      "  DeprecationWarning)\n",
      "[2019-08-08 06:08:58,454] {__init__.py:416} ERROR - Failed to import: /home/ubuntu/project1/airflow/dags/data_refine_DAG.py\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.6/site-packages/airflow/models/__init__.py\", line 413, in process_file\n",
      "    m = imp.load_source(mod_name, filepath)\n",
      "  File \"/usr/lib/python3.6/imp.py\", line 172, in load_source\n",
      "    module = _load(spec)\n",
      "  File \"<frozen importlib._bootstrap>\", line 684, in _load\n",
      "  File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/home/ubuntu/project1/airflow/dags/data_refine_DAG.py\", line 99, in <module>\n",
      "    dag=dag\n",
      "  File \"/home/ubuntu/.local/lib/python3.6/site-packages/airflow/utils/decorators.py\", line 98, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.6/site-packages/airflow/operators/bash_operator.py\", line 70, in __init__\n",
      "    super(BashOperator, self).__init__(*args, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.6/site-packages/airflow/utils/decorators.py\", line 98, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.6/site-packages/airflow/models/__init__.py\", line 2166, in __init__\n",
      "    validate_key(task_id)\n",
      "  File \"/home/ubuntu/.local/lib/python3.6/site-packages/airflow/utils/helpers.py\", line 61, in validate_key\n",
      "    \"dots and underscores exclusively\".format(**locals()))\n",
      "airflow.exceptions.AirflowException: The key () has to be made of alphanumeric characters, dashes, dots and underscores exclusively\n",
      "[2019-08-08 06:08:59 +0000] [4426] [INFO] Handling signal: ttou\n",
      "[2019-08-08 06:08:59 +0000] [4433] [INFO] Worker exiting (pid: 4433)\n",
      "112.150.121.158 - - [08/Aug/2019:06:09:18 +0000] \"GET /admin/connection/edit/?id=39&url=%2Fadmin%2Fconnection%2F HTTP/1.1\" 200 17824 \"http://ec2-13-209-183-234.ap-northeast-2.compute.amazonaws.com:8080/admin/connection/\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36\"\n",
      "112.150.121.158 - - [08/Aug/2019:06:09:25 +0000] \"GET /admin/connection/ HTTP/1.1\" 200 108610 \"http://ec2-13-209-183-234.ap-northeast-2.compute.amazonaws.com:8080/admin/\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112.150.121.158 - - [08/Aug/2019:06:09:28 +0000] \"GET /admin/connection/edit/?id=39&url=%2Fadmin%2Fconnection%2F HTTP/1.1\" 200 17824 \"http://ec2-13-209-183-234.ap-northeast-2.compute.amazonaws.com:8080/admin/connection/\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36\"\n",
      "[2019-08-08 06:09:29 +0000] [4426] [INFO] Handling signal: ttin\n",
      "[2019-08-08 06:09:29 +0000] [4468] [INFO] Booting worker with pid: 4468\n",
      "[2019-08-08 06:09:29,574] {__init__.py:51} INFO - Using executor SequentialExecutor\n",
      "[2019-08-08 06:09:29,753] {__init__.py:305} INFO - Filling up the DagBag from /home/ubuntu/project1/airflow/dags\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:387: DeprecationWarning: Importing 'SSHHook' directly from 'airflow.contrib.hooks' has been deprecated. Please import from 'airflow.contrib.hooks.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.\n",
      "  DeprecationWarning)\n",
      "[2019-08-08 06:09:29,809] {__init__.py:416} ERROR - Failed to import: /home/ubuntu/project1/airflow/dags/data_refine_DAG.py\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.6/site-packages/airflow/models/__init__.py\", line 413, in process_file\n",
      "    m = imp.load_source(mod_name, filepath)\n",
      "  File \"/usr/lib/python3.6/imp.py\", line 172, in load_source\n",
      "    module = _load(spec)\n",
      "  File \"<frozen importlib._bootstrap>\", line 684, in _load\n",
      "  File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/home/ubuntu/project1/airflow/dags/data_refine_DAG.py\", line 99, in <module>\n",
      "    dag=dag\n",
      "  File \"/home/ubuntu/.local/lib/python3.6/site-packages/airflow/utils/decorators.py\", line 98, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.6/site-packages/airflow/operators/bash_operator.py\", line 70, in __init__\n",
      "    super(BashOperator, self).__init__(*args, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.6/site-packages/airflow/utils/decorators.py\", line 98, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.6/site-packages/airflow/models/__init__.py\", line 2166, in __init__\n",
      "    validate_key(task_id)\n",
      "  File \"/home/ubuntu/.local/lib/python3.6/site-packages/airflow/utils/helpers.py\", line 61, in validate_key\n",
      "    \"dots and underscores exclusively\".format(**locals()))\n",
      "airflow.exceptions.AirflowException: The key () has to be made of alphanumeric characters, dashes, dots and underscores exclusively\n",
      "[2019-08-08 06:09:30 +0000] [4426] [INFO] Handling signal: ttou\n",
      "[2019-08-08 06:09:30 +0000] [4435] [INFO] Worker exiting (pid: 4435)\n",
      "112.150.121.158 - - [08/Aug/2019:06:09:33 +0000] \"GET /admin/connection/edit/?id=28&url=%2Fadmin%2Fconnection%2F HTTP/1.1\" 200 19671 \"http://ec2-13-209-183-234.ap-northeast-2.compute.amazonaws.com:8080/admin/connection/\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36\"\n",
      "112.150.121.158 - - [08/Aug/2019:06:09:48 +0000] \"GET /admin/connection/ HTTP/1.1\" 200 108610 \"http://ec2-13-209-183-234.ap-northeast-2.compute.amazonaws.com:8080/admin/\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36\"\n",
      "112.150.121.158 - - [08/Aug/2019:06:10:00 +0000] \"GET /admin/connection/edit/?id=38&url=%2Fadmin%2Fconnection%2F HTTP/1.1\" 200 17816 \"http://ec2-13-209-183-234.ap-northeast-2.compute.amazonaws.com:8080/admin/connection/\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36\"\n",
      "[2019-08-08 06:10:00 +0000] [4426] [INFO] Handling signal: ttin\n",
      "[2019-08-08 06:10:00 +0000] [4470] [INFO] Booting worker with pid: 4470\n",
      "[2019-08-08 06:10:00,928] {__init__.py:51} INFO - Using executor SequentialExecutor\n",
      "[2019-08-08 06:10:01,104] {__init__.py:305} INFO - Filling up the DagBag from /home/ubuntu/project1/airflow/dags\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:387: DeprecationWarning: Importing 'SSHHook' directly from 'airflow.contrib.hooks' has been deprecated. Please import from 'airflow.contrib.hooks.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.\n",
      "  DeprecationWarning)\n",
      "[2019-08-08 06:10:01,161] {__init__.py:416} ERROR - Failed to import: /home/ubuntu/project1/airflow/dags/data_refine_DAG.py\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.6/site-packages/airflow/models/__init__.py\", line 413, in process_file\n",
      "    m = imp.load_source(mod_name, filepath)\n",
      "  File \"/usr/lib/python3.6/imp.py\", line 172, in load_source\n",
      "    module = _load(spec)\n",
      "  File \"<frozen importlib._bootstrap>\", line 684, in _load\n",
      "  File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/home/ubuntu/project1/airflow/dags/data_refine_DAG.py\", line 99, in <module>\n",
      "    dag=dag\n",
      "  File \"/home/ubuntu/.local/lib/python3.6/site-packages/airflow/utils/decorators.py\", line 98, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.6/site-packages/airflow/operators/bash_operator.py\", line 70, in __init__\n",
      "    super(BashOperator, self).__init__(*args, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.6/site-packages/airflow/utils/decorators.py\", line 98, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.6/site-packages/airflow/models/__init__.py\", line 2166, in __init__\n",
      "    validate_key(task_id)\n",
      "  File \"/home/ubuntu/.local/lib/python3.6/site-packages/airflow/utils/helpers.py\", line 61, in validate_key\n",
      "    \"dots and underscores exclusively\".format(**locals()))\n",
      "airflow.exceptions.AirflowException: The key () has to be made of alphanumeric characters, dashes, dots and underscores exclusively\n",
      "[2019-08-08 06:10:01 +0000] [4426] [INFO] Handling signal: ttou\n",
      "[2019-08-08 06:10:01 +0000] [4439] [INFO] Worker exiting (pid: 4439)\n"
     ]
    }
   ],
   "source": [
    "# airflow 웹서버 실행\n",
    "# putty에서 직접 실행\n",
    "!airflow webserver -p 8080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.6/site-packages/airflow/configuration.py:590: DeprecationWarning: You have two airflow.cfg files: /home/ubuntu/airflow/airflow.cfg and /home/ubuntu/project1/airflow/airflow.cfg. Airflow used to look at ~/airflow/airflow.cfg, even when AIRFLOW_HOME was set to a different value. Airflow will now only read /home/ubuntu/project1/airflow/airflow.cfg, and you should remove the other file\n",
      "  category=DeprecationWarning,\n",
      "[2019-08-08 06:07:04,026] {__init__.py:51} INFO - Using executor SequentialExecutor\n",
      "\n",
      "\tSuccessfully added `conn_id`=emr_cluster_conn : ssh://:@:22\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# connections 생성\n",
    "!airflow connections -a --conn_id emr_cluster_conn --conn_type SSH --conn_port 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-14T02:58:20.951030Z",
     "start_time": "2022-07-14T02:58:19.256057Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.6/site-packages/airflow/configuration.py:590: DeprecationWarning: You have two airflow.cfg files: /home/ubuntu/airflow/airflow.cfg and /home/ubuntu/project1/airflow/airflow.cfg. Airflow used to look at ~/airflow/airflow.cfg, even when AIRFLOW_HOME was set to a different value. Airflow will now only read /home/ubuntu/project1/airflow/airflow.cfg, and you should remove the other file\n",
      "  category=DeprecationWarning,\n",
      "[2022-07-14 02:58:20,099] {__init__.py:51} INFO - Using executor SequentialExecutor\n",
      "DB: sqlite:////home/ubuntu/project1/airflow/airflow.db\n",
      "[2022-07-14 02:58:20,390] {db.py:350} INFO - Creating tables\n",
      "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:387: DeprecationWarning: Importing 'SSHHook' directly from 'airflow.contrib.hooks' has been deprecated. Please import from 'airflow.contrib.hooks.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.\n",
      "  DeprecationWarning)\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# airflow DB 초기화\n",
    "!airflow initdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# airflow dags에 DAG파일 넣기\n",
    "!ln -s ~/Project/02_Data_Batch_Processing/02_Model_Training_And_Deployment/model_training_DAG.py ~/project1/airflow/dags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-14T03:08:55.998688Z",
     "start_time": "2022-07-14T02:59:35.971715Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.6/site-packages/airflow/configuration.py:590: DeprecationWarning: You have two airflow.cfg files: /home/ubuntu/airflow/airflow.cfg and /home/ubuntu/project1/airflow/airflow.cfg. Airflow used to look at ~/airflow/airflow.cfg, even when AIRFLOW_HOME was set to a different value. Airflow will now only read /home/ubuntu/project1/airflow/airflow.cfg, and you should remove the other file\n",
      "  category=DeprecationWarning,\n",
      "[2022-07-14 02:59:36,798] {__init__.py:51} INFO - Using executor SequentialExecutor\n",
      "[2022-07-14 02:59:37,104] {__init__.py:305} INFO - Filling up the DagBag from /home/ubuntu/project1/airflow/dags\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:387: DeprecationWarning: Importing 'SSHHook' directly from 'airflow.contrib.hooks' has been deprecated. Please import from 'airflow.contrib.hooks.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.\n",
      "  DeprecationWarning)\n",
      "[2022-07-14 02:59:37,367] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'DAG_model_training', 'emr_cluster_create', '2019-08-01T00:00:00+00:00', '--local', '-sd', 'DAGS_FOLDER/model_training_DAG.py', '--cfg_path', '/tmp/tmp1bxlaef6']\n",
      "[2022-07-14 02:59:42,205] {sequential_executor.py:45} INFO - Executing command: ['airflow', 'run', 'DAG_model_training', 'emr_cluster_create', '2019-08-01T00:00:00+00:00', '--local', '-sd', 'DAGS_FOLDER/model_training_DAG.py', '--cfg_path', '/tmp/tmp1bxlaef6']\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/airflow/configuration.py:590: DeprecationWarning: You have two airflow.cfg files: /home/ubuntu/airflow/airflow.cfg and /home/ubuntu/project1/airflow/airflow.cfg. Airflow used to look at ~/airflow/airflow.cfg, even when AIRFLOW_HOME was set to a different value. Airflow will now only read /home/ubuntu/project1/airflow/airflow.cfg, and you should remove the other file\n",
      "  category=DeprecationWarning,\n",
      "[2022-07-14 02:59:43,014] {__init__.py:51} INFO - Using executor SequentialExecutor\n",
      "[2022-07-14 02:59:43,252] {__init__.py:305} INFO - Filling up the DagBag from /home/ubuntu/project1/airflow/dags/model_training_DAG.py\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:387: DeprecationWarning: Importing 'SSHHook' directly from 'airflow.contrib.hooks' has been deprecated. Please import from 'airflow.contrib.hooks.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.\n",
      "  DeprecationWarning)\n",
      "[2022-07-14 02:59:43,307] {cli.py:517} INFO - Running <TaskInstance: DAG_model_training.emr_cluster_create 2019-08-01T00:00:00+00:00 [queued]> on host ip-172-31-30-20.ap-northeast-2.compute.internal\n",
      "[2022-07-14 03:06:09,075] {jobs.py:2124} INFO - [backfill progress] | finished run 0 of 1 | tasks waiting: 7 | succeeded: 1 | running: 0 | failed: 0 | skipped: 0 | deadlocked: 0 | not ready: 7\n",
      "[2022-07-14 03:06:09,088] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'DAG_model_training', 'emr_ssh_connect', '2019-08-01T00:00:00+00:00', '--local', '-sd', 'DAGS_FOLDER/model_training_DAG.py', '--cfg_path', '/tmp/tmpwbk5eva4']\n",
      "[2022-07-14 03:06:09,177] {sequential_executor.py:45} INFO - Executing command: ['airflow', 'run', 'DAG_model_training', 'emr_ssh_connect', '2019-08-01T00:00:00+00:00', '--local', '-sd', 'DAGS_FOLDER/model_training_DAG.py', '--cfg_path', '/tmp/tmpwbk5eva4']\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/airflow/configuration.py:590: DeprecationWarning: You have two airflow.cfg files: /home/ubuntu/airflow/airflow.cfg and /home/ubuntu/project1/airflow/airflow.cfg. Airflow used to look at ~/airflow/airflow.cfg, even when AIRFLOW_HOME was set to a different value. Airflow will now only read /home/ubuntu/project1/airflow/airflow.cfg, and you should remove the other file\n",
      "  category=DeprecationWarning,\n",
      "[2022-07-14 03:06:10,020] {__init__.py:51} INFO - Using executor SequentialExecutor\n",
      "[2022-07-14 03:06:10,266] {__init__.py:305} INFO - Filling up the DagBag from /home/ubuntu/project1/airflow/dags/model_training_DAG.py\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:387: DeprecationWarning: Importing 'SSHHook' directly from 'airflow.contrib.hooks' has been deprecated. Please import from 'airflow.contrib.hooks.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.\n",
      "  DeprecationWarning)\n",
      "[2022-07-14 03:06:10,318] {cli.py:517} INFO - Running <TaskInstance: DAG_model_training.emr_ssh_connect 2019-08-01T00:00:00+00:00 [queued]> on host ip-172-31-30-20.ap-northeast-2.compute.internal\n",
      "[2022-07-14 03:06:15,595] {jobs.py:2124} INFO - [backfill progress] | finished run 0 of 1 | tasks waiting: 6 | succeeded: 2 | running: 0 | failed: 0 | skipped: 0 | deadlocked: 0 | not ready: 6\n",
      "[2022-07-14 03:06:15,608] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'DAG_model_training', 'ip_forwarding', '2019-08-01T00:00:00+00:00', '--local', '-sd', 'DAGS_FOLDER/model_training_DAG.py', '--cfg_path', '/tmp/tmp21pk7j6p']\n",
      "[2022-07-14 03:06:15,684] {sequential_executor.py:45} INFO - Executing command: ['airflow', 'run', 'DAG_model_training', 'ip_forwarding', '2019-08-01T00:00:00+00:00', '--local', '-sd', 'DAGS_FOLDER/model_training_DAG.py', '--cfg_path', '/tmp/tmp21pk7j6p']\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/airflow/configuration.py:590: DeprecationWarning: You have two airflow.cfg files: /home/ubuntu/airflow/airflow.cfg and /home/ubuntu/project1/airflow/airflow.cfg. Airflow used to look at ~/airflow/airflow.cfg, even when AIRFLOW_HOME was set to a different value. Airflow will now only read /home/ubuntu/project1/airflow/airflow.cfg, and you should remove the other file\n",
      "  category=DeprecationWarning,\n",
      "[2022-07-14 03:06:16,510] {__init__.py:51} INFO - Using executor SequentialExecutor\n",
      "[2022-07-14 03:06:16,758] {__init__.py:305} INFO - Filling up the DagBag from /home/ubuntu/project1/airflow/dags/model_training_DAG.py\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:387: DeprecationWarning: Importing 'SSHHook' directly from 'airflow.contrib.hooks' has been deprecated. Please import from 'airflow.contrib.hooks.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.\n",
      "  DeprecationWarning)\n",
      "[2022-07-14 03:06:16,816] {cli.py:517} INFO - Running <TaskInstance: DAG_model_training.ip_forwarding 2019-08-01T00:00:00+00:00 [queued]> on host ip-172-31-30-20.ap-northeast-2.compute.internal\n",
      "[2022-07-14 03:06:22,068] {jobs.py:2124} INFO - [backfill progress] | finished run 0 of 1 | tasks waiting: 5 | succeeded: 3 | running: 0 | failed: 0 | skipped: 0 | deadlocked: 0 | not ready: 5\n",
      "[2022-07-14 03:06:22,081] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'DAG_model_training', 'traindata_s3_to_hdfs', '2019-08-01T00:00:00+00:00', '--local', '-sd', 'DAGS_FOLDER/model_training_DAG.py', '--cfg_path', '/tmp/tmpo12ryvqz']\n",
      "[2022-07-14 03:06:22,150] {sequential_executor.py:45} INFO - Executing command: ['airflow', 'run', 'DAG_model_training', 'traindata_s3_to_hdfs', '2019-08-01T00:00:00+00:00', '--local', '-sd', 'DAGS_FOLDER/model_training_DAG.py', '--cfg_path', '/tmp/tmpo12ryvqz']\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/airflow/configuration.py:590: DeprecationWarning: You have two airflow.cfg files: /home/ubuntu/airflow/airflow.cfg and /home/ubuntu/project1/airflow/airflow.cfg. Airflow used to look at ~/airflow/airflow.cfg, even when AIRFLOW_HOME was set to a different value. Airflow will now only read /home/ubuntu/project1/airflow/airflow.cfg, and you should remove the other file\n",
      "  category=DeprecationWarning,\n",
      "[2022-07-14 03:06:22,955] {__init__.py:51} INFO - Using executor SequentialExecutor\n",
      "[2022-07-14 03:06:23,202] {__init__.py:305} INFO - Filling up the DagBag from /home/ubuntu/project1/airflow/dags/model_training_DAG.py\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:387: DeprecationWarning: Importing 'SSHHook' directly from 'airflow.contrib.hooks' has been deprecated. Please import from 'airflow.contrib.hooks.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-07-14 03:06:23,253] {cli.py:517} INFO - Running <TaskInstance: DAG_model_training.traindata_s3_to_hdfs 2019-08-01T00:00:00+00:00 [queued]> on host ip-172-31-30-20.ap-northeast-2.compute.internal\n",
      "[2022-07-14 03:06:58,570] {jobs.py:2124} INFO - [backfill progress] | finished run 0 of 1 | tasks waiting: 4 | succeeded: 4 | running: 0 | failed: 0 | skipped: 0 | deadlocked: 0 | not ready: 4\n",
      "[2022-07-14 03:06:58,583] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'DAG_model_training', 'training_model', '2019-08-01T00:00:00+00:00', '--local', '-sd', 'DAGS_FOLDER/model_training_DAG.py', '--cfg_path', '/tmp/tmpw2bml5n7']\n",
      "[2022-07-14 03:06:58,640] {sequential_executor.py:45} INFO - Executing command: ['airflow', 'run', 'DAG_model_training', 'training_model', '2019-08-01T00:00:00+00:00', '--local', '-sd', 'DAGS_FOLDER/model_training_DAG.py', '--cfg_path', '/tmp/tmpw2bml5n7']\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/airflow/configuration.py:590: DeprecationWarning: You have two airflow.cfg files: /home/ubuntu/airflow/airflow.cfg and /home/ubuntu/project1/airflow/airflow.cfg. Airflow used to look at ~/airflow/airflow.cfg, even when AIRFLOW_HOME was set to a different value. Airflow will now only read /home/ubuntu/project1/airflow/airflow.cfg, and you should remove the other file\n",
      "  category=DeprecationWarning,\n",
      "[2022-07-14 03:06:59,470] {__init__.py:51} INFO - Using executor SequentialExecutor\n",
      "[2022-07-14 03:06:59,712] {__init__.py:305} INFO - Filling up the DagBag from /home/ubuntu/project1/airflow/dags/model_training_DAG.py\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:387: DeprecationWarning: Importing 'SSHHook' directly from 'airflow.contrib.hooks' has been deprecated. Please import from 'airflow.contrib.hooks.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.\n",
      "  DeprecationWarning)\n",
      "[2022-07-14 03:06:59,763] {cli.py:517} INFO - Running <TaskInstance: DAG_model_training.training_model 2019-08-01T00:00:00+00:00 [queued]> on host ip-172-31-30-20.ap-northeast-2.compute.internal\n",
      "[2022-07-14 03:08:05,142] {jobs.py:2124} INFO - [backfill progress] | finished run 0 of 1 | tasks waiting: 3 | succeeded: 5 | running: 0 | failed: 0 | skipped: 0 | deadlocked: 0 | not ready: 3\n",
      "[2022-07-14 03:08:05,158] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'DAG_model_training', 'trained_model_hdfs_to_s3', '2019-08-01T00:00:00+00:00', '--local', '-sd', 'DAGS_FOLDER/model_training_DAG.py', '--cfg_path', '/tmp/tmp4bem7spu']\n",
      "[2022-07-14 03:08:05,211] {sequential_executor.py:45} INFO - Executing command: ['airflow', 'run', 'DAG_model_training', 'trained_model_hdfs_to_s3', '2019-08-01T00:00:00+00:00', '--local', '-sd', 'DAGS_FOLDER/model_training_DAG.py', '--cfg_path', '/tmp/tmp4bem7spu']\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/airflow/configuration.py:590: DeprecationWarning: You have two airflow.cfg files: /home/ubuntu/airflow/airflow.cfg and /home/ubuntu/project1/airflow/airflow.cfg. Airflow used to look at ~/airflow/airflow.cfg, even when AIRFLOW_HOME was set to a different value. Airflow will now only read /home/ubuntu/project1/airflow/airflow.cfg, and you should remove the other file\n",
      "  category=DeprecationWarning,\n",
      "[2022-07-14 03:08:07,144] {__init__.py:51} INFO - Using executor SequentialExecutor\n",
      "[2022-07-14 03:08:07,456] {__init__.py:305} INFO - Filling up the DagBag from /home/ubuntu/project1/airflow/dags/model_training_DAG.py\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:387: DeprecationWarning: Importing 'SSHHook' directly from 'airflow.contrib.hooks' has been deprecated. Please import from 'airflow.contrib.hooks.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.\n",
      "  DeprecationWarning)\n",
      "[2022-07-14 03:08:07,522] {cli.py:517} INFO - Running <TaskInstance: DAG_model_training.trained_model_hdfs_to_s3 2019-08-01T00:00:00+00:00 [queued]> on host ip-172-31-30-20.ap-northeast-2.compute.internal\n",
      "[2022-07-14 03:08:42,834] {jobs.py:2124} INFO - [backfill progress] | finished run 0 of 1 | tasks waiting: 2 | succeeded: 6 | running: 0 | failed: 0 | skipped: 0 | deadlocked: 0 | not ready: 2\n",
      "[2022-07-14 03:08:42,847] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'DAG_model_training', 'trained_model_s3_to_server', '2019-08-01T00:00:00+00:00', '--local', '-sd', 'DAGS_FOLDER/model_training_DAG.py', '--cfg_path', '/tmp/tmp1ssz6ijc']\n",
      "[2022-07-14 03:08:42,873] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'DAG_model_training', 'emr_cluster_close', '2019-08-01T00:00:00+00:00', '--local', '-sd', 'DAGS_FOLDER/model_training_DAG.py', '--cfg_path', '/tmp/tmp19gpqzi2']\n",
      "[2022-07-14 03:08:42,906] {sequential_executor.py:45} INFO - Executing command: ['airflow', 'run', 'DAG_model_training', 'trained_model_s3_to_server', '2019-08-01T00:00:00+00:00', '--local', '-sd', 'DAGS_FOLDER/model_training_DAG.py', '--cfg_path', '/tmp/tmp1ssz6ijc']\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/airflow/configuration.py:590: DeprecationWarning: You have two airflow.cfg files: /home/ubuntu/airflow/airflow.cfg and /home/ubuntu/project1/airflow/airflow.cfg. Airflow used to look at ~/airflow/airflow.cfg, even when AIRFLOW_HOME was set to a different value. Airflow will now only read /home/ubuntu/project1/airflow/airflow.cfg, and you should remove the other file\n",
      "  category=DeprecationWarning,\n",
      "[2022-07-14 03:08:43,723] {__init__.py:51} INFO - Using executor SequentialExecutor\n",
      "[2022-07-14 03:08:43,965] {__init__.py:305} INFO - Filling up the DagBag from /home/ubuntu/project1/airflow/dags/model_training_DAG.py\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:387: DeprecationWarning: Importing 'SSHHook' directly from 'airflow.contrib.hooks' has been deprecated. Please import from 'airflow.contrib.hooks.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.\n",
      "  DeprecationWarning)\n",
      "[2022-07-14 03:08:44,018] {cli.py:517} INFO - Running <TaskInstance: DAG_model_training.trained_model_s3_to_server 2019-08-01T00:00:00+00:00 [queued]> on host ip-172-31-30-20.ap-northeast-2.compute.internal\n",
      "[2022-07-14 03:08:49,279] {sequential_executor.py:45} INFO - Executing command: ['airflow', 'run', 'DAG_model_training', 'emr_cluster_close', '2019-08-01T00:00:00+00:00', '--local', '-sd', 'DAGS_FOLDER/model_training_DAG.py', '--cfg_path', '/tmp/tmp19gpqzi2']\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/airflow/configuration.py:590: DeprecationWarning: You have two airflow.cfg files: /home/ubuntu/airflow/airflow.cfg and /home/ubuntu/project1/airflow/airflow.cfg. Airflow used to look at ~/airflow/airflow.cfg, even when AIRFLOW_HOME was set to a different value. Airflow will now only read /home/ubuntu/project1/airflow/airflow.cfg, and you should remove the other file\n",
      "  category=DeprecationWarning,\n",
      "[2022-07-14 03:08:50,126] {__init__.py:51} INFO - Using executor SequentialExecutor\n",
      "[2022-07-14 03:08:50,368] {__init__.py:305} INFO - Filling up the DagBag from /home/ubuntu/project1/airflow/dags/model_training_DAG.py\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:387: DeprecationWarning: Importing 'SSHHook' directly from 'airflow.contrib.hooks' has been deprecated. Please import from 'airflow.contrib.hooks.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.\n",
      "  DeprecationWarning)\n",
      "[2022-07-14 03:08:50,420] {cli.py:517} INFO - Running <TaskInstance: DAG_model_training.emr_cluster_close 2019-08-01T00:00:00+00:00 [queued]> on host ip-172-31-30-20.ap-northeast-2.compute.internal\n",
      "[2022-07-14 03:08:55,673] {__init__.py:4854} INFO - Marking run <DagRun DAG_model_training @ 2019-08-01 00:00:00+00:00: backfill_2019-08-01T00:00:00+00:00, externally triggered: False> successful\n",
      "[2022-07-14 03:08:55,689] {jobs.py:2124} INFO - [backfill progress] | finished run 1 of 1 | tasks waiting: 0 | succeeded: 8 | running: 0 | failed: 0 | skipped: 0 | deadlocked: 0 | not ready: 0\n",
      "[2022-07-14 03:08:55,690] {jobs.py:2495} INFO - Backfill done. Exiting.\n"
     ]
    }
   ],
   "source": [
    "# 전체 테스트 220714 에 성공한거 확인\n",
    "!airflow backfill -s 2019-08-01 -e 2019-08-01 DAG_model_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 테스트 기록 지우기\n",
    "# putty에서 실행\n",
    "airflow clear -s 2019-08-01 -e 2019-08-01 DAG_model_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.6/site-packages/airflow/configuration.py:590: DeprecationWarning: You have two airflow.cfg files: /home/ubuntu/airflow/airflow.cfg and /home/ubuntu/project1/airflow/airflow.cfg. Airflow used to look at ~/airflow/airflow.cfg, even when AIRFLOW_HOME was set to a different value. Airflow will now only read /home/ubuntu/project1/airflow/airflow.cfg, and you should remove the other file\n",
      "  category=DeprecationWarning,\n",
      "[2019-08-17 01:04:09,058] {__init__.py:51} INFO - Using executor SequentialExecutor\n",
      "[2019-08-17 01:04:09,283] {__init__.py:305} INFO - Filling up the DagBag from /home/ubuntu/project1/airflow/dags\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:387: DeprecationWarning: Importing 'SSHHook' directly from 'airflow.contrib.hooks' has been deprecated. Please import from 'airflow.contrib.hooks.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.\n",
      "  DeprecationWarning)\n",
      "[2019-08-17 01:04:09,338] {__init__.py:1139} INFO - Dependencies all met for <TaskInstance: DAG_model_training.emr_cluster_create 2019-08-01T00:00:00+00:00 [success]>\n",
      "[2019-08-17 01:04:09,342] {__init__.py:1139} INFO - Dependencies all met for <TaskInstance: DAG_model_training.emr_cluster_create 2019-08-01T00:00:00+00:00 [success]>\n",
      "[2019-08-17 01:04:09,342] {__init__.py:1353} INFO - \n",
      "--------------------------------------------------------------------------------\n",
      "[2019-08-17 01:04:09,342] {__init__.py:1354} INFO - Starting attempt 2 of 1\n",
      "[2019-08-17 01:04:09,342] {__init__.py:1355} INFO - \n",
      "--------------------------------------------------------------------------------\n",
      "[2019-08-17 01:04:09,342] {__init__.py:1374} INFO - Executing <Task(BashOperator): emr_cluster_create> on 2019-08-01T00:00:00+00:00\n",
      "[2019-08-17 01:04:09,370] {bash_operator.py:81} INFO - Tmp dir root location: \n",
      " /tmp\n",
      "[2019-08-17 01:04:09,371] {bash_operator.py:90} INFO - Exporting the following env vars:\n",
      "AIRFLOW_CTX_DAG_ID=DAG_model_training\n",
      "AIRFLOW_CTX_TASK_ID=emr_cluster_create\n",
      "AIRFLOW_CTX_EXECUTION_DATE=2019-08-01T00:00:00+00:00\n",
      "AIRFLOW_CTX_DAG_RUN_ID=backfill_2019-08-01T00:00:00+00:00\n",
      "[2019-08-17 01:04:09,371] {bash_operator.py:104} INFO - Temporary script location: /tmp/airflowtmpaak2r8gl/emr_cluster_createka5y_al1\n",
      "[2019-08-17 01:04:09,371] {bash_operator.py:114} INFO - Running command: bash ~/Project/02_Data_Batch_Processing/shell_script/emr_cluster_create.sh 172.31.20.77\n",
      "[2019-08-17 01:04:09,375] {bash_operator.py:123} INFO - Output:\n",
      "[2019-08-17 01:04:10,233] {bash_operator.py:127} INFO - EMR 클러스터 생성이 완료 될 때까지 시간이 필요합니다.\n",
      "[2019-08-17 01:04:11,014] {bash_operator.py:127} INFO - EMR 클러스터 생성 중...\n",
      "[2019-08-17 01:04:16,886] {bash_operator.py:127} INFO - EMR 클러스터 생성 중...\n",
      "[2019-08-17 01:04:22,635] {bash_operator.py:127} INFO - EMR 클러스터 생성 중...\n",
      "[2019-08-17 01:04:28,391] {bash_operator.py:127} INFO - EMR 클러스터 생성 중...\n",
      "[2019-08-17 01:04:34,131] {bash_operator.py:127} INFO - EMR 클러스터 생성 중...\n",
      "[2019-08-17 01:04:39,867] {bash_operator.py:127} INFO - EMR 클러스터 생성 중...\n",
      "[2019-08-17 01:04:45,608] {bash_operator.py:127} INFO - EMR 클러스터 생성 중...\n",
      "[2019-08-17 01:04:51,357] {bash_operator.py:127} INFO - EMR 클러스터 생성 중...\n",
      "[2019-08-17 01:04:57,107] {bash_operator.py:127} INFO - EMR 클러스터 생성 중...\n",
      "[2019-08-17 01:05:02,888] {bash_operator.py:127} INFO - EMR 클러스터 생성 중...\n",
      "[2019-08-17 01:05:08,694] {bash_operator.py:127} INFO - EMR 클러스터 생성 중...\n",
      "[2019-08-17 01:05:14,459] {bash_operator.py:127} INFO - EMR 클러스터 생성 중...\n",
      "[2019-08-17 01:05:20,219] {bash_operator.py:127} INFO - EMR 클러스터 생성 중...\n",
      "[2019-08-17 01:05:25,979] {bash_operator.py:127} INFO - EMR 클러스터 생성 중...\n",
      "[2019-08-17 01:05:31,731] {bash_operator.py:127} INFO - EMR 클러스터 생성 중...\n",
      "[2019-08-17 01:05:37,479] {bash_operator.py:127} INFO - EMR 클러스터 생성 중...\n",
      "[2019-08-17 01:05:43,239] {bash_operator.py:127} INFO - EMR 클러스터 생성 중...\n",
      "[2019-08-17 01:05:49,020] {bash_operator.py:127} INFO - EMR 클러스터 생성 중...\n",
      "[2019-08-17 01:05:54,766] {bash_operator.py:127} INFO - EMR 클러스터 생성 중...\n",
      "[2019-08-17 01:06:00,519] {bash_operator.py:127} INFO - EMR 클러스터 생성 중...\n",
      "[2019-08-17 01:06:06,258] {bash_operator.py:127} INFO - EMR 클러스터 생성 중...\n",
      "[2019-08-17 01:06:12,002] {bash_operator.py:127} INFO - EMR 클러스터 생성 중...\n",
      "[2019-08-17 01:06:17,757] {bash_operator.py:127} INFO - EMR 클러스터 생성 중...\n",
      "[2019-08-17 01:06:23,512] {bash_operator.py:127} INFO - EMR 클러스터 생성 중...\n",
      "[2019-08-17 01:06:29,273] {bash_operator.py:127} INFO - EMR 클러스터 생성 중...\n",
      "[2019-08-17 01:06:35,031] {bash_operator.py:127} INFO - EMR 클러스터 생성 중...\n",
      "[2019-08-17 01:06:40,764] {bash_operator.py:127} INFO - EMR 클러스터 생성 중...\n",
      "[2019-08-17 01:06:46,506] {bash_operator.py:127} INFO - EMR 클러스터 생성 중...\n",
      "[2019-08-17 01:06:52,259] {bash_operator.py:127} INFO - EMR 클러스터 생성 중...\n",
      "[2019-08-17 01:06:57,993] {bash_operator.py:127} INFO - EMR 클러스터 생성 중...\n",
      "[2019-08-17 01:07:04,023] {bash_operator.py:127} INFO - EMR 클러스터 생성 중...\n",
      "[2019-08-17 01:07:09,784] {bash_operator.py:127} INFO - EMR 클러스터 생성 중...\n",
      "[2019-08-17 01:07:15,540] {bash_operator.py:127} INFO - EMR 클러스터 생성 중...\n",
      "[2019-08-17 01:07:21,300] {bash_operator.py:127} INFO - EMR 클러스터 생성 중...\n",
      "[2019-08-17 01:07:27,060] {bash_operator.py:127} INFO - EMR 클러스터 생성 중...\n",
      "[2019-08-17 01:07:32,852] {bash_operator.py:127} INFO - EMR 클러스터 생성 중...\n",
      "[2019-08-17 01:07:38,605] {bash_operator.py:127} INFO - EMR 클러스터 생성 중...\n",
      "[2019-08-17 01:07:44,345] {bash_operator.py:127} INFO - EMR 클러스터 생성 중...\n",
      "[2019-08-17 01:07:50,085] {bash_operator.py:127} INFO - EMR 클러스터 생성 중...\n",
      "[2019-08-17 01:07:55,819] {bash_operator.py:127} INFO - EMR 클러스터 생성 중...\n",
      "[2019-08-17 01:08:01,567] {bash_operator.py:127} INFO - EMR 클러스터 생성 중...\n",
      "[2019-08-17 01:08:07,594] {bash_operator.py:127} INFO - EMR 클러스터 생성 중...\n",
      "[2019-08-17 01:08:13,355] {bash_operator.py:127} INFO - EMR 클러스터 생성 중...\n",
      "[2019-08-17 01:08:19,109] {bash_operator.py:127} INFO - EMR 클러스터 생성 중...\n",
      "[2019-08-17 01:08:24,842] {bash_operator.py:127} INFO - EMR 클러스터 생성 중...\n",
      "[2019-08-17 01:08:30,589] {bash_operator.py:127} INFO - EMR 클러스터 생성 중...\n",
      "[2019-08-17 01:08:36,372] {bash_operator.py:127} INFO - EMR 클러스터 생성 중...\n",
      "[2019-08-17 01:08:42,120] {bash_operator.py:127} INFO - EMR 클러스터 생성 중...\n",
      "[2019-08-17 01:08:47,864] {bash_operator.py:127} INFO - EMR 클러스터 생성 중...\n",
      "[2019-08-17 01:08:53,596] {bash_operator.py:127} INFO - EMR 클러스터 생성 중...\n",
      "[2019-08-17 01:08:59,349] {bash_operator.py:127} INFO - EMR 클러스터 생성 중...\n",
      "[2019-08-17 01:09:05,094] {bash_operator.py:127} INFO - EMR 클러스터 생성 중...\n",
      "[2019-08-17 01:09:10,840] {bash_operator.py:127} INFO - EMR 클러스터 생성 중...\n",
      "[2019-08-17 01:09:16,604] {bash_operator.py:127} INFO - EMR 클러스터 생성 중...\n",
      "[2019-08-17 01:09:22,365] {bash_operator.py:127} INFO - EMR 클러스터 생성 중...\n",
      "[2019-08-17 01:09:28,109] {bash_operator.py:127} INFO - EMR 클러스터 생성 중...\n",
      "[2019-08-17 01:09:33,862] {bash_operator.py:127} INFO - EMR 클러스터 생성 중...\n",
      "[2019-08-17 01:09:39,638] {bash_operator.py:127} INFO - EMR 클러스터 생성 중...\n",
      "[2019-08-17 01:09:45,377] {bash_operator.py:127} INFO - EMR 클러스터 생성 중...\n",
      "[2019-08-17 01:09:51,121] {bash_operator.py:127} INFO - EMR 클러스터 생성 중...\n",
      "[2019-08-17 01:09:56,926] {bash_operator.py:127} INFO - EMR 클러스터 생성 중...\n",
      "[2019-08-17 01:10:02,717] {bash_operator.py:127} INFO - EMR 클러스터 생성 중...\n",
      "[2019-08-17 01:10:08,747] {bash_operator.py:127} INFO - EMR 클러스터 생성 중...\n",
      "[2019-08-17 01:10:14,503] {bash_operator.py:127} INFO - EMR 클러스터 생성 중...\n",
      "[2019-08-17 01:10:20,260] {bash_operator.py:127} INFO - EMR 클러스터 생성 중...\n",
      "[2019-08-17 01:10:26,059] {bash_operator.py:127} INFO - EMR 클러스터 생성 중...\n",
      "[2019-08-17 01:10:31,809] {bash_operator.py:127} INFO - EMR 클러스터 생성 중...\n",
      "[2019-08-17 01:10:31,810] {bash_operator.py:127} INFO - EMR 클러스터 생성 완료\n",
      "[2019-08-17 01:10:31,811] {bash_operator.py:127} INFO - j-2ZLYN842IOJGA\n",
      "[2019-08-17 01:10:31,811] {bash_operator.py:131} INFO - Command exited with return code 0\n"
     ]
    }
   ],
   "source": [
    "# 작업 단위로 테스트 하기\n",
    "!airflow test DAG_model_training emr_cluster_create 2019-08-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.6/site-packages/airflow/configuration.py:590: DeprecationWarning: You have two airflow.cfg files: /home/ubuntu/airflow/airflow.cfg and /home/ubuntu/project1/airflow/airflow.cfg. Airflow used to look at ~/airflow/airflow.cfg, even when AIRFLOW_HOME was set to a different value. Airflow will now only read /home/ubuntu/project1/airflow/airflow.cfg, and you should remove the other file\n",
      "  category=DeprecationWarning,\n",
      "[2019-08-17 01:10:55,289] {__init__.py:51} INFO - Using executor SequentialExecutor\n",
      "[2019-08-17 01:10:55,503] {__init__.py:305} INFO - Filling up the DagBag from /home/ubuntu/project1/airflow/dags\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:387: DeprecationWarning: Importing 'SSHHook' directly from 'airflow.contrib.hooks' has been deprecated. Please import from 'airflow.contrib.hooks.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.\n",
      "  DeprecationWarning)\n",
      "[2019-08-17 01:10:55,557] {__init__.py:1139} INFO - Dependencies all met for <TaskInstance: DAG_model_training.emr_ssh_connect 2019-08-01T00:00:00+00:00 [success]>\n",
      "[2019-08-17 01:10:55,561] {__init__.py:1139} INFO - Dependencies all met for <TaskInstance: DAG_model_training.emr_ssh_connect 2019-08-01T00:00:00+00:00 [success]>\n",
      "[2019-08-17 01:10:55,561] {__init__.py:1353} INFO - \n",
      "--------------------------------------------------------------------------------\n",
      "[2019-08-17 01:10:55,561] {__init__.py:1354} INFO - Starting attempt 2 of 1\n",
      "[2019-08-17 01:10:55,561] {__init__.py:1355} INFO - \n",
      "--------------------------------------------------------------------------------\n",
      "[2019-08-17 01:10:55,562] {__init__.py:1374} INFO - Executing <Task(BashOperator): emr_ssh_connect> on 2019-08-01T00:00:00+00:00\n",
      "[2019-08-17 01:10:55,577] {bash_operator.py:81} INFO - Tmp dir root location: \n",
      " /tmp\n",
      "[2019-08-17 01:10:55,577] {bash_operator.py:90} INFO - Exporting the following env vars:\n",
      "AIRFLOW_CTX_DAG_ID=DAG_model_training\n",
      "AIRFLOW_CTX_TASK_ID=emr_ssh_connect\n",
      "AIRFLOW_CTX_EXECUTION_DATE=2019-08-01T00:00:00+00:00\n",
      "AIRFLOW_CTX_DAG_RUN_ID=backfill_2019-08-01T00:00:00+00:00\n",
      "[2019-08-17 01:10:55,577] {bash_operator.py:104} INFO - Temporary script location: /tmp/airflowtmpioaw8cid/emr_ssh_connectwmtaqqbp\n",
      "[2019-08-17 01:10:55,577] {bash_operator.py:114} INFO - Running command: bash ~/Project/02_Data_Batch_Processing/shell_script/emr_ssh_connect.sh 172.31.20.77 ip-172-31-20-77.ap-northeast-2.compute.internal\n",
      "[2019-08-17 01:10:55,582] {bash_operator.py:123} INFO - Output:\n",
      "[2019-08-17 01:10:55,584] {bash_operator.py:127} INFO - ssh 접속 중..\n",
      "[2019-08-17 01:10:55,586] {bash_operator.py:127} INFO - # Host 172.31.20.77 found: line 6\n",
      "[2019-08-17 01:10:55,586] {bash_operator.py:127} INFO - /home/ubuntu/.ssh/known_hosts updated.\n",
      "[2019-08-17 01:10:55,586] {bash_operator.py:127} INFO - Original contents retained as /home/ubuntu/.ssh/known_hosts.old\n",
      "[2019-08-17 01:10:55,588] {bash_operator.py:127} INFO - # Host ip-172-31-20-77.ap-northeast-2.compute.internal found: line 5\n",
      "[2019-08-17 01:10:55,588] {bash_operator.py:127} INFO - /home/ubuntu/.ssh/known_hosts updated.\n",
      "[2019-08-17 01:10:55,588] {bash_operator.py:127} INFO - Original contents retained as /home/ubuntu/.ssh/known_hosts.old\n",
      "[2019-08-17 01:10:55,616] {bash_operator.py:127} INFO - Warning: Permanently added 'ip-172-31-20-77.ap-northeast-2.compute.internal,172.31.20.77' (ECDSA) to the list of known hosts.\n",
      "[2019-08-17 01:10:55,764] {bash_operator.py:127} INFO - SSH 성공\n",
      "[2019-08-17 01:10:55,764] {bash_operator.py:131} INFO - Command exited with return code 0\n"
     ]
    }
   ],
   "source": [
    "!airflow test DAG_model_training emr_ssh_connect 2019-08-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.6/site-packages/airflow/configuration.py:590: DeprecationWarning: You have two airflow.cfg files: /home/ubuntu/airflow/airflow.cfg and /home/ubuntu/project1/airflow/airflow.cfg. Airflow used to look at ~/airflow/airflow.cfg, even when AIRFLOW_HOME was set to a different value. Airflow will now only read /home/ubuntu/project1/airflow/airflow.cfg, and you should remove the other file\n",
      "  category=DeprecationWarning,\n",
      "[2019-08-17 01:10:59,379] {__init__.py:51} INFO - Using executor SequentialExecutor\n",
      "[2019-08-17 01:10:59,593] {__init__.py:305} INFO - Filling up the DagBag from /home/ubuntu/project1/airflow/dags\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:387: DeprecationWarning: Importing 'SSHHook' directly from 'airflow.contrib.hooks' has been deprecated. Please import from 'airflow.contrib.hooks.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.\n",
      "  DeprecationWarning)\n",
      "[2019-08-17 01:10:59,648] {__init__.py:1139} INFO - Dependencies all met for <TaskInstance: DAG_model_training.ip_forwarding 2019-08-01T00:00:00+00:00 [success]>\n",
      "[2019-08-17 01:10:59,652] {__init__.py:1139} INFO - Dependencies all met for <TaskInstance: DAG_model_training.ip_forwarding 2019-08-01T00:00:00+00:00 [success]>\n",
      "[2019-08-17 01:10:59,652] {__init__.py:1353} INFO - \n",
      "--------------------------------------------------------------------------------\n",
      "[2019-08-17 01:10:59,652] {__init__.py:1354} INFO - Starting attempt 2 of 1\n",
      "[2019-08-17 01:10:59,652] {__init__.py:1355} INFO - \n",
      "--------------------------------------------------------------------------------\n",
      "[2019-08-17 01:10:59,652] {__init__.py:1374} INFO - Executing <Task(SSHOperator): ip_forwarding> on 2019-08-01T00:00:00+00:00\n",
      "[2019-08-17 01:10:59,679] {ssh_hook.py:154} WARNING - Remote Identification Change is not verified. This wont protect against Man-In-The-Middle attacks\n",
      "[2019-08-17 01:10:59,681] {ssh_hook.py:158} WARNING - No Host Key Verification. This wont protect against Man-In-The-Middle attacks\n",
      "[2019-08-17 01:10:59,687] {transport.py:1819} INFO - Connected (version 2.0, client OpenSSH_7.4)\n",
      "[2019-08-17 01:10:59,763] {transport.py:1819} INFO - Authentication (password) successful!\n",
      "[2019-08-17 01:10:59,841] {ssh_operator.py:133} INFO - 172.31.16.7  ip-172-31-20-77.ap-northeast-2.compute.internal\n"
     ]
    }
   ],
   "source": [
    "!airflow test DAG_model_training ip_forwarding 2019-08-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.6/site-packages/airflow/configuration.py:590: DeprecationWarning: You have two airflow.cfg files: /home/ubuntu/airflow/airflow.cfg and /home/ubuntu/project1/airflow/airflow.cfg. Airflow used to look at ~/airflow/airflow.cfg, even when AIRFLOW_HOME was set to a different value. Airflow will now only read /home/ubuntu/project1/airflow/airflow.cfg, and you should remove the other file\n",
      "  category=DeprecationWarning,\n",
      "[2019-08-17 03:14:42,297] {__init__.py:51} INFO - Using executor SequentialExecutor\n",
      "[2019-08-17 03:14:42,516] {__init__.py:305} INFO - Filling up the DagBag from /home/ubuntu/project1/airflow/dags\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:387: DeprecationWarning: Importing 'SSHHook' directly from 'airflow.contrib.hooks' has been deprecated. Please import from 'airflow.contrib.hooks.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.\n",
      "  DeprecationWarning)\n",
      "[2019-08-17 03:14:42,571] {__init__.py:1139} INFO - Dependencies all met for <TaskInstance: DAG_model_training.traindata_s3_to_hdfs 2019-08-01T00:00:00+00:00 [success]>\n",
      "[2019-08-17 03:14:42,576] {__init__.py:1139} INFO - Dependencies all met for <TaskInstance: DAG_model_training.traindata_s3_to_hdfs 2019-08-01T00:00:00+00:00 [success]>\n",
      "[2019-08-17 03:14:42,576] {__init__.py:1353} INFO - \n",
      "--------------------------------------------------------------------------------\n",
      "[2019-08-17 03:14:42,576] {__init__.py:1354} INFO - Starting attempt 2 of 1\n",
      "[2019-08-17 03:14:42,576] {__init__.py:1355} INFO - \n",
      "--------------------------------------------------------------------------------\n",
      "[2019-08-17 03:14:42,576] {__init__.py:1374} INFO - Executing <Task(SSHOperator): traindata_s3_to_hdfs> on 2019-08-01T00:00:00+00:00\n",
      "[2019-08-17 03:14:42,604] {ssh_hook.py:154} WARNING - Remote Identification Change is not verified. This wont protect against Man-In-The-Middle attacks\n",
      "[2019-08-17 03:14:42,606] {ssh_hook.py:158} WARNING - No Host Key Verification. This wont protect against Man-In-The-Middle attacks\n",
      "[2019-08-17 03:14:42,619] {transport.py:1819} INFO - Connected (version 2.0, client OpenSSH_7.4)\n",
      "[2019-08-17 03:14:42,750] {transport.py:1819} INFO - Authentication (password) successful!\n",
      "[2019-08-17 03:14:43,393] {ssh_operator.py:138} WARNING - 19/08/17 03:14:43 INFO s3distcp.S3DistCp: Running with args: -libjars /usr/share/aws/emr/s3-dist-cp/lib/commons-httpclient-3.1.jar,/usr/share/aws/emr/s3-dist-cp/lib/commons-logging-1.0.4.jar,/usr/share/aws/emr/s3-dist-cp/lib/guava-18.0.jar,/usr/share/aws/emr/s3-dist-cp/lib/s3-dist-cp-2.11.0.jar,/usr/share/aws/emr/s3-dist-cp/lib/s3-dist-cp.jar --src s3://jhw620/RefineData/ --dest hdfs://ip-172-31-20-77.ap-northeast-2.compute.internal:8020/data/ --srcPattern .*[^_$folder$]$ \n",
      "[2019-08-17 03:14:44,416] {ssh_operator.py:138} WARNING - 19/08/17 03:14:44 INFO s3distcp.S3DistCp: S3DistCp args: --src s3://jhw620/RefineData/ --dest hdfs://ip-172-31-20-77.ap-northeast-2.compute.internal:8020/data/ --srcPattern .*[^_$folder$]$ \n",
      "[2019-08-17 03:14:44,427] {ssh_operator.py:138} WARNING - 19/08/17 03:14:44 INFO s3distcp.S3DistCp: Using output path 'hdfs:/tmp/61c4047d-7247-477c-af0f-4b27c97febf1/output'\n",
      "[2019-08-17 03:14:44,513] {ssh_operator.py:138} WARNING - 19/08/17 03:14:44 INFO s3distcp.S3DistCp: GET http://169.254.169.254/latest/meta-data/placement/availability-zone result: ap-northeast-2c\n",
      "[2019-08-17 03:14:48,212] {ssh_operator.py:138} WARNING - 19/08/17 03:14:48 INFO s3distcp.S3DistCp: DefaultAWSCredentialsProviderChain is used to create AmazonS3Client. KeyId: AKIA4K43PB6OKAXVTJUA\n",
      "19/08/17 03:14:48 INFO s3distcp.S3DistCp: AmazonS3Client setEndpoint s3.ap-northeast-2.amazonaws.com\n",
      "[2019-08-17 03:14:48,429] {ssh_operator.py:138} WARNING - 19/08/17 03:14:48 INFO s3distcp.S3DistCp: Skipping key 'RefineData/' because it ends with '/'\n",
      "19/08/17 03:14:48 INFO s3distcp.S3DistCp: Skipping key 'RefineData/2019-07-10/' because it ends with '/'\n",
      "[2019-08-17 03:14:48,429] {ssh_operator.py:138} WARNING - 19/08/17 03:14:48 INFO s3distcp.FileInfoListing: Opening new file: hdfs:/tmp/61c4047d-7247-477c-af0f-4b27c97febf1/files/1\n",
      "[2019-08-17 03:14:48,507] {ssh_operator.py:138} WARNING - 19/08/17 03:14:48 INFO s3distcp.S3DistCp: Created 1 files to copy 1 files \n",
      "[2019-08-17 03:14:48,613] {ssh_operator.py:138} WARNING - 19/08/17 03:14:48 INFO s3distcp.S3DistCp: Reducer number: 7\n",
      "[2019-08-17 03:14:48,749] {ssh_operator.py:138} WARNING - 19/08/17 03:14:48 INFO client.RMProxy: Connecting to ResourceManager at ip-172-31-16-7.ap-northeast-2.compute.internal/172.31.16.7:8032\n",
      "[2019-08-17 03:14:49,057] {ssh_operator.py:138} WARNING - 19/08/17 03:14:49 INFO input.FileInputFormat: Total input files to process : 1\n",
      "[2019-08-17 03:14:49,086] {ssh_operator.py:138} WARNING - 19/08/17 03:14:49 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "[2019-08-17 03:14:49,206] {ssh_operator.py:138} WARNING - 19/08/17 03:14:49 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1566004128108_0032\n",
      "[2019-08-17 03:14:49,443] {ssh_operator.py:138} WARNING - 19/08/17 03:14:49 INFO impl.YarnClientImpl: Submitted application application_1566004128108_0032\n",
      "[2019-08-17 03:14:49,481] {ssh_operator.py:138} WARNING - 19/08/17 03:14:49 INFO mapreduce.Job: The url to track the job: http://ip-172-31-16-7.ap-northeast-2.compute.internal:20888/proxy/application_1566004128108_0032/\n",
      "[2019-08-17 03:14:49,481] {ssh_operator.py:138} WARNING - 19/08/17 03:14:49 INFO mapreduce.Job: Running job: job_1566004128108_0032\n",
      "[2019-08-17 03:14:56,578] {ssh_operator.py:138} WARNING - 19/08/17 03:14:56 INFO mapreduce.Job: Job job_1566004128108_0032 running in uber mode : false\n",
      "[2019-08-17 03:14:56,579] {ssh_operator.py:138} WARNING - 19/08/17 03:14:56 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "[2019-08-17 03:15:01,625] {ssh_operator.py:138} WARNING - 19/08/17 03:15:01 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "[2019-08-17 03:15:09,677] {ssh_operator.py:138} WARNING - 19/08/17 03:15:09 INFO mapreduce.Job:  map 100% reduce 14%\n",
      "[2019-08-17 03:15:10,684] {ssh_operator.py:138} WARNING - 19/08/17 03:15:10 INFO mapreduce.Job:  map 100% reduce 43%\n",
      "[2019-08-17 03:15:11,688] {ssh_operator.py:138} WARNING - 19/08/17 03:15:11 INFO mapreduce.Job:  map 100% reduce 57%\n",
      "[2019-08-17 03:15:14,703] {ssh_operator.py:138} WARNING - 19/08/17 03:15:14 INFO mapreduce.Job:  map 100% reduce 71%\n",
      "[2019-08-17 03:15:15,710] {ssh_operator.py:138} WARNING - 19/08/17 03:15:15 INFO mapreduce.Job:  map 100% reduce 86%\n",
      "[2019-08-17 03:15:17,719] {ssh_operator.py:138} WARNING - 19/08/17 03:15:17 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "[2019-08-17 03:15:17,726] {ssh_operator.py:138} WARNING - 19/08/17 03:15:17 INFO mapreduce.Job: Job job_1566004128108_0032 completed successfully\n",
      "[2019-08-17 03:15:17,843] {ssh_operator.py:138} WARNING - 19/08/17 03:15:17 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=285\n",
      "\t\tFILE: Number of bytes written=1403941\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=462\n",
      "\t\tHDFS: Number of bytes written=35925\n",
      "\t\tHDFS: Number of read operations=27\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=15\n",
      "\t\tS3: Number of bytes read=35925\n",
      "\t\tS3: Number of bytes written=0\n",
      "\t\tS3: Number of read operations=0\n",
      "\t\tS3: Number of large read operations=0\n",
      "\t\tS3: Number of write operations=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=1\n",
      "\t\tLaunched reduce tasks=7\n",
      "\t\tData-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=289340\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=8772837\n",
      "\t\tTotal time spent by all map tasks (ms)=3145\n",
      "\t\tTotal time spent by all reduce tasks (ms)=47939\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=3145\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=47939\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=9208560\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=280730784\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=1\n",
      "\t\tMap output records=1\n",
      "\t\tMap output bytes=212\n",
      "\t\tMap output materialized bytes=257\n",
      "\t\tInput split bytes=172\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce shuffle bytes=257\n",
      "\t\tReduce input records=1\n",
      "\t\tReduce output records=0\n",
      "\t\tSpilled Records=2\n",
      "\t\tShuffled Maps =7\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=7\n",
      "\t\tGC time elapsed (ms)=1044\n",
      "\t\tCPU time spent (ms)=13670\n",
      "\t\tPhysical memory (bytes) snapshot=2565718016\n",
      "\t\tVirtual memory (bytes) snapshot=53907222528\n",
      "\t\tTotal committed heap usage (bytes)=3575644160\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=290\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=0\n",
      "[2019-08-17 03:15:17,844] {ssh_operator.py:138} WARNING - 19/08/17 03:15:17 INFO s3distcp.S3DistCp: Try to recursively delete hdfs:/tmp/61c4047d-7247-477c-af0f-4b27c97febf1\n"
     ]
    }
   ],
   "source": [
    "!airflow test DAG_model_training traindata_s3_to_hdfs 2019-08-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.6/site-packages/airflow/configuration.py:590: DeprecationWarning: You have two airflow.cfg files: /home/ubuntu/airflow/airflow.cfg and /home/ubuntu/project1/airflow/airflow.cfg. Airflow used to look at ~/airflow/airflow.cfg, even when AIRFLOW_HOME was set to a different value. Airflow will now only read /home/ubuntu/project1/airflow/airflow.cfg, and you should remove the other file\n",
      "  category=DeprecationWarning,\n",
      "[2019-08-17 03:15:37,245] {__init__.py:51} INFO - Using executor SequentialExecutor\n",
      "[2019-08-17 03:15:37,466] {__init__.py:305} INFO - Filling up the DagBag from /home/ubuntu/project1/airflow/dags\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:387: DeprecationWarning: Importing 'SSHHook' directly from 'airflow.contrib.hooks' has been deprecated. Please import from 'airflow.contrib.hooks.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.\n",
      "  DeprecationWarning)\n",
      "[2019-08-17 03:15:37,522] {__init__.py:1139} INFO - Dependencies all met for <TaskInstance: DAG_model_training.training_model 2019-08-01T00:00:00+00:00 [failed]>\n",
      "[2019-08-17 03:15:37,526] {__init__.py:1139} INFO - Dependencies all met for <TaskInstance: DAG_model_training.training_model 2019-08-01T00:00:00+00:00 [failed]>\n",
      "[2019-08-17 03:15:37,526] {__init__.py:1353} INFO - \n",
      "--------------------------------------------------------------------------------\n",
      "[2019-08-17 03:15:37,526] {__init__.py:1354} INFO - Starting attempt 2 of 1\n",
      "[2019-08-17 03:15:37,526] {__init__.py:1355} INFO - \n",
      "--------------------------------------------------------------------------------\n",
      "[2019-08-17 03:15:37,527] {__init__.py:1374} INFO - Executing <Task(BashOperator): training_model> on 2019-08-01T00:00:00+00:00\n",
      "[2019-08-17 03:15:37,543] {bash_operator.py:81} INFO - Tmp dir root location: \n",
      " /tmp\n",
      "[2019-08-17 03:15:37,543] {bash_operator.py:90} INFO - Exporting the following env vars:\n",
      "AIRFLOW_CTX_DAG_ID=DAG_model_training\n",
      "AIRFLOW_CTX_TASK_ID=training_model\n",
      "AIRFLOW_CTX_EXECUTION_DATE=2019-08-01T00:00:00+00:00\n",
      "AIRFLOW_CTX_DAG_RUN_ID=backfill_2019-08-01T00:00:00+00:00\n",
      "[2019-08-17 03:15:37,544] {bash_operator.py:104} INFO - Temporary script location: /tmp/airflowtmpc5vgkd_r/training_model1r6jpbli\n",
      "[2019-08-17 03:15:37,544] {bash_operator.py:114} INFO - Running command: \n",
      "export HADOOP_CONF_DIR='/home/ubuntu/project1/hadoop/etc/hadoop-model';\n",
      "export YARN_CONF_DIR='/home/ubuntu/project1/hadoop/etc/hadoop-model';\n",
      "spark-submit --deploy-mode client --master yarn --num-executors 4 --executor-cores 5 --executor-memory 18G ~/Project/02_Data_Batch_Processing/02_Model_Training_And_Deployment/model_training.py\n",
      "[2019-08-17 03:15:37,548] {bash_operator.py:123} INFO - Output:\n",
      "[2019-08-17 03:15:38,464] {bash_operator.py:127} INFO - Ivy Default Cache set to: /home/ubuntu/.ivy2/cache\n",
      "[2019-08-17 03:15:38,464] {bash_operator.py:127} INFO - The jars for the packages stored in: /home/ubuntu/.ivy2/jars\n",
      "[2019-08-17 03:15:38,494] {bash_operator.py:127} INFO - :: loading settings :: url = jar:file:/home/ubuntu/project1/spark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "[2019-08-17 03:15:38,634] {bash_operator.py:127} INFO - org.apache.spark#spark-streaming-kafka-0-8_2.11 added as a dependency\n",
      "[2019-08-17 03:15:38,634] {bash_operator.py:127} INFO - org.apache.spark#spark-streaming-kinesis-asl_2.11 added as a dependency\n",
      "[2019-08-17 03:15:38,635] {bash_operator.py:127} INFO - :: resolving dependencies :: org.apache.spark#spark-submit-parent-91da80f7-7436-4736-8aaf-54647df23850;1.0\n",
      "[2019-08-17 03:15:38,635] {bash_operator.py:127} INFO - \tconfs: [default]\n",
      "[2019-08-17 03:15:38,884] {bash_operator.py:127} INFO - \tfound org.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.0 in central\n",
      "[2019-08-17 03:15:38,938] {bash_operator.py:127} INFO - \tfound org.apache.kafka#kafka_2.11;0.8.2.1 in central\n",
      "[2019-08-17 03:15:38,976] {bash_operator.py:127} INFO - \tfound org.scala-lang.modules#scala-xml_2.11;1.0.2 in central\n",
      "[2019-08-17 03:15:39,003] {bash_operator.py:127} INFO - \tfound com.yammer.metrics#metrics-core;2.2.0 in central\n",
      "[2019-08-17 03:15:39,032] {bash_operator.py:127} INFO - \tfound org.slf4j#slf4j-api;1.7.16 in central\n",
      "[2019-08-17 03:15:39,055] {bash_operator.py:127} INFO - \tfound org.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 in central\n",
      "[2019-08-17 03:15:39,094] {bash_operator.py:127} INFO - \tfound com.101tec#zkclient;0.3 in central\n",
      "[2019-08-17 03:15:39,144] {bash_operator.py:127} INFO - \tfound log4j#log4j;1.2.17 in central\n",
      "[2019-08-17 03:15:39,169] {bash_operator.py:127} INFO - \tfound org.apache.kafka#kafka-clients;0.8.2.1 in central\n",
      "[2019-08-17 03:15:39,222] {bash_operator.py:127} INFO - \tfound net.jpountz.lz4#lz4;1.2.0 in central\n",
      "[2019-08-17 03:15:39,260] {bash_operator.py:127} INFO - \tfound org.xerial.snappy#snappy-java;1.1.7.1 in central\n",
      "[2019-08-17 03:15:39,303] {bash_operator.py:127} INFO - \tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "[2019-08-17 03:15:39,363] {bash_operator.py:127} INFO - \tfound org.apache.spark#spark-streaming-kinesis-asl_2.11;2.4.0 in central\n",
      "[2019-08-17 03:15:39,394] {bash_operator.py:127} INFO - \tfound com.amazonaws#amazon-kinesis-client;1.8.10 in central\n",
      "[2019-08-17 03:15:39,422] {bash_operator.py:127} INFO - \tfound com.amazonaws#aws-java-sdk-dynamodb;1.11.271 in central\n",
      "[2019-08-17 03:15:39,447] {bash_operator.py:127} INFO - \tfound com.amazonaws#aws-java-sdk-s3;1.11.271 in central\n",
      "[2019-08-17 03:15:39,470] {bash_operator.py:127} INFO - \tfound com.amazonaws#aws-java-sdk-kms;1.11.271 in central\n",
      "[2019-08-17 03:15:39,483] {bash_operator.py:127} INFO - \tfound com.amazonaws#aws-java-sdk-core;1.11.271 in central\n",
      "[2019-08-17 03:15:39,499] {bash_operator.py:127} INFO - \tfound commons-logging#commons-logging;1.1.3 in central\n",
      "[2019-08-17 03:15:39,512] {bash_operator.py:127} INFO - \tfound org.apache.httpcomponents#httpclient;4.5.6 in central\n",
      "[2019-08-17 03:15:39,528] {bash_operator.py:127} INFO - \tfound org.apache.httpcomponents#httpcore;4.4.10 in central\n",
      "[2019-08-17 03:15:39,554] {bash_operator.py:127} INFO - \tfound commons-codec#commons-codec;1.10 in central\n",
      "[2019-08-17 03:15:39,584] {bash_operator.py:127} INFO - \tfound software.amazon.ion#ion-java;1.0.2 in central\n",
      "[2019-08-17 03:15:39,608] {bash_operator.py:127} INFO - \tfound com.fasterxml.jackson.core#jackson-databind;2.6.7.1 in central\n",
      "[2019-08-17 03:15:39,621] {bash_operator.py:127} INFO - \tfound com.fasterxml.jackson.core#jackson-annotations;2.6.7 in central\n",
      "[2019-08-17 03:15:39,636] {bash_operator.py:127} INFO - \tfound com.fasterxml.jackson.core#jackson-core;2.6.7 in central\n",
      "[2019-08-17 03:15:39,654] {bash_operator.py:127} INFO - \tfound com.fasterxml.jackson.dataformat#jackson-dataformat-cbor;2.6.7 in central\n",
      "[2019-08-17 03:15:39,671] {bash_operator.py:127} INFO - \tfound joda-time#joda-time;2.9.3 in central\n",
      "[2019-08-17 03:15:39,706] {bash_operator.py:127} INFO - \tfound com.amazonaws#jmespath-java;1.11.271 in central\n",
      "[2019-08-17 03:15:39,723] {bash_operator.py:127} INFO - \tfound com.amazonaws#aws-java-sdk-kinesis;1.11.271 in central\n",
      "[2019-08-17 03:15:39,765] {bash_operator.py:127} INFO - \tfound com.amazonaws#aws-java-sdk-cloudwatch;1.11.271 in central\n",
      "[2019-08-17 03:15:39,791] {bash_operator.py:127} INFO - \tfound com.google.protobuf#protobuf-java;2.5.0 in central\n",
      "[2019-08-17 03:15:39,807] {bash_operator.py:127} INFO - \tfound commons-lang#commons-lang;2.6 in central\n",
      "[2019-08-17 03:15:39,844] {bash_operator.py:127} INFO - \tfound com.amazonaws#aws-java-sdk-sts;1.11.271 in central\n",
      "[2019-08-17 03:15:39,909] {bash_operator.py:127} INFO - :: resolution report :: resolve 1238ms :: artifacts dl 36ms\n",
      "[2019-08-17 03:15:39,909] {bash_operator.py:127} INFO - \t:: modules in use:\n",
      "[2019-08-17 03:15:39,916] {bash_operator.py:127} INFO - \tcom.101tec#zkclient;0.3 from central in [default]\n",
      "[2019-08-17 03:15:39,916] {bash_operator.py:127} INFO - \tcom.amazonaws#amazon-kinesis-client;1.8.10 from central in [default]\n",
      "[2019-08-17 03:15:39,916] {bash_operator.py:127} INFO - \tcom.amazonaws#aws-java-sdk-cloudwatch;1.11.271 from central in [default]\n",
      "[2019-08-17 03:15:39,917] {bash_operator.py:127} INFO - \tcom.amazonaws#aws-java-sdk-core;1.11.271 from central in [default]\n",
      "[2019-08-17 03:15:39,917] {bash_operator.py:127} INFO - \tcom.amazonaws#aws-java-sdk-dynamodb;1.11.271 from central in [default]\n",
      "[2019-08-17 03:15:39,917] {bash_operator.py:127} INFO - \tcom.amazonaws#aws-java-sdk-kinesis;1.11.271 from central in [default]\n",
      "[2019-08-17 03:15:39,918] {bash_operator.py:127} INFO - \tcom.amazonaws#aws-java-sdk-kms;1.11.271 from central in [default]\n",
      "[2019-08-17 03:15:39,918] {bash_operator.py:127} INFO - \tcom.amazonaws#aws-java-sdk-s3;1.11.271 from central in [default]\n",
      "[2019-08-17 03:15:39,918] {bash_operator.py:127} INFO - \tcom.amazonaws#aws-java-sdk-sts;1.11.271 from central in [default]\n",
      "[2019-08-17 03:15:39,918] {bash_operator.py:127} INFO - \tcom.amazonaws#jmespath-java;1.11.271 from central in [default]\n",
      "[2019-08-17 03:15:39,918] {bash_operator.py:127} INFO - \tcom.fasterxml.jackson.core#jackson-annotations;2.6.7 from central in [default]\n",
      "[2019-08-17 03:15:39,918] {bash_operator.py:127} INFO - \tcom.fasterxml.jackson.core#jackson-core;2.6.7 from central in [default]\n",
      "[2019-08-17 03:15:39,918] {bash_operator.py:127} INFO - \tcom.fasterxml.jackson.core#jackson-databind;2.6.7.1 from central in [default]\n",
      "[2019-08-17 03:15:39,918] {bash_operator.py:127} INFO - \tcom.fasterxml.jackson.dataformat#jackson-dataformat-cbor;2.6.7 from central in [default]\n",
      "[2019-08-17 03:15:39,919] {bash_operator.py:127} INFO - \tcom.google.protobuf#protobuf-java;2.5.0 from central in [default]\n",
      "[2019-08-17 03:15:39,919] {bash_operator.py:127} INFO - \tcom.yammer.metrics#metrics-core;2.2.0 from central in [default]\n",
      "[2019-08-17 03:15:39,919] {bash_operator.py:127} INFO - \tcommons-codec#commons-codec;1.10 from central in [default]\n",
      "[2019-08-17 03:15:39,919] {bash_operator.py:127} INFO - \tcommons-lang#commons-lang;2.6 from central in [default]\n",
      "[2019-08-17 03:15:39,919] {bash_operator.py:127} INFO - \tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "[2019-08-17 03:15:39,919] {bash_operator.py:127} INFO - \tjoda-time#joda-time;2.9.3 from central in [default]\n",
      "[2019-08-17 03:15:39,920] {bash_operator.py:127} INFO - \tlog4j#log4j;1.2.17 from central in [default]\n",
      "[2019-08-17 03:15:39,920] {bash_operator.py:127} INFO - \tnet.jpountz.lz4#lz4;1.2.0 from central in [default]\n",
      "[2019-08-17 03:15:39,920] {bash_operator.py:127} INFO - \torg.apache.httpcomponents#httpclient;4.5.6 from central in [default]\n",
      "[2019-08-17 03:15:39,920] {bash_operator.py:127} INFO - \torg.apache.httpcomponents#httpcore;4.4.10 from central in [default]\n",
      "[2019-08-17 03:15:39,920] {bash_operator.py:127} INFO - \torg.apache.kafka#kafka-clients;0.8.2.1 from central in [default]\n",
      "[2019-08-17 03:15:39,922] {bash_operator.py:127} INFO - \torg.apache.kafka#kafka_2.11;0.8.2.1 from central in [default]\n",
      "[2019-08-17 03:15:39,922] {bash_operator.py:127} INFO - \torg.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.0 from central in [default]\n",
      "[2019-08-17 03:15:39,922] {bash_operator.py:127} INFO - \torg.apache.spark#spark-streaming-kinesis-asl_2.11;2.4.0 from central in [default]\n",
      "[2019-08-17 03:15:39,922] {bash_operator.py:127} INFO - \torg.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 from central in [default]\n",
      "[2019-08-17 03:15:39,922] {bash_operator.py:127} INFO - \torg.scala-lang.modules#scala-xml_2.11;1.0.2 from central in [default]\n",
      "[2019-08-17 03:15:39,923] {bash_operator.py:127} INFO - \torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
      "[2019-08-17 03:15:39,923] {bash_operator.py:127} INFO - \torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "[2019-08-17 03:15:39,923] {bash_operator.py:127} INFO - \torg.xerial.snappy#snappy-java;1.1.7.1 from central in [default]\n",
      "[2019-08-17 03:15:39,923] {bash_operator.py:127} INFO - \tsoftware.amazon.ion#ion-java;1.0.2 from central in [default]\n",
      "[2019-08-17 03:15:39,923] {bash_operator.py:127} INFO - \t:: evicted modules:\n",
      "[2019-08-17 03:15:39,923] {bash_operator.py:127} INFO - \tcommons-logging#commons-logging;1.2 by [commons-logging#commons-logging;1.1.3] in [default]\n",
      "[2019-08-17 03:15:39,923] {bash_operator.py:127} INFO - \t---------------------------------------------------------------------\n",
      "[2019-08-17 03:15:39,923] {bash_operator.py:127} INFO - \t|                  |            modules            ||   artifacts   |\n",
      "[2019-08-17 03:15:39,923] {bash_operator.py:127} INFO - \t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "[2019-08-17 03:15:39,923] {bash_operator.py:127} INFO - \t---------------------------------------------------------------------\n",
      "[2019-08-17 03:15:39,923] {bash_operator.py:127} INFO - \t|      default     |   35  |   0   |   0   |   1   ||   34  |   0   |\n",
      "[2019-08-17 03:15:39,923] {bash_operator.py:127} INFO - \t---------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-08-17 03:15:39,966] {bash_operator.py:127} INFO - :: retrieving :: org.apache.spark#spark-submit-parent-91da80f7-7436-4736-8aaf-54647df23850\n",
      "[2019-08-17 03:15:39,966] {bash_operator.py:127} INFO - \tconfs: [default]\n",
      "[2019-08-17 03:15:39,997] {bash_operator.py:127} INFO - \t0 artifacts copied, 34 already retrieved (0kB/30ms)\n",
      "[2019-08-17 03:15:40,574] {bash_operator.py:127} INFO - 2019-08-17 03:15:40 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "[2019-08-17 03:15:41,938] {bash_operator.py:127} INFO - 2019-08-17 03:15:41 INFO  SparkContext:54 - Running Spark version 2.4.0\n",
      "[2019-08-17 03:15:41,958] {bash_operator.py:127} INFO - 2019-08-17 03:15:41 INFO  SparkContext:54 - Submitted application: model_training.py\n",
      "[2019-08-17 03:15:42,013] {bash_operator.py:127} INFO - 2019-08-17 03:15:42 INFO  SecurityManager:54 - Changing view acls to: ubuntu\n",
      "[2019-08-17 03:15:42,014] {bash_operator.py:127} INFO - 2019-08-17 03:15:42 INFO  SecurityManager:54 - Changing modify acls to: ubuntu\n",
      "[2019-08-17 03:15:42,014] {bash_operator.py:127} INFO - 2019-08-17 03:15:42 INFO  SecurityManager:54 - Changing view acls groups to:\n",
      "[2019-08-17 03:15:42,014] {bash_operator.py:127} INFO - 2019-08-17 03:15:42 INFO  SecurityManager:54 - Changing modify acls groups to:\n",
      "[2019-08-17 03:15:42,014] {bash_operator.py:127} INFO - 2019-08-17 03:15:42 INFO  SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(ubuntu); groups with view permissions: Set(); users  with modify permissions: Set(ubuntu); groups with modify permissions: Set()\n",
      "[2019-08-17 03:15:42,282] {bash_operator.py:127} INFO - 2019-08-17 03:15:42 INFO  Utils:54 - Successfully started service 'sparkDriver' on port 46865.\n",
      "[2019-08-17 03:15:42,322] {bash_operator.py:127} INFO - 2019-08-17 03:15:42 INFO  SparkEnv:54 - Registering MapOutputTracker\n",
      "[2019-08-17 03:15:42,348] {bash_operator.py:127} INFO - 2019-08-17 03:15:42 INFO  SparkEnv:54 - Registering BlockManagerMaster\n",
      "[2019-08-17 03:15:42,352] {bash_operator.py:127} INFO - 2019-08-17 03:15:42 INFO  BlockManagerMasterEndpoint:54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "[2019-08-17 03:15:42,352] {bash_operator.py:127} INFO - 2019-08-17 03:15:42 INFO  BlockManagerMasterEndpoint:54 - BlockManagerMasterEndpoint up\n",
      "[2019-08-17 03:15:42,362] {bash_operator.py:127} INFO - 2019-08-17 03:15:42 INFO  DiskBlockManager:54 - Created local directory at /tmp/blockmgr-3ffe54ac-b292-43a2-ae87-e711826f9331\n",
      "[2019-08-17 03:15:42,380] {bash_operator.py:127} INFO - 2019-08-17 03:15:42 INFO  MemoryStore:54 - MemoryStore started with capacity 366.3 MB\n",
      "[2019-08-17 03:15:42,452] {bash_operator.py:127} INFO - 2019-08-17 03:15:42 INFO  SparkEnv:54 - Registering OutputCommitCoordinator\n",
      "[2019-08-17 03:15:42,525] {bash_operator.py:127} INFO - 2019-08-17 03:15:42 INFO  log:192 - Logging initialized @4805ms\n",
      "[2019-08-17 03:15:42,585] {bash_operator.py:127} INFO - 2019-08-17 03:15:42 INFO  Server:351 - jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown\n",
      "[2019-08-17 03:15:42,605] {bash_operator.py:127} INFO - 2019-08-17 03:15:42 INFO  Server:419 - Started @4884ms\n",
      "[2019-08-17 03:15:42,627] {bash_operator.py:127} INFO - 2019-08-17 03:15:42 INFO  AbstractConnector:278 - Started ServerConnector@73b2dc29{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n",
      "[2019-08-17 03:15:42,628] {bash_operator.py:127} INFO - 2019-08-17 03:15:42 INFO  Utils:54 - Successfully started service 'SparkUI' on port 4040.\n",
      "[2019-08-17 03:15:42,660] {bash_operator.py:127} INFO - 2019-08-17 03:15:42 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@10063bde{/jobs,null,AVAILABLE,@Spark}\n",
      "[2019-08-17 03:15:42,661] {bash_operator.py:127} INFO - 2019-08-17 03:15:42 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@7d02d559{/jobs/json,null,AVAILABLE,@Spark}\n",
      "[2019-08-17 03:15:42,662] {bash_operator.py:127} INFO - 2019-08-17 03:15:42 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@313f2d54{/jobs/job,null,AVAILABLE,@Spark}\n",
      "[2019-08-17 03:15:42,664] {bash_operator.py:127} INFO - 2019-08-17 03:15:42 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@57cf6b7{/jobs/job/json,null,AVAILABLE,@Spark}\n",
      "[2019-08-17 03:15:42,665] {bash_operator.py:127} INFO - 2019-08-17 03:15:42 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@6ebf535c{/stages,null,AVAILABLE,@Spark}\n",
      "[2019-08-17 03:15:42,666] {bash_operator.py:127} INFO - 2019-08-17 03:15:42 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@26229652{/stages/json,null,AVAILABLE,@Spark}\n",
      "[2019-08-17 03:15:42,668] {bash_operator.py:127} INFO - 2019-08-17 03:15:42 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@41e908e3{/stages/stage,null,AVAILABLE,@Spark}\n",
      "[2019-08-17 03:15:42,670] {bash_operator.py:127} INFO - 2019-08-17 03:15:42 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@5d1951a{/stages/stage/json,null,AVAILABLE,@Spark}\n",
      "[2019-08-17 03:15:42,671] {bash_operator.py:127} INFO - 2019-08-17 03:15:42 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@cf63b53{/stages/pool,null,AVAILABLE,@Spark}\n",
      "[2019-08-17 03:15:42,673] {bash_operator.py:127} INFO - 2019-08-17 03:15:42 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3bba6e45{/stages/pool/json,null,AVAILABLE,@Spark}\n",
      "[2019-08-17 03:15:42,674] {bash_operator.py:127} INFO - 2019-08-17 03:15:42 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@144bb287{/storage,null,AVAILABLE,@Spark}\n",
      "[2019-08-17 03:15:42,675] {bash_operator.py:127} INFO - 2019-08-17 03:15:42 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@1d3b9c51{/storage/json,null,AVAILABLE,@Spark}\n",
      "[2019-08-17 03:15:42,675] {bash_operator.py:127} INFO - 2019-08-17 03:15:42 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@20aa0006{/storage/rdd,null,AVAILABLE,@Spark}\n",
      "[2019-08-17 03:15:42,676] {bash_operator.py:127} INFO - 2019-08-17 03:15:42 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@480a8a10{/storage/rdd/json,null,AVAILABLE,@Spark}\n",
      "[2019-08-17 03:15:42,677] {bash_operator.py:127} INFO - 2019-08-17 03:15:42 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@11cd61c{/environment,null,AVAILABLE,@Spark}\n",
      "[2019-08-17 03:15:42,679] {bash_operator.py:127} INFO - 2019-08-17 03:15:42 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@40840b7a{/environment/json,null,AVAILABLE,@Spark}\n",
      "[2019-08-17 03:15:42,680] {bash_operator.py:127} INFO - 2019-08-17 03:15:42 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@305e895c{/executors,null,AVAILABLE,@Spark}\n",
      "[2019-08-17 03:15:42,682] {bash_operator.py:127} INFO - 2019-08-17 03:15:42 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2c18cf4d{/executors/json,null,AVAILABLE,@Spark}\n",
      "[2019-08-17 03:15:42,684] {bash_operator.py:127} INFO - 2019-08-17 03:15:42 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4bd340e4{/executors/threadDump,null,AVAILABLE,@Spark}\n",
      "[2019-08-17 03:15:42,684] {bash_operator.py:127} INFO - 2019-08-17 03:15:42 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2bec5d3f{/executors/threadDump/json,null,AVAILABLE,@Spark}\n",
      "[2019-08-17 03:15:42,691] {bash_operator.py:127} INFO - 2019-08-17 03:15:42 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@6fbc2577{/static,null,AVAILABLE,@Spark}\n",
      "[2019-08-17 03:15:42,692] {bash_operator.py:127} INFO - 2019-08-17 03:15:42 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@7f71622b{/,null,AVAILABLE,@Spark}\n",
      "[2019-08-17 03:15:42,694] {bash_operator.py:127} INFO - 2019-08-17 03:15:42 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@b75ff43{/api,null,AVAILABLE,@Spark}\n",
      "[2019-08-17 03:15:42,696] {bash_operator.py:127} INFO - 2019-08-17 03:15:42 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@52bdbfd9{/jobs/job/kill,null,AVAILABLE,@Spark}\n",
      "[2019-08-17 03:15:42,697] {bash_operator.py:127} INFO - 2019-08-17 03:15:42 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@39415a29{/stages/stage/kill,null,AVAILABLE,@Spark}\n",
      "[2019-08-17 03:15:42,699] {bash_operator.py:127} INFO - 2019-08-17 03:15:42 INFO  SparkUI:54 - Bound SparkUI to 0.0.0.0, and started at http://ip-172-31-30-20.ap-northeast-2.compute.internal:4040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-08-17 03:15:42,731] {bash_operator.py:127} INFO - 2019-08-17 03:15:42 INFO  SparkContext:54 - Added JAR file:/home/ubuntu/project1/lib/mongo-hadoop-spark-2.0.2.jar at spark://ip-172-31-30-20.ap-northeast-2.compute.internal:46865/jars/mongo-hadoop-spark-2.0.2.jar with timestamp 1566011742730\n",
      "[2019-08-17 03:15:43,470] {bash_operator.py:127} INFO - 2019-08-17 03:15:43 INFO  RMProxy:123 - Connecting to ResourceManager at ip-172-31-20-77.ap-northeast-2.compute.internal/172.31.20.77:8032\n",
      "[2019-08-17 03:15:43,674] {bash_operator.py:127} INFO - 2019-08-17 03:15:43 INFO  Client:54 - Requesting a new application from cluster with 2 NodeManagers\n",
      "[2019-08-17 03:15:43,699] {bash_operator.py:127} INFO - 2019-08-17 03:15:43 INFO  Client:54 - Verifying our application has not requested more than the maximum memory capability of the cluster (23424 MB per container)\n",
      "[2019-08-17 03:15:43,702] {bash_operator.py:127} INFO - 2019-08-17 03:15:43 INFO  Client:54 - Will allocate AM container, with 896 MB memory including 384 MB overhead\n",
      "[2019-08-17 03:15:43,702] {bash_operator.py:127} INFO - 2019-08-17 03:15:43 INFO  Client:54 - Setting up container launch context for our AM\n",
      "[2019-08-17 03:15:43,704] {bash_operator.py:127} INFO - 2019-08-17 03:15:43 INFO  Client:54 - Setting up the launch environment for our AM container\n",
      "[2019-08-17 03:15:43,711] {bash_operator.py:127} INFO - 2019-08-17 03:15:43 INFO  Client:54 - Preparing resources for our AM container\n",
      "[2019-08-17 03:15:43,744] {bash_operator.py:127} INFO - 2019-08-17 03:15:43 WARN  Client:66 - Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "[2019-08-17 03:15:45,935] {bash_operator.py:127} INFO - 2019-08-17 03:15:45 INFO  Client:54 - Uploading resource file:/tmp/spark-6b419b56-5731-41fc-b728-a5d6d14382a9/__spark_libs__8540481570031575496.zip -> hdfs://ip-172-31-20-77.ap-northeast-2.compute.internal:8020/user/ubuntu/.sparkStaging/application_1566004128108_0033/__spark_libs__8540481570031575496.zip\n",
      "[2019-08-17 03:15:47,522] {bash_operator.py:127} INFO - 2019-08-17 03:15:47 INFO  Client:54 - Uploading resource file:/home/ubuntu/project1/lib/mongo-hadoop-spark-2.0.2.jar -> hdfs://ip-172-31-20-77.ap-northeast-2.compute.internal:8020/user/ubuntu/.sparkStaging/application_1566004128108_0033/mongo-hadoop-spark-2.0.2.jar\n",
      "[2019-08-17 03:15:47,562] {bash_operator.py:127} INFO - 2019-08-17 03:15:47 INFO  Client:54 - Uploading resource file:/home/ubuntu/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.0.jar -> hdfs://ip-172-31-20-77.ap-northeast-2.compute.internal:8020/user/ubuntu/.sparkStaging/application_1566004128108_0033/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.0.jar\n",
      "[2019-08-17 03:15:47,582] {bash_operator.py:127} INFO - 2019-08-17 03:15:47 INFO  Client:54 - Uploading resource file:/home/ubuntu/.ivy2/jars/org.apache.spark_spark-streaming-kinesis-asl_2.11-2.4.0.jar -> hdfs://ip-172-31-20-77.ap-northeast-2.compute.internal:8020/user/ubuntu/.sparkStaging/application_1566004128108_0033/org.apache.spark_spark-streaming-kinesis-asl_2.11-2.4.0.jar\n",
      "[2019-08-17 03:15:47,607] {bash_operator.py:127} INFO - 2019-08-17 03:15:47 INFO  Client:54 - Uploading resource file:/home/ubuntu/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar -> hdfs://ip-172-31-20-77.ap-northeast-2.compute.internal:8020/user/ubuntu/.sparkStaging/application_1566004128108_0033/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
      "[2019-08-17 03:15:47,661] {bash_operator.py:127} INFO - 2019-08-17 03:15:47 INFO  Client:54 - Uploading resource file:/home/ubuntu/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar -> hdfs://ip-172-31-20-77.ap-northeast-2.compute.internal:8020/user/ubuntu/.sparkStaging/application_1566004128108_0033/org.spark-project.spark_unused-1.0.0.jar\n",
      "[2019-08-17 03:15:47,677] {bash_operator.py:127} INFO - 2019-08-17 03:15:47 INFO  Client:54 - Uploading resource file:/home/ubuntu/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar -> hdfs://ip-172-31-20-77.ap-northeast-2.compute.internal:8020/user/ubuntu/.sparkStaging/application_1566004128108_0033/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
      "[2019-08-17 03:15:47,698] {bash_operator.py:127} INFO - 2019-08-17 03:15:47 INFO  Client:54 - Uploading resource file:/home/ubuntu/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar -> hdfs://ip-172-31-20-77.ap-northeast-2.compute.internal:8020/user/ubuntu/.sparkStaging/application_1566004128108_0033/com.yammer.metrics_metrics-core-2.2.0.jar\n",
      "[2019-08-17 03:15:47,717] {bash_operator.py:127} INFO - 2019-08-17 03:15:47 INFO  Client:54 - Uploading resource file:/home/ubuntu/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar -> hdfs://ip-172-31-20-77.ap-northeast-2.compute.internal:8020/user/ubuntu/.sparkStaging/application_1566004128108_0033/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
      "[2019-08-17 03:15:47,741] {bash_operator.py:127} INFO - 2019-08-17 03:15:47 INFO  Client:54 - Uploading resource file:/home/ubuntu/.ivy2/jars/com.101tec_zkclient-0.3.jar -> hdfs://ip-172-31-20-77.ap-northeast-2.compute.internal:8020/user/ubuntu/.sparkStaging/application_1566004128108_0033/com.101tec_zkclient-0.3.jar\n",
      "[2019-08-17 03:15:47,756] {bash_operator.py:127} INFO - 2019-08-17 03:15:47 INFO  Client:54 - Uploading resource file:/home/ubuntu/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar -> hdfs://ip-172-31-20-77.ap-northeast-2.compute.internal:8020/user/ubuntu/.sparkStaging/application_1566004128108_0033/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
      "[2019-08-17 03:15:47,777] {bash_operator.py:127} INFO - 2019-08-17 03:15:47 INFO  Client:54 - Uploading resource file:/home/ubuntu/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar -> hdfs://ip-172-31-20-77.ap-northeast-2.compute.internal:8020/user/ubuntu/.sparkStaging/application_1566004128108_0033/org.slf4j_slf4j-api-1.7.16.jar\n",
      "[2019-08-17 03:15:47,795] {bash_operator.py:127} INFO - 2019-08-17 03:15:47 INFO  Client:54 - Uploading resource file:/home/ubuntu/.ivy2/jars/log4j_log4j-1.2.17.jar -> hdfs://ip-172-31-20-77.ap-northeast-2.compute.internal:8020/user/ubuntu/.sparkStaging/application_1566004128108_0033/log4j_log4j-1.2.17.jar\n",
      "[2019-08-17 03:15:47,815] {bash_operator.py:127} INFO - 2019-08-17 03:15:47 INFO  Client:54 - Uploading resource file:/home/ubuntu/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar -> hdfs://ip-172-31-20-77.ap-northeast-2.compute.internal:8020/user/ubuntu/.sparkStaging/application_1566004128108_0033/net.jpountz.lz4_lz4-1.2.0.jar\n",
      "[2019-08-17 03:15:47,836] {bash_operator.py:127} INFO - 2019-08-17 03:15:47 INFO  Client:54 - Uploading resource file:/home/ubuntu/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.1.jar -> hdfs://ip-172-31-20-77.ap-northeast-2.compute.internal:8020/user/ubuntu/.sparkStaging/application_1566004128108_0033/org.xerial.snappy_snappy-java-1.1.7.1.jar\n",
      "[2019-08-17 03:15:47,876] {bash_operator.py:127} INFO - 2019-08-17 03:15:47 INFO  Client:54 - Uploading resource file:/home/ubuntu/.ivy2/jars/com.amazonaws_amazon-kinesis-client-1.8.10.jar -> hdfs://ip-172-31-20-77.ap-northeast-2.compute.internal:8020/user/ubuntu/.sparkStaging/application_1566004128108_0033/com.amazonaws_amazon-kinesis-client-1.8.10.jar\n",
      "[2019-08-17 03:15:47,897] {bash_operator.py:127} INFO - 2019-08-17 03:15:47 INFO  Client:54 - Uploading resource file:/home/ubuntu/.ivy2/jars/com.amazonaws_aws-java-sdk-sts-1.11.271.jar -> hdfs://ip-172-31-20-77.ap-northeast-2.compute.internal:8020/user/ubuntu/.sparkStaging/application_1566004128108_0033/com.amazonaws_aws-java-sdk-sts-1.11.271.jar\n",
      "[2019-08-17 03:15:47,914] {bash_operator.py:127} INFO - 2019-08-17 03:15:47 INFO  Client:54 - Uploading resource file:/home/ubuntu/.ivy2/jars/com.amazonaws_aws-java-sdk-dynamodb-1.11.271.jar -> hdfs://ip-172-31-20-77.ap-northeast-2.compute.internal:8020/user/ubuntu/.sparkStaging/application_1566004128108_0033/com.amazonaws_aws-java-sdk-dynamodb-1.11.271.jar\n",
      "[2019-08-17 03:15:47,951] {bash_operator.py:127} INFO - 2019-08-17 03:15:47 INFO  Client:54 - Uploading resource file:/home/ubuntu/.ivy2/jars/com.amazonaws_aws-java-sdk-kinesis-1.11.271.jar -> hdfs://ip-172-31-20-77.ap-northeast-2.compute.internal:8020/user/ubuntu/.sparkStaging/application_1566004128108_0033/com.amazonaws_aws-java-sdk-kinesis-1.11.271.jar\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-08-17 03:15:47,979] {bash_operator.py:127} INFO - 2019-08-17 03:15:47 INFO  Client:54 - Uploading resource file:/home/ubuntu/.ivy2/jars/com.amazonaws_aws-java-sdk-cloudwatch-1.11.271.jar -> hdfs://ip-172-31-20-77.ap-northeast-2.compute.internal:8020/user/ubuntu/.sparkStaging/application_1566004128108_0033/com.amazonaws_aws-java-sdk-cloudwatch-1.11.271.jar\n",
      "[2019-08-17 03:15:48,006] {bash_operator.py:127} INFO - 2019-08-17 03:15:48 INFO  Client:54 - Uploading resource file:/home/ubuntu/.ivy2/jars/com.google.protobuf_protobuf-java-2.5.0.jar -> hdfs://ip-172-31-20-77.ap-northeast-2.compute.internal:8020/user/ubuntu/.sparkStaging/application_1566004128108_0033/com.google.protobuf_protobuf-java-2.5.0.jar\n",
      "[2019-08-17 03:15:48,031] {bash_operator.py:127} INFO - 2019-08-17 03:15:48 INFO  Client:54 - Uploading resource file:/home/ubuntu/.ivy2/jars/commons-lang_commons-lang-2.6.jar -> hdfs://ip-172-31-20-77.ap-northeast-2.compute.internal:8020/user/ubuntu/.sparkStaging/application_1566004128108_0033/commons-lang_commons-lang-2.6.jar\n",
      "[2019-08-17 03:15:48,049] {bash_operator.py:127} INFO - 2019-08-17 03:15:48 INFO  Client:54 - Uploading resource file:/home/ubuntu/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar -> hdfs://ip-172-31-20-77.ap-northeast-2.compute.internal:8020/user/ubuntu/.sparkStaging/application_1566004128108_0033/commons-logging_commons-logging-1.1.3.jar\n",
      "[2019-08-17 03:15:48,072] {bash_operator.py:127} INFO - 2019-08-17 03:15:48 INFO  Client:54 - Uploading resource file:/home/ubuntu/.ivy2/jars/com.amazonaws_aws-java-sdk-s3-1.11.271.jar -> hdfs://ip-172-31-20-77.ap-northeast-2.compute.internal:8020/user/ubuntu/.sparkStaging/application_1566004128108_0033/com.amazonaws_aws-java-sdk-s3-1.11.271.jar\n",
      "[2019-08-17 03:15:48,097] {bash_operator.py:127} INFO - 2019-08-17 03:15:48 INFO  Client:54 - Uploading resource file:/home/ubuntu/.ivy2/jars/com.amazonaws_aws-java-sdk-core-1.11.271.jar -> hdfs://ip-172-31-20-77.ap-northeast-2.compute.internal:8020/user/ubuntu/.sparkStaging/application_1566004128108_0033/com.amazonaws_aws-java-sdk-core-1.11.271.jar\n",
      "[2019-08-17 03:15:48,128] {bash_operator.py:127} INFO - 2019-08-17 03:15:48 INFO  Client:54 - Uploading resource file:/home/ubuntu/.ivy2/jars/com.amazonaws_jmespath-java-1.11.271.jar -> hdfs://ip-172-31-20-77.ap-northeast-2.compute.internal:8020/user/ubuntu/.sparkStaging/application_1566004128108_0033/com.amazonaws_jmespath-java-1.11.271.jar\n",
      "[2019-08-17 03:15:48,147] {bash_operator.py:127} INFO - 2019-08-17 03:15:48 INFO  Client:54 - Uploading resource file:/home/ubuntu/.ivy2/jars/com.amazonaws_aws-java-sdk-kms-1.11.271.jar -> hdfs://ip-172-31-20-77.ap-northeast-2.compute.internal:8020/user/ubuntu/.sparkStaging/application_1566004128108_0033/com.amazonaws_aws-java-sdk-kms-1.11.271.jar\n",
      "[2019-08-17 03:15:48,165] {bash_operator.py:127} INFO - 2019-08-17 03:15:48 INFO  Client:54 - Uploading resource file:/home/ubuntu/.ivy2/jars/org.apache.httpcomponents_httpclient-4.5.6.jar -> hdfs://ip-172-31-20-77.ap-northeast-2.compute.internal:8020/user/ubuntu/.sparkStaging/application_1566004128108_0033/org.apache.httpcomponents_httpclient-4.5.6.jar\n",
      "[2019-08-17 03:15:48,196] {bash_operator.py:127} INFO - 2019-08-17 03:15:48 INFO  Client:54 - Uploading resource file:/home/ubuntu/.ivy2/jars/software.amazon.ion_ion-java-1.0.2.jar -> hdfs://ip-172-31-20-77.ap-northeast-2.compute.internal:8020/user/ubuntu/.sparkStaging/application_1566004128108_0033/software.amazon.ion_ion-java-1.0.2.jar\n",
      "[2019-08-17 03:15:48,214] {bash_operator.py:127} INFO - 2019-08-17 03:15:48 INFO  Client:54 - Uploading resource file:/home/ubuntu/.ivy2/jars/com.fasterxml.jackson.core_jackson-databind-2.6.7.1.jar -> hdfs://ip-172-31-20-77.ap-northeast-2.compute.internal:8020/user/ubuntu/.sparkStaging/application_1566004128108_0033/com.fasterxml.jackson.core_jackson-databind-2.6.7.1.jar\n",
      "[2019-08-17 03:15:48,243] {bash_operator.py:127} INFO - 2019-08-17 03:15:48 INFO  Client:54 - Uploading resource file:/home/ubuntu/.ivy2/jars/com.fasterxml.jackson.dataformat_jackson-dataformat-cbor-2.6.7.jar -> hdfs://ip-172-31-20-77.ap-northeast-2.compute.internal:8020/user/ubuntu/.sparkStaging/application_1566004128108_0033/com.fasterxml.jackson.dataformat_jackson-dataformat-cbor-2.6.7.jar\n",
      "[2019-08-17 03:15:48,266] {bash_operator.py:127} INFO - 2019-08-17 03:15:48 INFO  Client:54 - Uploading resource file:/home/ubuntu/.ivy2/jars/joda-time_joda-time-2.9.3.jar -> hdfs://ip-172-31-20-77.ap-northeast-2.compute.internal:8020/user/ubuntu/.sparkStaging/application_1566004128108_0033/joda-time_joda-time-2.9.3.jar\n",
      "[2019-08-17 03:15:48,287] {bash_operator.py:127} INFO - 2019-08-17 03:15:48 INFO  Client:54 - Uploading resource file:/home/ubuntu/.ivy2/jars/org.apache.httpcomponents_httpcore-4.4.10.jar -> hdfs://ip-172-31-20-77.ap-northeast-2.compute.internal:8020/user/ubuntu/.sparkStaging/application_1566004128108_0033/org.apache.httpcomponents_httpcore-4.4.10.jar\n",
      "[2019-08-17 03:15:48,302] {bash_operator.py:127} INFO - 2019-08-17 03:15:48 INFO  Client:54 - Uploading resource file:/home/ubuntu/.ivy2/jars/commons-codec_commons-codec-1.10.jar -> hdfs://ip-172-31-20-77.ap-northeast-2.compute.internal:8020/user/ubuntu/.sparkStaging/application_1566004128108_0033/commons-codec_commons-codec-1.10.jar\n",
      "[2019-08-17 03:15:48,324] {bash_operator.py:127} INFO - 2019-08-17 03:15:48 INFO  Client:54 - Uploading resource file:/home/ubuntu/.ivy2/jars/com.fasterxml.jackson.core_jackson-annotations-2.6.7.jar -> hdfs://ip-172-31-20-77.ap-northeast-2.compute.internal:8020/user/ubuntu/.sparkStaging/application_1566004128108_0033/com.fasterxml.jackson.core_jackson-annotations-2.6.7.jar\n",
      "[2019-08-17 03:15:48,344] {bash_operator.py:127} INFO - 2019-08-17 03:15:48 INFO  Client:54 - Uploading resource file:/home/ubuntu/.ivy2/jars/com.fasterxml.jackson.core_jackson-core-2.6.7.jar -> hdfs://ip-172-31-20-77.ap-northeast-2.compute.internal:8020/user/ubuntu/.sparkStaging/application_1566004128108_0033/com.fasterxml.jackson.core_jackson-core-2.6.7.jar\n",
      "[2019-08-17 03:15:48,363] {bash_operator.py:127} INFO - 2019-08-17 03:15:48 INFO  Client:54 - Uploading resource file:/home/ubuntu/project1/spark/python/lib/pyspark.zip -> hdfs://ip-172-31-20-77.ap-northeast-2.compute.internal:8020/user/ubuntu/.sparkStaging/application_1566004128108_0033/pyspark.zip\n",
      "[2019-08-17 03:15:48,382] {bash_operator.py:127} INFO - 2019-08-17 03:15:48 INFO  Client:54 - Uploading resource file:/home/ubuntu/project1/spark/python/lib/py4j-0.10.7-src.zip -> hdfs://ip-172-31-20-77.ap-northeast-2.compute.internal:8020/user/ubuntu/.sparkStaging/application_1566004128108_0033/py4j-0.10.7-src.zip\n",
      "[2019-08-17 03:15:48,404] {bash_operator.py:127} INFO - 2019-08-17 03:15:48 WARN  Client:66 - Same path resource file:///home/ubuntu/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.0.jar added multiple times to distributed cache.\n",
      "[2019-08-17 03:15:48,405] {bash_operator.py:127} INFO - 2019-08-17 03:15:48 WARN  Client:66 - Same path resource file:///home/ubuntu/.ivy2/jars/org.apache.spark_spark-streaming-kinesis-asl_2.11-2.4.0.jar added multiple times to distributed cache.\n",
      "[2019-08-17 03:15:48,405] {bash_operator.py:127} INFO - 2019-08-17 03:15:48 WARN  Client:66 - Same path resource file:///home/ubuntu/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar added multiple times to distributed cache.\n",
      "[2019-08-17 03:15:48,405] {bash_operator.py:127} INFO - 2019-08-17 03:15:48 WARN  Client:66 - Same path resource file:///home/ubuntu/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar added multiple times to distributed cache.\n",
      "[2019-08-17 03:15:48,406] {bash_operator.py:127} INFO - 2019-08-17 03:15:48 WARN  Client:66 - Same path resource file:///home/ubuntu/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar added multiple times to distributed cache.\n",
      "[2019-08-17 03:15:48,406] {bash_operator.py:127} INFO - 2019-08-17 03:15:48 WARN  Client:66 - Same path resource file:///home/ubuntu/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar added multiple times to distributed cache.\n",
      "[2019-08-17 03:15:48,406] {bash_operator.py:127} INFO - 2019-08-17 03:15:48 WARN  Client:66 - Same path resource file:///home/ubuntu/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar added multiple times to distributed cache.\n",
      "[2019-08-17 03:15:48,407] {bash_operator.py:127} INFO - 2019-08-17 03:15:48 WARN  Client:66 - Same path resource file:///home/ubuntu/.ivy2/jars/com.101tec_zkclient-0.3.jar added multiple times to distributed cache.\n",
      "[2019-08-17 03:15:48,407] {bash_operator.py:127} INFO - 2019-08-17 03:15:48 WARN  Client:66 - Same path resource file:///home/ubuntu/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar added multiple times to distributed cache.\n",
      "[2019-08-17 03:15:48,407] {bash_operator.py:127} INFO - 2019-08-17 03:15:48 WARN  Client:66 - Same path resource file:///home/ubuntu/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar added multiple times to distributed cache.\n",
      "[2019-08-17 03:15:48,408] {bash_operator.py:127} INFO - 2019-08-17 03:15:48 WARN  Client:66 - Same path resource file:///home/ubuntu/.ivy2/jars/log4j_log4j-1.2.17.jar added multiple times to distributed cache.\n",
      "[2019-08-17 03:15:48,408] {bash_operator.py:127} INFO - 2019-08-17 03:15:48 WARN  Client:66 - Same path resource file:///home/ubuntu/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar added multiple times to distributed cache.\n",
      "[2019-08-17 03:15:48,408] {bash_operator.py:127} INFO - 2019-08-17 03:15:48 WARN  Client:66 - Same path resource file:///home/ubuntu/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.1.jar added multiple times to distributed cache.\n",
      "[2019-08-17 03:15:48,408] {bash_operator.py:127} INFO - 2019-08-17 03:15:48 WARN  Client:66 - Same path resource file:///home/ubuntu/.ivy2/jars/com.amazonaws_amazon-kinesis-client-1.8.10.jar added multiple times to distributed cache.\n",
      "[2019-08-17 03:15:48,409] {bash_operator.py:127} INFO - 2019-08-17 03:15:48 WARN  Client:66 - Same path resource file:///home/ubuntu/.ivy2/jars/com.amazonaws_aws-java-sdk-sts-1.11.271.jar added multiple times to distributed cache.\n",
      "[2019-08-17 03:15:48,409] {bash_operator.py:127} INFO - 2019-08-17 03:15:48 WARN  Client:66 - Same path resource file:///home/ubuntu/.ivy2/jars/com.amazonaws_aws-java-sdk-dynamodb-1.11.271.jar added multiple times to distributed cache.\n",
      "[2019-08-17 03:15:48,409] {bash_operator.py:127} INFO - 2019-08-17 03:15:48 WARN  Client:66 - Same path resource file:///home/ubuntu/.ivy2/jars/com.amazonaws_aws-java-sdk-kinesis-1.11.271.jar added multiple times to distributed cache.\n",
      "[2019-08-17 03:15:48,410] {bash_operator.py:127} INFO - 2019-08-17 03:15:48 WARN  Client:66 - Same path resource file:///home/ubuntu/.ivy2/jars/com.amazonaws_aws-java-sdk-cloudwatch-1.11.271.jar added multiple times to distributed cache.\n",
      "[2019-08-17 03:15:48,410] {bash_operator.py:127} INFO - 2019-08-17 03:15:48 WARN  Client:66 - Same path resource file:///home/ubuntu/.ivy2/jars/com.google.protobuf_protobuf-java-2.5.0.jar added multiple times to distributed cache.\n",
      "[2019-08-17 03:15:48,410] {bash_operator.py:127} INFO - 2019-08-17 03:15:48 WARN  Client:66 - Same path resource file:///home/ubuntu/.ivy2/jars/commons-lang_commons-lang-2.6.jar added multiple times to distributed cache.\n",
      "[2019-08-17 03:15:48,410] {bash_operator.py:127} INFO - 2019-08-17 03:15:48 WARN  Client:66 - Same path resource file:///home/ubuntu/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar added multiple times to distributed cache.\n",
      "[2019-08-17 03:15:48,411] {bash_operator.py:127} INFO - 2019-08-17 03:15:48 WARN  Client:66 - Same path resource file:///home/ubuntu/.ivy2/jars/com.amazonaws_aws-java-sdk-s3-1.11.271.jar added multiple times to distributed cache.\n",
      "[2019-08-17 03:15:48,411] {bash_operator.py:127} INFO - 2019-08-17 03:15:48 WARN  Client:66 - Same path resource file:///home/ubuntu/.ivy2/jars/com.amazonaws_aws-java-sdk-core-1.11.271.jar added multiple times to distributed cache.\n",
      "[2019-08-17 03:15:48,412] {bash_operator.py:127} INFO - 2019-08-17 03:15:48 WARN  Client:66 - Same path resource file:///home/ubuntu/.ivy2/jars/com.amazonaws_jmespath-java-1.11.271.jar added multiple times to distributed cache.\n",
      "[2019-08-17 03:15:48,412] {bash_operator.py:127} INFO - 2019-08-17 03:15:48 WARN  Client:66 - Same path resource file:///home/ubuntu/.ivy2/jars/com.amazonaws_aws-java-sdk-kms-1.11.271.jar added multiple times to distributed cache.\n",
      "[2019-08-17 03:15:48,412] {bash_operator.py:127} INFO - 2019-08-17 03:15:48 WARN  Client:66 - Same path resource file:///home/ubuntu/.ivy2/jars/org.apache.httpcomponents_httpclient-4.5.6.jar added multiple times to distributed cache.\n",
      "[2019-08-17 03:15:48,412] {bash_operator.py:127} INFO - 2019-08-17 03:15:48 WARN  Client:66 - Same path resource file:///home/ubuntu/.ivy2/jars/software.amazon.ion_ion-java-1.0.2.jar added multiple times to distributed cache.\n",
      "[2019-08-17 03:15:48,413] {bash_operator.py:127} INFO - 2019-08-17 03:15:48 WARN  Client:66 - Same path resource file:///home/ubuntu/.ivy2/jars/com.fasterxml.jackson.core_jackson-databind-2.6.7.1.jar added multiple times to distributed cache.\n",
      "[2019-08-17 03:15:48,413] {bash_operator.py:127} INFO - 2019-08-17 03:15:48 WARN  Client:66 - Same path resource file:///home/ubuntu/.ivy2/jars/com.fasterxml.jackson.dataformat_jackson-dataformat-cbor-2.6.7.jar added multiple times to distributed cache.\n",
      "[2019-08-17 03:15:48,413] {bash_operator.py:127} INFO - 2019-08-17 03:15:48 WARN  Client:66 - Same path resource file:///home/ubuntu/.ivy2/jars/joda-time_joda-time-2.9.3.jar added multiple times to distributed cache.\n",
      "[2019-08-17 03:15:48,414] {bash_operator.py:127} INFO - 2019-08-17 03:15:48 WARN  Client:66 - Same path resource file:///home/ubuntu/.ivy2/jars/org.apache.httpcomponents_httpcore-4.4.10.jar added multiple times to distributed cache.\n",
      "[2019-08-17 03:15:48,414] {bash_operator.py:127} INFO - 2019-08-17 03:15:48 WARN  Client:66 - Same path resource file:///home/ubuntu/.ivy2/jars/commons-codec_commons-codec-1.10.jar added multiple times to distributed cache.\n",
      "[2019-08-17 03:15:48,414] {bash_operator.py:127} INFO - 2019-08-17 03:15:48 WARN  Client:66 - Same path resource file:///home/ubuntu/.ivy2/jars/com.fasterxml.jackson.core_jackson-annotations-2.6.7.jar added multiple times to distributed cache.\n",
      "[2019-08-17 03:15:48,415] {bash_operator.py:127} INFO - 2019-08-17 03:15:48 WARN  Client:66 - Same path resource file:///home/ubuntu/.ivy2/jars/com.fasterxml.jackson.core_jackson-core-2.6.7.jar added multiple times to distributed cache.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-08-17 03:15:48,503] {bash_operator.py:127} INFO - 2019-08-17 03:15:48 INFO  Client:54 - Uploading resource file:/tmp/spark-6b419b56-5731-41fc-b728-a5d6d14382a9/__spark_conf__7398868603628305738.zip -> hdfs://ip-172-31-20-77.ap-northeast-2.compute.internal:8020/user/ubuntu/.sparkStaging/application_1566004128108_0033/__spark_conf__.zip\n",
      "[2019-08-17 03:15:48,538] {bash_operator.py:127} INFO - 2019-08-17 03:15:48 INFO  SecurityManager:54 - Changing view acls to: ubuntu\n",
      "[2019-08-17 03:15:48,538] {bash_operator.py:127} INFO - 2019-08-17 03:15:48 INFO  SecurityManager:54 - Changing modify acls to: ubuntu\n",
      "[2019-08-17 03:15:48,539] {bash_operator.py:127} INFO - 2019-08-17 03:15:48 INFO  SecurityManager:54 - Changing view acls groups to:\n",
      "[2019-08-17 03:15:48,539] {bash_operator.py:127} INFO - 2019-08-17 03:15:48 INFO  SecurityManager:54 - Changing modify acls groups to:\n",
      "[2019-08-17 03:15:48,539] {bash_operator.py:127} INFO - 2019-08-17 03:15:48 INFO  SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(ubuntu); groups with view permissions: Set(); users  with modify permissions: Set(ubuntu); groups with modify permissions: Set()\n",
      "[2019-08-17 03:15:49,639] {bash_operator.py:127} INFO - 2019-08-17 03:15:49 INFO  Client:54 - Submitting application application_1566004128108_0033 to ResourceManager\n",
      "[2019-08-17 03:15:49,675] {bash_operator.py:127} INFO - 2019-08-17 03:15:49 INFO  YarnClientImpl:278 - Submitted application application_1566004128108_0033\n",
      "[2019-08-17 03:15:49,677] {bash_operator.py:127} INFO - 2019-08-17 03:15:49 INFO  SchedulerExtensionServices:54 - Starting Yarn extension services with app application_1566004128108_0033 and attemptId None\n",
      "[2019-08-17 03:15:50,683] {bash_operator.py:127} INFO - 2019-08-17 03:15:50 INFO  Client:54 - Application report for application_1566004128108_0033 (state: ACCEPTED)\n",
      "[2019-08-17 03:15:50,686] {bash_operator.py:127} INFO - 2019-08-17 03:15:50 INFO  Client:54 -\n",
      "[2019-08-17 03:15:50,686] {bash_operator.py:127} INFO - \t client token: N/A\n",
      "[2019-08-17 03:15:50,686] {bash_operator.py:127} INFO - \t diagnostics: AM container is launched, waiting for AM container to Register with RM\n",
      "[2019-08-17 03:15:50,686] {bash_operator.py:127} INFO - \t ApplicationMaster host: N/A\n",
      "[2019-08-17 03:15:50,686] {bash_operator.py:127} INFO - \t ApplicationMaster RPC port: -1\n",
      "[2019-08-17 03:15:50,686] {bash_operator.py:127} INFO - \t queue: default\n",
      "[2019-08-17 03:15:50,686] {bash_operator.py:127} INFO - \t start time: 1566011749657\n",
      "[2019-08-17 03:15:50,686] {bash_operator.py:127} INFO - \t final status: UNDEFINED\n",
      "[2019-08-17 03:15:50,686] {bash_operator.py:127} INFO - \t tracking URL: http://ip-172-31-16-7.ap-northeast-2.compute.internal:20888/proxy/application_1566004128108_0033/\n",
      "[2019-08-17 03:15:50,686] {bash_operator.py:127} INFO - \t user: ubuntu\n",
      "[2019-08-17 03:15:51,688] {bash_operator.py:127} INFO - 2019-08-17 03:15:51 INFO  Client:54 - Application report for application_1566004128108_0033 (state: ACCEPTED)\n",
      "[2019-08-17 03:15:52,689] {bash_operator.py:127} INFO - 2019-08-17 03:15:52 INFO  Client:54 - Application report for application_1566004128108_0033 (state: ACCEPTED)\n",
      "[2019-08-17 03:15:53,691] {bash_operator.py:127} INFO - 2019-08-17 03:15:53 INFO  Client:54 - Application report for application_1566004128108_0033 (state: ACCEPTED)\n",
      "[2019-08-17 03:15:54,693] {bash_operator.py:127} INFO - 2019-08-17 03:15:54 INFO  Client:54 - Application report for application_1566004128108_0033 (state: ACCEPTED)\n",
      "[2019-08-17 03:15:55,555] {bash_operator.py:127} INFO - 2019-08-17 03:15:55 INFO  YarnClientSchedulerBackend:54 - Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> ip-172-31-16-7.ap-northeast-2.compute.internal, PROXY_URI_BASES -> http://ip-172-31-16-7.ap-northeast-2.compute.internal:20888/proxy/application_1566004128108_0033), /proxy/application_1566004128108_0033\n",
      "[2019-08-17 03:15:55,557] {bash_operator.py:127} INFO - 2019-08-17 03:15:55 INFO  JettyUtils:54 - Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /jobs, /jobs/json, /jobs/job, /jobs/job/json, /stages, /stages/json, /stages/stage, /stages/stage/json, /stages/pool, /stages/pool/json, /storage, /storage/json, /storage/rdd, /storage/rdd/json, /environment, /environment/json, /executors, /executors/json, /executors/threadDump, /executors/threadDump/json, /static, /, /api, /jobs/job/kill, /stages/stage/kill.\n",
      "[2019-08-17 03:15:55,694] {bash_operator.py:127} INFO - 2019-08-17 03:15:55 INFO  Client:54 - Application report for application_1566004128108_0033 (state: RUNNING)\n",
      "[2019-08-17 03:15:55,694] {bash_operator.py:127} INFO - 2019-08-17 03:15:55 INFO  Client:54 -\n",
      "[2019-08-17 03:15:55,695] {bash_operator.py:127} INFO - \t client token: N/A\n",
      "[2019-08-17 03:15:55,695] {bash_operator.py:127} INFO - \t diagnostics: N/A\n",
      "[2019-08-17 03:15:55,695] {bash_operator.py:127} INFO - \t ApplicationMaster host: 172.31.29.159\n",
      "[2019-08-17 03:15:55,695] {bash_operator.py:127} INFO - \t ApplicationMaster RPC port: -1\n",
      "[2019-08-17 03:15:55,695] {bash_operator.py:127} INFO - \t queue: default\n",
      "[2019-08-17 03:15:55,695] {bash_operator.py:127} INFO - \t start time: 1566011749657\n",
      "[2019-08-17 03:15:55,695] {bash_operator.py:127} INFO - \t final status: UNDEFINED\n",
      "[2019-08-17 03:15:55,695] {bash_operator.py:127} INFO - \t tracking URL: http://ip-172-31-16-7.ap-northeast-2.compute.internal:20888/proxy/application_1566004128108_0033/\n",
      "[2019-08-17 03:15:55,695] {bash_operator.py:127} INFO - \t user: ubuntu\n",
      "[2019-08-17 03:15:55,697] {bash_operator.py:127} INFO - 2019-08-17 03:15:55 INFO  YarnClientSchedulerBackend:54 - Application application_1566004128108_0033 has started running.\n",
      "[2019-08-17 03:15:55,708] {bash_operator.py:127} INFO - 2019-08-17 03:15:55 INFO  Utils:54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34313.\n",
      "[2019-08-17 03:15:55,709] {bash_operator.py:127} INFO - 2019-08-17 03:15:55 INFO  NettyBlockTransferService:54 - Server created on ip-172-31-30-20.ap-northeast-2.compute.internal:34313\n",
      "[2019-08-17 03:15:55,710] {bash_operator.py:127} INFO - 2019-08-17 03:15:55 INFO  BlockManager:54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "[2019-08-17 03:15:55,733] {bash_operator.py:127} INFO - 2019-08-17 03:15:55 INFO  BlockManagerMaster:54 - Registering BlockManager BlockManagerId(driver, ip-172-31-30-20.ap-northeast-2.compute.internal, 34313, None)\n",
      "[2019-08-17 03:15:55,737] {bash_operator.py:127} INFO - 2019-08-17 03:15:55 INFO  BlockManagerMasterEndpoint:54 - Registering block manager ip-172-31-30-20.ap-northeast-2.compute.internal:34313 with 366.3 MB RAM, BlockManagerId(driver, ip-172-31-30-20.ap-northeast-2.compute.internal, 34313, None)\n",
      "[2019-08-17 03:15:55,741] {bash_operator.py:127} INFO - 2019-08-17 03:15:55 INFO  BlockManagerMaster:54 - Registered BlockManager BlockManagerId(driver, ip-172-31-30-20.ap-northeast-2.compute.internal, 34313, None)\n",
      "[2019-08-17 03:15:55,741] {bash_operator.py:127} INFO - 2019-08-17 03:15:55 INFO  BlockManager:54 - Initialized BlockManager: BlockManagerId(driver, ip-172-31-30-20.ap-northeast-2.compute.internal, 34313, None)\n",
      "[2019-08-17 03:15:55,790] {bash_operator.py:127} INFO - 2019-08-17 03:15:55 INFO  YarnSchedulerBackend$YarnSchedulerEndpoint:54 - ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\n",
      "[2019-08-17 03:15:55,992] {bash_operator.py:127} INFO - 2019-08-17 03:15:55 INFO  JettyUtils:54 - Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /metrics/json.\n",
      "[2019-08-17 03:15:55,993] {bash_operator.py:127} INFO - 2019-08-17 03:15:55 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@6cf1b220{/metrics/json,null,AVAILABLE,@Spark}\n",
      "[2019-08-17 03:16:00,371] {bash_operator.py:127} INFO - 2019-08-17 03:16:00 INFO  YarnSchedulerBackend$YarnDriverEndpoint:54 - Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.31.29.159:58394) with ID 2\n",
      "[2019-08-17 03:16:00,513] {bash_operator.py:127} INFO - 2019-08-17 03:16:00 INFO  BlockManagerMasterEndpoint:54 - Registering block manager ip-172-31-29-159.ap-northeast-2.compute.internal:35703 with 9.4 GB RAM, BlockManagerId(2, ip-172-31-29-159.ap-northeast-2.compute.internal, 35703, None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-08-17 03:16:00,786] {bash_operator.py:127} INFO - 2019-08-17 03:16:00 INFO  YarnSchedulerBackend$YarnDriverEndpoint:54 - Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.31.18.11:32922) with ID 1\n",
      "[2019-08-17 03:16:00,916] {bash_operator.py:127} INFO - 2019-08-17 03:16:00 INFO  BlockManagerMasterEndpoint:54 - Registering block manager ip-172-31-18-11.ap-northeast-2.compute.internal:38933 with 9.4 GB RAM, BlockManagerId(1, ip-172-31-18-11.ap-northeast-2.compute.internal, 38933, None)\n",
      "[2019-08-17 03:16:12,858] {bash_operator.py:127} INFO - 2019-08-17 03:16:12 INFO  YarnClientSchedulerBackend:54 - SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000(ms)\n",
      "[2019-08-17 03:16:13,114] {bash_operator.py:127} INFO - 2019-08-17 03:16:13 INFO  SharedState:54 - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/tmp/airflowtmpc5vgkd_r/spark-warehouse').\n",
      "[2019-08-17 03:16:13,114] {bash_operator.py:127} INFO - 2019-08-17 03:16:13 INFO  SharedState:54 - Warehouse path is 'file:/tmp/airflowtmpc5vgkd_r/spark-warehouse'.\n",
      "[2019-08-17 03:16:13,122] {bash_operator.py:127} INFO - 2019-08-17 03:16:13 INFO  JettyUtils:54 - Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL.\n",
      "[2019-08-17 03:16:13,123] {bash_operator.py:127} INFO - 2019-08-17 03:16:13 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@c214781{/SQL,null,AVAILABLE,@Spark}\n",
      "[2019-08-17 03:16:13,123] {bash_operator.py:127} INFO - 2019-08-17 03:16:13 INFO  JettyUtils:54 - Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/json.\n",
      "[2019-08-17 03:16:13,123] {bash_operator.py:127} INFO - 2019-08-17 03:16:13 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@26023177{/SQL/json,null,AVAILABLE,@Spark}\n",
      "[2019-08-17 03:16:13,124] {bash_operator.py:127} INFO - 2019-08-17 03:16:13 INFO  JettyUtils:54 - Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/execution.\n",
      "[2019-08-17 03:16:13,124] {bash_operator.py:127} INFO - 2019-08-17 03:16:13 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@717b9726{/SQL/execution,null,AVAILABLE,@Spark}\n",
      "[2019-08-17 03:16:13,124] {bash_operator.py:127} INFO - 2019-08-17 03:16:13 INFO  JettyUtils:54 - Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/execution/json.\n",
      "[2019-08-17 03:16:13,125] {bash_operator.py:127} INFO - 2019-08-17 03:16:13 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@92413c6{/SQL/execution/json,null,AVAILABLE,@Spark}\n",
      "[2019-08-17 03:16:13,126] {bash_operator.py:127} INFO - 2019-08-17 03:16:13 INFO  JettyUtils:54 - Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /static/sql.\n",
      "[2019-08-17 03:16:13,126] {bash_operator.py:127} INFO - 2019-08-17 03:16:13 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@531ef5cc{/static/sql,null,AVAILABLE,@Spark}\n",
      "[2019-08-17 03:16:13,536] {bash_operator.py:127} INFO - 2019-08-17 03:16:13 INFO  StateStoreCoordinatorRef:54 - Registered StateStoreCoordinator endpoint\n",
      "[2019-08-17 03:16:13,992] {bash_operator.py:127} INFO - 2019-08-17 03:16:13 INFO  SparkContext:54 - Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "[2019-08-17 03:16:14,012] {bash_operator.py:127} INFO - 2019-08-17 03:16:14 INFO  DAGScheduler:54 - Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "[2019-08-17 03:16:14,013] {bash_operator.py:127} INFO - 2019-08-17 03:16:14 INFO  DAGScheduler:54 - Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "[2019-08-17 03:16:14,019] {bash_operator.py:127} INFO - 2019-08-17 03:16:14 INFO  DAGScheduler:54 - Parents of final stage: List()\n",
      "[2019-08-17 03:16:14,021] {bash_operator.py:127} INFO - 2019-08-17 03:16:14 INFO  DAGScheduler:54 - Missing parents: List()\n",
      "[2019-08-17 03:16:14,028] {bash_operator.py:127} INFO - 2019-08-17 03:16:14 INFO  DAGScheduler:54 - Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "[2019-08-17 03:16:14,109] {bash_operator.py:127} INFO - 2019-08-17 03:16:14 INFO  MemoryStore:54 - Block broadcast_0 stored as values in memory (estimated size 100.6 KB, free 366.2 MB)\n",
      "[2019-08-17 03:16:14,125] {bash_operator.py:127} INFO - 2019-08-17 03:16:14 INFO  MemoryStore:54 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.7 KB, free 366.2 MB)\n",
      "[2019-08-17 03:16:14,129] {bash_operator.py:127} INFO - 2019-08-17 03:16:14 INFO  BlockManagerInfo:54 - Added broadcast_0_piece0 in memory on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 (size: 34.7 KB, free: 366.3 MB)\n",
      "[2019-08-17 03:16:14,133] {bash_operator.py:127} INFO - 2019-08-17 03:16:14 INFO  SparkContext:54 - Created broadcast 0 from broadcast at DAGScheduler.scala:1161\n",
      "[2019-08-17 03:16:14,152] {bash_operator.py:127} INFO - 2019-08-17 03:16:14 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "[2019-08-17 03:16:14,153] {bash_operator.py:127} INFO - 2019-08-17 03:16:14 INFO  YarnScheduler:54 - Adding task set 0.0 with 1 tasks\n",
      "[2019-08-17 03:16:14,186] {bash_operator.py:127} INFO - 2019-08-17 03:16:14 INFO  TaskSetManager:54 - Starting task 0.0 in stage 0.0 (TID 0, ip-172-31-18-11.ap-northeast-2.compute.internal, executor 1, partition 0, PROCESS_LOCAL, 8074 bytes)\n",
      "[2019-08-17 03:16:14,461] {bash_operator.py:127} INFO - 2019-08-17 03:16:14 INFO  BlockManagerInfo:54 - Added broadcast_0_piece0 in memory on ip-172-31-18-11.ap-northeast-2.compute.internal:38933 (size: 34.7 KB, free: 9.4 GB)\n",
      "[2019-08-17 03:16:15,905] {bash_operator.py:127} INFO - 2019-08-17 03:16:15 INFO  TaskSetManager:54 - Finished task 0.0 in stage 0.0 (TID 0) in 1732 ms on ip-172-31-18-11.ap-northeast-2.compute.internal (executor 1) (1/1)\n",
      "[2019-08-17 03:16:15,913] {bash_operator.py:127} INFO - 2019-08-17 03:16:15 INFO  YarnScheduler:54 - Removed TaskSet 0.0, whose tasks have all completed, from pool\n",
      "[2019-08-17 03:16:15,923] {bash_operator.py:127} INFO - 2019-08-17 03:16:15 INFO  DAGScheduler:54 - ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.860 s\n",
      "[2019-08-17 03:16:15,932] {bash_operator.py:127} INFO - 2019-08-17 03:16:15 INFO  DAGScheduler:54 - Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.936703 s\n",
      "[2019-08-17 03:16:15,987] {bash_operator.py:127} INFO - 2019-08-17 03:16:15 INFO  ContextCleaner:54 - Cleaned accumulator 13\n",
      "[2019-08-17 03:16:16,023] {bash_operator.py:127} INFO - 2019-08-17 03:16:16 INFO  BlockManagerInfo:54 - Removed broadcast_0_piece0 on ip-172-31-18-11.ap-northeast-2.compute.internal:38933 in memory (size: 34.7 KB, free: 9.4 GB)\n",
      "[2019-08-17 03:16:16,034] {bash_operator.py:127} INFO - 2019-08-17 03:16:16 INFO  BlockManagerInfo:54 - Removed broadcast_0_piece0 on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 in memory (size: 34.7 KB, free: 366.3 MB)\n",
      "[2019-08-17 03:16:16,045] {bash_operator.py:127} INFO - 2019-08-17 03:16:16 INFO  ContextCleaner:54 - Cleaned accumulator 4\n",
      "[2019-08-17 03:16:16,045] {bash_operator.py:127} INFO - 2019-08-17 03:16:16 INFO  ContextCleaner:54 - Cleaned accumulator 10\n",
      "[2019-08-17 03:16:16,045] {bash_operator.py:127} INFO - 2019-08-17 03:16:16 INFO  ContextCleaner:54 - Cleaned accumulator 6\n",
      "[2019-08-17 03:16:16,045] {bash_operator.py:127} INFO - 2019-08-17 03:16:16 INFO  ContextCleaner:54 - Cleaned accumulator 17\n",
      "[2019-08-17 03:16:16,047] {bash_operator.py:127} INFO - 2019-08-17 03:16:16 INFO  ContextCleaner:54 - Cleaned accumulator 11\n",
      "[2019-08-17 03:16:16,047] {bash_operator.py:127} INFO - 2019-08-17 03:16:16 INFO  ContextCleaner:54 - Cleaned accumulator 16\n",
      "[2019-08-17 03:16:16,047] {bash_operator.py:127} INFO - 2019-08-17 03:16:16 INFO  ContextCleaner:54 - Cleaned accumulator 1\n",
      "[2019-08-17 03:16:16,047] {bash_operator.py:127} INFO - 2019-08-17 03:16:16 INFO  ContextCleaner:54 - Cleaned accumulator 12\n",
      "[2019-08-17 03:16:16,047] {bash_operator.py:127} INFO - 2019-08-17 03:16:16 INFO  ContextCleaner:54 - Cleaned accumulator 8\n",
      "[2019-08-17 03:16:16,047] {bash_operator.py:127} INFO - 2019-08-17 03:16:16 INFO  ContextCleaner:54 - Cleaned accumulator 22\n",
      "[2019-08-17 03:16:16,047] {bash_operator.py:127} INFO - 2019-08-17 03:16:16 INFO  ContextCleaner:54 - Cleaned accumulator 5\n",
      "[2019-08-17 03:16:16,047] {bash_operator.py:127} INFO - 2019-08-17 03:16:16 INFO  ContextCleaner:54 - Cleaned accumulator 7\n",
      "[2019-08-17 03:16:16,047] {bash_operator.py:127} INFO - 2019-08-17 03:16:16 INFO  ContextCleaner:54 - Cleaned accumulator 3\n",
      "[2019-08-17 03:16:16,047] {bash_operator.py:127} INFO - 2019-08-17 03:16:16 INFO  ContextCleaner:54 - Cleaned accumulator 21\n",
      "[2019-08-17 03:16:16,047] {bash_operator.py:127} INFO - 2019-08-17 03:16:16 INFO  ContextCleaner:54 - Cleaned accumulator 23\n",
      "[2019-08-17 03:16:16,047] {bash_operator.py:127} INFO - 2019-08-17 03:16:16 INFO  ContextCleaner:54 - Cleaned accumulator 14\n",
      "[2019-08-17 03:16:16,047] {bash_operator.py:127} INFO - 2019-08-17 03:16:16 INFO  ContextCleaner:54 - Cleaned accumulator 18\n",
      "[2019-08-17 03:16:16,047] {bash_operator.py:127} INFO - 2019-08-17 03:16:16 INFO  ContextCleaner:54 - Cleaned accumulator 20\n",
      "[2019-08-17 03:16:16,047] {bash_operator.py:127} INFO - 2019-08-17 03:16:16 INFO  ContextCleaner:54 - Cleaned accumulator 9\n",
      "[2019-08-17 03:16:16,047] {bash_operator.py:127} INFO - 2019-08-17 03:16:16 INFO  ContextCleaner:54 - Cleaned accumulator 2\n",
      "[2019-08-17 03:16:16,047] {bash_operator.py:127} INFO - 2019-08-17 03:16:16 INFO  ContextCleaner:54 - Cleaned accumulator 15\n",
      "[2019-08-17 03:16:16,047] {bash_operator.py:127} INFO - 2019-08-17 03:16:16 INFO  ContextCleaner:54 - Cleaned accumulator 25\n",
      "[2019-08-17 03:16:16,047] {bash_operator.py:127} INFO - 2019-08-17 03:16:16 INFO  ContextCleaner:54 - Cleaned accumulator 19\n",
      "[2019-08-17 03:16:16,047] {bash_operator.py:127} INFO - 2019-08-17 03:16:16 INFO  ContextCleaner:54 - Cleaned accumulator 24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-08-17 03:16:16,544] {bash_operator.py:127} INFO - 2019-08-17 03:16:16 WARN  Utils:66 - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.\n",
      "[2019-08-17 03:16:16,749] {bash_operator.py:127} INFO - 2019-08-17 03:16:16 INFO  FileSourceStrategy:54 - Pruning directories with:\n",
      "[2019-08-17 03:16:16,752] {bash_operator.py:127} INFO - 2019-08-17 03:16:16 INFO  FileSourceStrategy:54 - Post-Scan Filters:\n",
      "[2019-08-17 03:16:16,755] {bash_operator.py:127} INFO - 2019-08-17 03:16:16 INFO  FileSourceStrategy:54 - Output Data Schema: struct<month: string, dayofmonth: string, dayofweek: string, flightdate: string, carrier: string ... 11 more fields>\n",
      "[2019-08-17 03:16:16,761] {bash_operator.py:127} INFO - 2019-08-17 03:16:16 INFO  FileSourceScanExec:54 - Pushed Filters:\n",
      "[2019-08-17 03:16:17,148] {bash_operator.py:127} INFO - 2019-08-17 03:16:17 INFO  CodeGenerator:54 - Code generated in 247.852818 ms\n",
      "[2019-08-17 03:16:17,204] {bash_operator.py:127} INFO - 2019-08-17 03:16:17 INFO  MemoryStore:54 - Block broadcast_1 stored as values in memory (estimated size 372.4 KB, free 365.9 MB)\n",
      "[2019-08-17 03:16:17,224] {bash_operator.py:127} INFO - 2019-08-17 03:16:17 INFO  MemoryStore:54 - Block broadcast_1_piece0 stored as bytes in memory (estimated size 34.0 KB, free 365.9 MB)\n",
      "[2019-08-17 03:16:17,225] {bash_operator.py:127} INFO - 2019-08-17 03:16:17 INFO  BlockManagerInfo:54 - Added broadcast_1_piece0 in memory on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 (size: 34.0 KB, free: 366.3 MB)\n",
      "[2019-08-17 03:16:17,227] {bash_operator.py:127} INFO - 2019-08-17 03:16:17 INFO  SparkContext:54 - Created broadcast 1 from javaToPython at NativeMethodAccessorImpl.java:0\n",
      "[2019-08-17 03:16:17,237] {bash_operator.py:127} INFO - 2019-08-17 03:16:17 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "[2019-08-17 03:16:17,642] {bash_operator.py:127} INFO - 2019-08-17 03:16:17 INFO  SparkContext:54 - Starting job: runJob at PythonRDD.scala:153\n",
      "[2019-08-17 03:16:17,646] {bash_operator.py:127} INFO - 2019-08-17 03:16:17 INFO  DAGScheduler:54 - Got job 1 (runJob at PythonRDD.scala:153) with 1 output partitions\n",
      "[2019-08-17 03:16:17,647] {bash_operator.py:127} INFO - 2019-08-17 03:16:17 INFO  DAGScheduler:54 - Final stage: ResultStage 1 (runJob at PythonRDD.scala:153)\n",
      "[2019-08-17 03:16:17,647] {bash_operator.py:127} INFO - 2019-08-17 03:16:17 INFO  DAGScheduler:54 - Parents of final stage: List()\n",
      "[2019-08-17 03:16:17,647] {bash_operator.py:127} INFO - 2019-08-17 03:16:17 INFO  DAGScheduler:54 - Missing parents: List()\n",
      "[2019-08-17 03:16:17,647] {bash_operator.py:127} INFO - 2019-08-17 03:16:17 INFO  DAGScheduler:54 - Submitting ResultStage 1 (PythonRDD[6] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "[2019-08-17 03:16:17,679] {bash_operator.py:127} INFO - 2019-08-17 03:16:17 INFO  MemoryStore:54 - Block broadcast_2 stored as values in memory (estimated size 23.7 KB, free 365.9 MB)\n",
      "[2019-08-17 03:16:17,682] {bash_operator.py:127} INFO - 2019-08-17 03:16:17 INFO  MemoryStore:54 - Block broadcast_2_piece0 stored as bytes in memory (estimated size 10.8 KB, free 365.9 MB)\n",
      "[2019-08-17 03:16:17,682] {bash_operator.py:127} INFO - 2019-08-17 03:16:17 INFO  BlockManagerInfo:54 - Added broadcast_2_piece0 in memory on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 (size: 10.8 KB, free: 366.3 MB)\n",
      "[2019-08-17 03:16:17,683] {bash_operator.py:127} INFO - 2019-08-17 03:16:17 INFO  SparkContext:54 - Created broadcast 2 from broadcast at DAGScheduler.scala:1161\n",
      "[2019-08-17 03:16:17,684] {bash_operator.py:127} INFO - 2019-08-17 03:16:17 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 1 (PythonRDD[6] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "[2019-08-17 03:16:17,685] {bash_operator.py:127} INFO - 2019-08-17 03:16:17 INFO  YarnScheduler:54 - Adding task set 1.0 with 1 tasks\n",
      "[2019-08-17 03:16:17,693] {bash_operator.py:127} INFO - 2019-08-17 03:16:17 INFO  TaskSetManager:54 - Starting task 0.0 in stage 1.0 (TID 1, ip-172-31-29-159.ap-northeast-2.compute.internal, executor 2, partition 0, NODE_LOCAL, 8385 bytes)\n",
      "[2019-08-17 03:16:17,945] {bash_operator.py:127} INFO - 2019-08-17 03:16:17 INFO  BlockManagerInfo:54 - Added broadcast_2_piece0 in memory on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 (size: 10.8 KB, free: 9.4 GB)\n",
      "[2019-08-17 03:16:19,591] {bash_operator.py:127} INFO - 2019-08-17 03:16:19 INFO  BlockManagerInfo:54 - Added broadcast_1_piece0 in memory on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 (size: 34.0 KB, free: 9.4 GB)\n",
      "[2019-08-17 03:16:21,470] {bash_operator.py:127} INFO - 2019-08-17 03:16:21 INFO  TaskSetManager:54 - Finished task 0.0 in stage 1.0 (TID 1) in 3780 ms on ip-172-31-29-159.ap-northeast-2.compute.internal (executor 2) (1/1)\n",
      "[2019-08-17 03:16:21,473] {bash_operator.py:127} INFO - 2019-08-17 03:16:21 INFO  PythonAccumulatorV2:54 - Connected to AccumulatorServer at host: 127.0.0.1 port: 55183\n",
      "[2019-08-17 03:16:21,477] {bash_operator.py:127} INFO - 2019-08-17 03:16:21 INFO  DAGScheduler:54 - ResultStage 1 (runJob at PythonRDD.scala:153) finished in 3.822 s\n",
      "[2019-08-17 03:16:21,478] {bash_operator.py:127} INFO - 2019-08-17 03:16:21 INFO  YarnScheduler:54 - Removed TaskSet 1.0, whose tasks have all completed, from pool\n",
      "[2019-08-17 03:16:21,478] {bash_operator.py:127} INFO - 2019-08-17 03:16:21 INFO  DAGScheduler:54 - Job 1 finished: runJob at PythonRDD.scala:153, took 3.832274 s\n",
      "[2019-08-17 03:16:21,482] {bash_operator.py:127} INFO - /home/ubuntu/project1/spark/python/lib/pyspark.zip/pyspark/sql/session.py:366: UserWarning: Using RDD of dict to inferSchema is deprecated. Use pyspark.sql.Row instead\n",
      "[2019-08-17 03:16:22,166] {bash_operator.py:127} INFO - /models/arrival_bucketizer_2.0.bin\n",
      "[2019-08-17 03:16:22,186] {bash_operator.py:127} INFO - 2019-08-17 03:16:22 INFO  FileSystemOverwrite:54 - Path /models/arrival_bucketizer_2.0.bin already exists. It will be overwritten.\n",
      "[2019-08-17 03:16:22,232] {bash_operator.py:127} INFO - 2019-08-17 03:16:22 INFO  deprecation:1285 - mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir\n",
      "[2019-08-17 03:16:22,236] {bash_operator.py:127} INFO - 2019-08-17 03:16:22 INFO  HadoopMapRedCommitProtocol:54 - Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "[2019-08-17 03:16:22,237] {bash_operator.py:127} INFO - 2019-08-17 03:16:22 INFO  FileOutputCommitter:123 - File Output Committer Algorithm version is 1\n",
      "[2019-08-17 03:16:22,237] {bash_operator.py:127} INFO - 2019-08-17 03:16:22 INFO  FileOutputCommitter:138 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "[2019-08-17 03:16:22,254] {bash_operator.py:127} INFO - 2019-08-17 03:16:22 INFO  SparkContext:54 - Starting job: runJob at SparkHadoopWriter.scala:78\n",
      "[2019-08-17 03:16:22,256] {bash_operator.py:127} INFO - 2019-08-17 03:16:22 INFO  DAGScheduler:54 - Got job 2 (runJob at SparkHadoopWriter.scala:78) with 1 output partitions\n",
      "[2019-08-17 03:16:22,256] {bash_operator.py:127} INFO - 2019-08-17 03:16:22 INFO  DAGScheduler:54 - Final stage: ResultStage 2 (runJob at SparkHadoopWriter.scala:78)\n",
      "[2019-08-17 03:16:22,256] {bash_operator.py:127} INFO - 2019-08-17 03:16:22 INFO  DAGScheduler:54 - Parents of final stage: List()\n",
      "[2019-08-17 03:16:22,256] {bash_operator.py:127} INFO - 2019-08-17 03:16:22 INFO  DAGScheduler:54 - Missing parents: List()\n",
      "[2019-08-17 03:16:22,256] {bash_operator.py:127} INFO - 2019-08-17 03:16:22 INFO  DAGScheduler:54 - Submitting ResultStage 2 (MapPartitionsRDD[12] at saveAsTextFile at ReadWrite.scala:441), which has no missing parents\n",
      "[2019-08-17 03:16:22,266] {bash_operator.py:127} INFO - 2019-08-17 03:16:22 INFO  MemoryStore:54 - Block broadcast_3 stored as values in memory (estimated size 86.1 KB, free 365.8 MB)\n",
      "[2019-08-17 03:16:22,268] {bash_operator.py:127} INFO - 2019-08-17 03:16:22 INFO  MemoryStore:54 - Block broadcast_3_piece0 stored as bytes in memory (estimated size 30.3 KB, free 365.8 MB)\n",
      "[2019-08-17 03:16:22,269] {bash_operator.py:127} INFO - 2019-08-17 03:16:22 INFO  BlockManagerInfo:54 - Added broadcast_3_piece0 in memory on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 (size: 30.3 KB, free: 366.2 MB)\n",
      "[2019-08-17 03:16:22,269] {bash_operator.py:127} INFO - 2019-08-17 03:16:22 INFO  SparkContext:54 - Created broadcast 3 from broadcast at DAGScheduler.scala:1161\n",
      "[2019-08-17 03:16:22,270] {bash_operator.py:127} INFO - 2019-08-17 03:16:22 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[12] at saveAsTextFile at ReadWrite.scala:441) (first 15 tasks are for partitions Vector(0))\n",
      "[2019-08-17 03:16:22,270] {bash_operator.py:127} INFO - 2019-08-17 03:16:22 INFO  YarnScheduler:54 - Adding task set 2.0 with 1 tasks\n",
      "[2019-08-17 03:16:22,272] {bash_operator.py:127} INFO - 2019-08-17 03:16:22 INFO  TaskSetManager:54 - Starting task 0.0 in stage 2.0 (TID 2, ip-172-31-18-11.ap-northeast-2.compute.internal, executor 1, partition 0, PROCESS_LOCAL, 8206 bytes)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-08-17 03:16:22,333] {bash_operator.py:127} INFO - 2019-08-17 03:16:22 INFO  BlockManagerInfo:54 - Added broadcast_3_piece0 in memory on ip-172-31-18-11.ap-northeast-2.compute.internal:38933 (size: 30.3 KB, free: 9.4 GB)\n",
      "[2019-08-17 03:16:22,533] {bash_operator.py:127} INFO - 2019-08-17 03:16:22 INFO  TaskSetManager:54 - Finished task 0.0 in stage 2.0 (TID 2) in 262 ms on ip-172-31-18-11.ap-northeast-2.compute.internal (executor 1) (1/1)\n",
      "[2019-08-17 03:16:22,533] {bash_operator.py:127} INFO - 2019-08-17 03:16:22 INFO  YarnScheduler:54 - Removed TaskSet 2.0, whose tasks have all completed, from pool\n",
      "[2019-08-17 03:16:22,534] {bash_operator.py:127} INFO - 2019-08-17 03:16:22 INFO  DAGScheduler:54 - ResultStage 2 (runJob at SparkHadoopWriter.scala:78) finished in 0.278 s\n",
      "[2019-08-17 03:16:22,537] {bash_operator.py:127} INFO - 2019-08-17 03:16:22 INFO  DAGScheduler:54 - Job 2 finished: runJob at SparkHadoopWriter.scala:78, took 0.280084 s\n",
      "[2019-08-17 03:16:22,552] {bash_operator.py:127} INFO - 2019-08-17 03:16:22 INFO  SparkHadoopWriter:54 - Job job_20190817031622_0012 committed.\n",
      "[2019-08-17 03:16:22,862] {bash_operator.py:127} INFO - 2019-08-17 03:16:22 INFO  ContextCleaner:54 - Cleaned accumulator 46\n",
      "[2019-08-17 03:16:22,862] {bash_operator.py:127} INFO - 2019-08-17 03:16:22 INFO  ContextCleaner:54 - Cleaned accumulator 56\n",
      "[2019-08-17 03:16:22,862] {bash_operator.py:127} INFO - 2019-08-17 03:16:22 INFO  ContextCleaner:54 - Cleaned accumulator 40\n",
      "[2019-08-17 03:16:22,862] {bash_operator.py:127} INFO - 2019-08-17 03:16:22 INFO  ContextCleaner:54 - Cleaned accumulator 66\n",
      "[2019-08-17 03:16:22,862] {bash_operator.py:127} INFO - 2019-08-17 03:16:22 INFO  ContextCleaner:54 - Cleaned accumulator 44\n",
      "[2019-08-17 03:16:22,862] {bash_operator.py:127} INFO - 2019-08-17 03:16:22 INFO  ContextCleaner:54 - Cleaned accumulator 78\n",
      "[2019-08-17 03:16:22,862] {bash_operator.py:127} INFO - 2019-08-17 03:16:22 INFO  ContextCleaner:54 - Cleaned accumulator 49\n",
      "[2019-08-17 03:16:22,862] {bash_operator.py:127} INFO - 2019-08-17 03:16:22 INFO  ContextCleaner:54 - Cleaned accumulator 79\n",
      "[2019-08-17 03:16:22,862] {bash_operator.py:127} INFO - 2019-08-17 03:16:22 INFO  ContextCleaner:54 - Cleaned accumulator 34\n",
      "[2019-08-17 03:16:22,863] {bash_operator.py:127} INFO - 2019-08-17 03:16:22 INFO  ContextCleaner:54 - Cleaned accumulator 53\n",
      "[2019-08-17 03:16:22,863] {bash_operator.py:127} INFO - 2019-08-17 03:16:22 INFO  ContextCleaner:54 - Cleaned accumulator 37\n",
      "[2019-08-17 03:16:22,863] {bash_operator.py:127} INFO - 2019-08-17 03:16:22 INFO  ContextCleaner:54 - Cleaned accumulator 65\n",
      "[2019-08-17 03:16:22,863] {bash_operator.py:127} INFO - 2019-08-17 03:16:22 INFO  ContextCleaner:54 - Cleaned accumulator 73\n",
      "[2019-08-17 03:16:22,863] {bash_operator.py:127} INFO - 2019-08-17 03:16:22 INFO  ContextCleaner:54 - Cleaned accumulator 55\n",
      "[2019-08-17 03:16:22,863] {bash_operator.py:127} INFO - 2019-08-17 03:16:22 INFO  ContextCleaner:54 - Cleaned accumulator 43\n",
      "[2019-08-17 03:16:22,863] {bash_operator.py:127} INFO - 2019-08-17 03:16:22 INFO  ContextCleaner:54 - Cleaned accumulator 72\n",
      "[2019-08-17 03:16:22,863] {bash_operator.py:127} INFO - 2019-08-17 03:16:22 INFO  ContextCleaner:54 - Cleaned accumulator 57\n",
      "[2019-08-17 03:16:22,863] {bash_operator.py:127} INFO - 2019-08-17 03:16:22 INFO  ContextCleaner:54 - Cleaned accumulator 45\n",
      "[2019-08-17 03:16:22,863] {bash_operator.py:127} INFO - 2019-08-17 03:16:22 INFO  ContextCleaner:54 - Cleaned accumulator 47\n",
      "[2019-08-17 03:16:22,875] {bash_operator.py:127} INFO - 2019-08-17 03:16:22 INFO  BlockManagerInfo:54 - Removed broadcast_3_piece0 on ip-172-31-18-11.ap-northeast-2.compute.internal:38933 in memory (size: 30.3 KB, free: 9.4 GB)\n",
      "[2019-08-17 03:16:22,907] {bash_operator.py:127} INFO - 2019-08-17 03:16:22 INFO  BlockManagerInfo:54 - Removed broadcast_3_piece0 on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 in memory (size: 30.3 KB, free: 366.3 MB)\n",
      "[2019-08-17 03:16:22,956] {bash_operator.py:127} INFO - 2019-08-17 03:16:22 INFO  CodeGenerator:54 - Code generated in 13.275757 ms\n",
      "[2019-08-17 03:16:22,983] {bash_operator.py:127} INFO - 2019-08-17 03:16:22 INFO  ContextCleaner:54 - Cleaned accumulator 58\n",
      "[2019-08-17 03:16:22,983] {bash_operator.py:127} INFO - 2019-08-17 03:16:22 INFO  ContextCleaner:54 - Cleaned accumulator 54\n",
      "[2019-08-17 03:16:22,983] {bash_operator.py:127} INFO - 2019-08-17 03:16:22 INFO  ContextCleaner:54 - Cleaned accumulator 77\n",
      "[2019-08-17 03:16:22,983] {bash_operator.py:127} INFO - 2019-08-17 03:16:22 INFO  ContextCleaner:54 - Cleaned accumulator 51\n",
      "[2019-08-17 03:16:22,983] {bash_operator.py:127} INFO - 2019-08-17 03:16:22 INFO  ContextCleaner:54 - Cleaned accumulator 61\n",
      "[2019-08-17 03:16:22,983] {bash_operator.py:127} INFO - 2019-08-17 03:16:22 INFO  ContextCleaner:54 - Cleaned accumulator 76\n",
      "[2019-08-17 03:16:22,983] {bash_operator.py:127} INFO - 2019-08-17 03:16:22 INFO  ContextCleaner:54 - Cleaned accumulator 71\n",
      "[2019-08-17 03:16:22,984] {bash_operator.py:127} INFO - 2019-08-17 03:16:22 INFO  ContextCleaner:54 - Cleaned accumulator 68\n",
      "[2019-08-17 03:16:22,984] {bash_operator.py:127} INFO - 2019-08-17 03:16:22 INFO  ContextCleaner:54 - Cleaned accumulator 74\n",
      "[2019-08-17 03:16:22,984] {bash_operator.py:127} INFO - 2019-08-17 03:16:22 INFO  ContextCleaner:54 - Cleaned accumulator 31\n",
      "[2019-08-17 03:16:22,984] {bash_operator.py:127} INFO - 2019-08-17 03:16:22 INFO  ContextCleaner:54 - Cleaned accumulator 36\n",
      "[2019-08-17 03:16:22,985] {bash_operator.py:127} INFO - 2019-08-17 03:16:22 INFO  ContextCleaner:54 - Cleaned accumulator 35\n",
      "[2019-08-17 03:16:22,985] {bash_operator.py:127} INFO - 2019-08-17 03:16:22 INFO  ContextCleaner:54 - Cleaned accumulator 50\n",
      "[2019-08-17 03:16:22,985] {bash_operator.py:127} INFO - 2019-08-17 03:16:22 INFO  ContextCleaner:54 - Cleaned accumulator 67\n",
      "[2019-08-17 03:16:22,985] {bash_operator.py:127} INFO - 2019-08-17 03:16:22 INFO  ContextCleaner:54 - Cleaned accumulator 32\n",
      "[2019-08-17 03:16:22,985] {bash_operator.py:127} INFO - 2019-08-17 03:16:22 INFO  ContextCleaner:54 - Cleaned accumulator 59\n",
      "[2019-08-17 03:16:22,985] {bash_operator.py:127} INFO - 2019-08-17 03:16:22 INFO  ContextCleaner:54 - Cleaned accumulator 39\n",
      "[2019-08-17 03:16:22,988] {bash_operator.py:127} INFO - 2019-08-17 03:16:22 INFO  ContextCleaner:54 - Cleaned accumulator 60\n",
      "[2019-08-17 03:16:22,988] {bash_operator.py:127} INFO - 2019-08-17 03:16:22 INFO  ContextCleaner:54 - Cleaned accumulator 64\n",
      "[2019-08-17 03:16:22,988] {bash_operator.py:127} INFO - 2019-08-17 03:16:22 INFO  ContextCleaner:54 - Cleaned accumulator 33\n",
      "[2019-08-17 03:16:22,992] {bash_operator.py:127} INFO - 2019-08-17 03:16:22 INFO  BlockManagerInfo:54 - Removed broadcast_2_piece0 on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 in memory (size: 10.8 KB, free: 366.3 MB)\n",
      "[2019-08-17 03:16:22,997] {bash_operator.py:127} INFO - 2019-08-17 03:16:22 INFO  BlockManagerInfo:54 - Removed broadcast_2_piece0 on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 in memory (size: 10.8 KB, free: 9.4 GB)\n",
      "[2019-08-17 03:16:23,018] {bash_operator.py:127} INFO - 2019-08-17 03:16:23 INFO  SparkContext:54 - Starting job: countByValue at StringIndexer.scala:140\n",
      "[2019-08-17 03:16:23,081] {bash_operator.py:127} INFO - 2019-08-17 03:16:23 INFO  ContextCleaner:54 - Cleaned accumulator 42\n",
      "[2019-08-17 03:16:23,082] {bash_operator.py:127} INFO - 2019-08-17 03:16:23 INFO  ContextCleaner:54 - Cleaned accumulator 38\n",
      "[2019-08-17 03:16:23,082] {bash_operator.py:127} INFO - 2019-08-17 03:16:23 INFO  ContextCleaner:54 - Cleaned accumulator 28\n",
      "[2019-08-17 03:16:23,082] {bash_operator.py:127} INFO - 2019-08-17 03:16:23 INFO  ContextCleaner:54 - Cleaned accumulator 52\n",
      "[2019-08-17 03:16:23,082] {bash_operator.py:127} INFO - 2019-08-17 03:16:23 INFO  ContextCleaner:54 - Cleaned accumulator 48\n",
      "[2019-08-17 03:16:23,082] {bash_operator.py:127} INFO - 2019-08-17 03:16:23 INFO  ContextCleaner:54 - Cleaned accumulator 27\n",
      "[2019-08-17 03:16:23,082] {bash_operator.py:127} INFO - 2019-08-17 03:16:23 INFO  ContextCleaner:54 - Cleaned accumulator 63\n",
      "[2019-08-17 03:16:23,082] {bash_operator.py:127} INFO - 2019-08-17 03:16:23 INFO  ContextCleaner:54 - Cleaned accumulator 69\n",
      "[2019-08-17 03:16:23,082] {bash_operator.py:127} INFO - 2019-08-17 03:16:23 INFO  ContextCleaner:54 - Cleaned accumulator 75\n",
      "[2019-08-17 03:16:23,082] {bash_operator.py:127} INFO - 2019-08-17 03:16:23 INFO  ContextCleaner:54 - Cleaned accumulator 62\n",
      "[2019-08-17 03:16:23,082] {bash_operator.py:127} INFO - 2019-08-17 03:16:23 INFO  ContextCleaner:54 - Cleaned accumulator 41\n",
      "[2019-08-17 03:16:23,082] {bash_operator.py:127} INFO - 2019-08-17 03:16:23 INFO  ContextCleaner:54 - Cleaned accumulator 70\n",
      "[2019-08-17 03:16:23,082] {bash_operator.py:127} INFO - 2019-08-17 03:16:23 INFO  ContextCleaner:54 - Cleaned accumulator 80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-08-17 03:16:23,215] {bash_operator.py:127} INFO - 2019-08-17 03:16:23 INFO  DAGScheduler:54 - Registering RDD 19 (countByValue at StringIndexer.scala:140)\n",
      "[2019-08-17 03:16:23,216] {bash_operator.py:127} INFO - 2019-08-17 03:16:23 INFO  DAGScheduler:54 - Got job 3 (countByValue at StringIndexer.scala:140) with 1 output partitions\n",
      "[2019-08-17 03:16:23,216] {bash_operator.py:127} INFO - 2019-08-17 03:16:23 INFO  DAGScheduler:54 - Final stage: ResultStage 4 (countByValue at StringIndexer.scala:140)\n",
      "[2019-08-17 03:16:23,216] {bash_operator.py:127} INFO - 2019-08-17 03:16:23 INFO  DAGScheduler:54 - Parents of final stage: List(ShuffleMapStage 3)\n",
      "[2019-08-17 03:16:23,217] {bash_operator.py:127} INFO - 2019-08-17 03:16:23 INFO  DAGScheduler:54 - Missing parents: List(ShuffleMapStage 3)\n",
      "[2019-08-17 03:16:23,219] {bash_operator.py:127} INFO - 2019-08-17 03:16:23 INFO  DAGScheduler:54 - Submitting ShuffleMapStage 3 (MapPartitionsRDD[19] at countByValue at StringIndexer.scala:140), which has no missing parents\n",
      "[2019-08-17 03:16:23,270] {bash_operator.py:127} INFO - 2019-08-17 03:16:23 INFO  MemoryStore:54 - Block broadcast_4 stored as values in memory (estimated size 39.0 KB, free 365.9 MB)\n",
      "[2019-08-17 03:16:23,276] {bash_operator.py:127} INFO - 2019-08-17 03:16:23 INFO  MemoryStore:54 - Block broadcast_4_piece0 stored as bytes in memory (estimated size 18.7 KB, free 365.8 MB)\n",
      "[2019-08-17 03:16:23,279] {bash_operator.py:127} INFO - 2019-08-17 03:16:23 INFO  BlockManagerInfo:54 - Added broadcast_4_piece0 in memory on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 (size: 18.7 KB, free: 366.2 MB)\n",
      "[2019-08-17 03:16:23,281] {bash_operator.py:127} INFO - 2019-08-17 03:16:23 INFO  SparkContext:54 - Created broadcast 4 from broadcast at DAGScheduler.scala:1161\n",
      "[2019-08-17 03:16:23,284] {bash_operator.py:127} INFO - 2019-08-17 03:16:23 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[19] at countByValue at StringIndexer.scala:140) (first 15 tasks are for partitions Vector(0))\n",
      "[2019-08-17 03:16:23,286] {bash_operator.py:127} INFO - 2019-08-17 03:16:23 INFO  YarnScheduler:54 - Adding task set 3.0 with 1 tasks\n",
      "[2019-08-17 03:16:23,290] {bash_operator.py:127} INFO - 2019-08-17 03:16:23 INFO  TaskSetManager:54 - Starting task 0.0 in stage 3.0 (TID 3, ip-172-31-29-159.ap-northeast-2.compute.internal, executor 2, partition 0, NODE_LOCAL, 8374 bytes)\n",
      "[2019-08-17 03:16:23,357] {bash_operator.py:127} INFO - 2019-08-17 03:16:23 INFO  BlockManagerInfo:54 - Added broadcast_4_piece0 in memory on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 (size: 18.7 KB, free: 9.4 GB)\n",
      "[2019-08-17 03:16:25,594] {bash_operator.py:127} INFO - 2019-08-17 03:16:25 INFO  TaskSetManager:54 - Finished task 0.0 in stage 3.0 (TID 3) in 2305 ms on ip-172-31-29-159.ap-northeast-2.compute.internal (executor 2) (1/1)\n",
      "[2019-08-17 03:16:25,596] {bash_operator.py:127} INFO - 2019-08-17 03:16:25 INFO  DAGScheduler:54 - ShuffleMapStage 3 (countByValue at StringIndexer.scala:140) finished in 2.374 s\n",
      "[2019-08-17 03:16:25,596] {bash_operator.py:127} INFO - 2019-08-17 03:16:25 INFO  DAGScheduler:54 - looking for newly runnable stages\n",
      "[2019-08-17 03:16:25,597] {bash_operator.py:127} INFO - 2019-08-17 03:16:25 INFO  DAGScheduler:54 - running: Set()\n",
      "[2019-08-17 03:16:25,598] {bash_operator.py:127} INFO - 2019-08-17 03:16:25 INFO  DAGScheduler:54 - waiting: Set(ResultStage 4)\n",
      "[2019-08-17 03:16:25,598] {bash_operator.py:127} INFO - 2019-08-17 03:16:25 INFO  DAGScheduler:54 - failed: Set()\n",
      "[2019-08-17 03:16:25,599] {bash_operator.py:127} INFO - 2019-08-17 03:16:25 INFO  YarnScheduler:54 - Removed TaskSet 3.0, whose tasks have all completed, from pool\n",
      "[2019-08-17 03:16:25,601] {bash_operator.py:127} INFO - 2019-08-17 03:16:25 INFO  DAGScheduler:54 - Submitting ResultStage 4 (ShuffledRDD[20] at countByValue at StringIndexer.scala:140), which has no missing parents\n",
      "[2019-08-17 03:16:25,605] {bash_operator.py:127} INFO - 2019-08-17 03:16:25 INFO  MemoryStore:54 - Block broadcast_5 stored as values in memory (estimated size 3.3 KB, free 365.8 MB)\n",
      "[2019-08-17 03:16:25,608] {bash_operator.py:127} INFO - 2019-08-17 03:16:25 INFO  MemoryStore:54 - Block broadcast_5_piece0 stored as bytes in memory (estimated size 1959.0 B, free 365.8 MB)\n",
      "[2019-08-17 03:16:25,608] {bash_operator.py:127} INFO - 2019-08-17 03:16:25 INFO  BlockManagerInfo:54 - Added broadcast_5_piece0 in memory on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 (size: 1959.0 B, free: 366.2 MB)\n",
      "[2019-08-17 03:16:25,609] {bash_operator.py:127} INFO - 2019-08-17 03:16:25 INFO  SparkContext:54 - Created broadcast 5 from broadcast at DAGScheduler.scala:1161\n",
      "[2019-08-17 03:16:25,609] {bash_operator.py:127} INFO - 2019-08-17 03:16:25 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 4 (ShuffledRDD[20] at countByValue at StringIndexer.scala:140) (first 15 tasks are for partitions Vector(0))\n",
      "[2019-08-17 03:16:25,610] {bash_operator.py:127} INFO - 2019-08-17 03:16:25 INFO  YarnScheduler:54 - Adding task set 4.0 with 1 tasks\n",
      "[2019-08-17 03:16:25,612] {bash_operator.py:127} INFO - 2019-08-17 03:16:25 INFO  TaskSetManager:54 - Starting task 0.0 in stage 4.0 (TID 4, ip-172-31-29-159.ap-northeast-2.compute.internal, executor 2, partition 0, NODE_LOCAL, 7673 bytes)\n",
      "[2019-08-17 03:16:25,630] {bash_operator.py:127} INFO - 2019-08-17 03:16:25 INFO  BlockManagerInfo:54 - Added broadcast_5_piece0 in memory on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 (size: 1959.0 B, free: 9.4 GB)\n",
      "[2019-08-17 03:16:25,654] {bash_operator.py:127} INFO - 2019-08-17 03:16:25 INFO  MapOutputTrackerMasterEndpoint:54 - Asked to send map output locations for shuffle 0 to 172.31.29.159:58394\n",
      "[2019-08-17 03:16:25,729] {bash_operator.py:127} INFO - 2019-08-17 03:16:25 INFO  TaskSetManager:54 - Finished task 0.0 in stage 4.0 (TID 4) in 119 ms on ip-172-31-29-159.ap-northeast-2.compute.internal (executor 2) (1/1)\n",
      "[2019-08-17 03:16:25,729] {bash_operator.py:127} INFO - 2019-08-17 03:16:25 INFO  YarnScheduler:54 - Removed TaskSet 4.0, whose tasks have all completed, from pool\n",
      "[2019-08-17 03:16:25,731] {bash_operator.py:127} INFO - 2019-08-17 03:16:25 INFO  DAGScheduler:54 - ResultStage 4 (countByValue at StringIndexer.scala:140) finished in 0.127 s\n",
      "[2019-08-17 03:16:25,731] {bash_operator.py:127} INFO - 2019-08-17 03:16:25 INFO  DAGScheduler:54 - Job 3 finished: countByValue at StringIndexer.scala:140, took 2.712333 s\n",
      "[2019-08-17 03:16:25,811] {bash_operator.py:127} INFO - /models/string_indexer_model_Carrier.bin\n",
      "[2019-08-17 03:16:25,821] {bash_operator.py:127} INFO - 2019-08-17 03:16:25 INFO  FileSystemOverwrite:54 - Path /models/string_indexer_model_Carrier.bin already exists. It will be overwritten.\n",
      "[2019-08-17 03:16:25,827] {bash_operator.py:127} INFO - 2019-08-17 03:16:25 INFO  HadoopMapRedCommitProtocol:54 - Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "[2019-08-17 03:16:25,827] {bash_operator.py:127} INFO - 2019-08-17 03:16:25 INFO  FileOutputCommitter:123 - File Output Committer Algorithm version is 1\n",
      "[2019-08-17 03:16:25,827] {bash_operator.py:127} INFO - 2019-08-17 03:16:25 INFO  FileOutputCommitter:138 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "[2019-08-17 03:16:25,838] {bash_operator.py:127} INFO - 2019-08-17 03:16:25 INFO  SparkContext:54 - Starting job: runJob at SparkHadoopWriter.scala:78\n",
      "[2019-08-17 03:16:25,839] {bash_operator.py:127} INFO - 2019-08-17 03:16:25 INFO  DAGScheduler:54 - Got job 4 (runJob at SparkHadoopWriter.scala:78) with 1 output partitions\n",
      "[2019-08-17 03:16:25,839] {bash_operator.py:127} INFO - 2019-08-17 03:16:25 INFO  DAGScheduler:54 - Final stage: ResultStage 5 (runJob at SparkHadoopWriter.scala:78)\n",
      "[2019-08-17 03:16:25,840] {bash_operator.py:127} INFO - 2019-08-17 03:16:25 INFO  DAGScheduler:54 - Parents of final stage: List()\n",
      "[2019-08-17 03:16:25,840] {bash_operator.py:127} INFO - 2019-08-17 03:16:25 INFO  DAGScheduler:54 - Missing parents: List()\n",
      "[2019-08-17 03:16:25,843] {bash_operator.py:127} INFO - 2019-08-17 03:16:25 INFO  DAGScheduler:54 - Submitting ResultStage 5 (MapPartitionsRDD[22] at saveAsTextFile at ReadWrite.scala:441), which has no missing parents\n",
      "[2019-08-17 03:16:25,852] {bash_operator.py:127} INFO - 2019-08-17 03:16:25 INFO  MemoryStore:54 - Block broadcast_6 stored as values in memory (estimated size 86.1 KB, free 365.8 MB)\n",
      "[2019-08-17 03:16:25,854] {bash_operator.py:127} INFO - 2019-08-17 03:16:25 INFO  MemoryStore:54 - Block broadcast_6_piece0 stored as bytes in memory (estimated size 30.3 KB, free 365.7 MB)\n",
      "[2019-08-17 03:16:25,856] {bash_operator.py:127} INFO - 2019-08-17 03:16:25 INFO  BlockManagerInfo:54 - Added broadcast_6_piece0 in memory on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 (size: 30.3 KB, free: 366.2 MB)\n",
      "[2019-08-17 03:16:25,856] {bash_operator.py:127} INFO - 2019-08-17 03:16:25 INFO  SparkContext:54 - Created broadcast 6 from broadcast at DAGScheduler.scala:1161\n",
      "[2019-08-17 03:16:25,861] {bash_operator.py:127} INFO - 2019-08-17 03:16:25 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[22] at saveAsTextFile at ReadWrite.scala:441) (first 15 tasks are for partitions Vector(0))\n",
      "[2019-08-17 03:16:25,861] {bash_operator.py:127} INFO - 2019-08-17 03:16:25 INFO  YarnScheduler:54 - Adding task set 5.0 with 1 tasks\n",
      "[2019-08-17 03:16:25,862] {bash_operator.py:127} INFO - 2019-08-17 03:16:25 INFO  TaskSetManager:54 - Starting task 0.0 in stage 5.0 (TID 5, ip-172-31-29-159.ap-northeast-2.compute.internal, executor 2, partition 0, PROCESS_LOCAL, 8213 bytes)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-08-17 03:16:25,879] {bash_operator.py:127} INFO - 2019-08-17 03:16:25 INFO  BlockManagerInfo:54 - Added broadcast_6_piece0 in memory on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 (size: 30.3 KB, free: 9.4 GB)\n",
      "[2019-08-17 03:16:26,009] {bash_operator.py:127} INFO - 2019-08-17 03:16:26 INFO  TaskSetManager:54 - Finished task 0.0 in stage 5.0 (TID 5) in 147 ms on ip-172-31-29-159.ap-northeast-2.compute.internal (executor 2) (1/1)\n",
      "[2019-08-17 03:16:26,009] {bash_operator.py:127} INFO - 2019-08-17 03:16:26 INFO  YarnScheduler:54 - Removed TaskSet 5.0, whose tasks have all completed, from pool\n",
      "[2019-08-17 03:16:26,011] {bash_operator.py:127} INFO - 2019-08-17 03:16:26 INFO  DAGScheduler:54 - ResultStage 5 (runJob at SparkHadoopWriter.scala:78) finished in 0.171 s\n",
      "[2019-08-17 03:16:26,013] {bash_operator.py:127} INFO - 2019-08-17 03:16:26 INFO  DAGScheduler:54 - Job 4 finished: runJob at SparkHadoopWriter.scala:78, took 0.173303 s\n",
      "[2019-08-17 03:16:26,030] {bash_operator.py:127} INFO - 2019-08-17 03:16:26 INFO  SparkHadoopWriter:54 - Job job_20190817031625_0022 committed.\n",
      "[2019-08-17 03:16:26,152] {bash_operator.py:127} INFO - 2019-08-17 03:16:26 INFO  ParquetFileFormat:54 - Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "[2019-08-17 03:16:26,162] {bash_operator.py:127} INFO - 2019-08-17 03:16:26 INFO  FileOutputCommitter:123 - File Output Committer Algorithm version is 1\n",
      "[2019-08-17 03:16:26,163] {bash_operator.py:127} INFO - 2019-08-17 03:16:26 INFO  FileOutputCommitter:138 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "[2019-08-17 03:16:26,163] {bash_operator.py:127} INFO - 2019-08-17 03:16:26 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "[2019-08-17 03:16:26,164] {bash_operator.py:127} INFO - 2019-08-17 03:16:26 INFO  FileOutputCommitter:123 - File Output Committer Algorithm version is 1\n",
      "[2019-08-17 03:16:26,164] {bash_operator.py:127} INFO - 2019-08-17 03:16:26 INFO  FileOutputCommitter:138 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "[2019-08-17 03:16:26,165] {bash_operator.py:127} INFO - 2019-08-17 03:16:26 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "[2019-08-17 03:16:26,202] {bash_operator.py:127} INFO - 2019-08-17 03:16:26 INFO  CodeGenerator:54 - Code generated in 25.345957 ms\n",
      "[2019-08-17 03:16:26,241] {bash_operator.py:127} INFO - 2019-08-17 03:16:26 INFO  SparkContext:54 - Starting job: parquet at StringIndexer.scala:302\n",
      "[2019-08-17 03:16:26,245] {bash_operator.py:127} INFO - 2019-08-17 03:16:26 INFO  DAGScheduler:54 - Registering RDD 25 (parquet at StringIndexer.scala:302)\n",
      "[2019-08-17 03:16:26,246] {bash_operator.py:127} INFO - 2019-08-17 03:16:26 INFO  DAGScheduler:54 - Got job 5 (parquet at StringIndexer.scala:302) with 1 output partitions\n",
      "[2019-08-17 03:16:26,246] {bash_operator.py:127} INFO - 2019-08-17 03:16:26 INFO  DAGScheduler:54 - Final stage: ResultStage 7 (parquet at StringIndexer.scala:302)\n",
      "[2019-08-17 03:16:26,246] {bash_operator.py:127} INFO - 2019-08-17 03:16:26 INFO  DAGScheduler:54 - Parents of final stage: List(ShuffleMapStage 6)\n",
      "[2019-08-17 03:16:26,246] {bash_operator.py:127} INFO - 2019-08-17 03:16:26 INFO  DAGScheduler:54 - Missing parents: List(ShuffleMapStage 6)\n",
      "[2019-08-17 03:16:26,246] {bash_operator.py:127} INFO - 2019-08-17 03:16:26 INFO  DAGScheduler:54 - Submitting ShuffleMapStage 6 (MapPartitionsRDD[25] at parquet at StringIndexer.scala:302), which has no missing parents\n",
      "[2019-08-17 03:16:26,250] {bash_operator.py:127} INFO - 2019-08-17 03:16:26 INFO  MemoryStore:54 - Block broadcast_7 stored as values in memory (estimated size 4.8 KB, free 365.7 MB)\n",
      "[2019-08-17 03:16:26,255] {bash_operator.py:127} INFO - 2019-08-17 03:16:26 INFO  MemoryStore:54 - Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.9 KB, free 365.7 MB)\n",
      "[2019-08-17 03:16:26,255] {bash_operator.py:127} INFO - 2019-08-17 03:16:26 INFO  BlockManagerInfo:54 - Added broadcast_7_piece0 in memory on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 (size: 2.9 KB, free: 366.2 MB)\n",
      "[2019-08-17 03:16:26,256] {bash_operator.py:127} INFO - 2019-08-17 03:16:26 INFO  SparkContext:54 - Created broadcast 7 from broadcast at DAGScheduler.scala:1161\n",
      "[2019-08-17 03:16:26,256] {bash_operator.py:127} INFO - 2019-08-17 03:16:26 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[25] at parquet at StringIndexer.scala:302) (first 15 tasks are for partitions Vector(0))\n",
      "[2019-08-17 03:16:26,256] {bash_operator.py:127} INFO - 2019-08-17 03:16:26 INFO  YarnScheduler:54 - Adding task set 6.0 with 1 tasks\n",
      "[2019-08-17 03:16:26,261] {bash_operator.py:127} INFO - 2019-08-17 03:16:26 INFO  TaskSetManager:54 - Starting task 0.0 in stage 6.0 (TID 6, ip-172-31-29-159.ap-northeast-2.compute.internal, executor 2, partition 0, PROCESS_LOCAL, 8076 bytes)\n",
      "[2019-08-17 03:16:26,280] {bash_operator.py:127} INFO - 2019-08-17 03:16:26 INFO  BlockManagerInfo:54 - Added broadcast_7_piece0 in memory on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 (size: 2.9 KB, free: 9.4 GB)\n",
      "[2019-08-17 03:16:26,314] {bash_operator.py:127} INFO - 2019-08-17 03:16:26 INFO  TaskSetManager:54 - Finished task 0.0 in stage 6.0 (TID 6) in 56 ms on ip-172-31-29-159.ap-northeast-2.compute.internal (executor 2) (1/1)\n",
      "[2019-08-17 03:16:26,314] {bash_operator.py:127} INFO - 2019-08-17 03:16:26 INFO  YarnScheduler:54 - Removed TaskSet 6.0, whose tasks have all completed, from pool\n",
      "[2019-08-17 03:16:26,314] {bash_operator.py:127} INFO - 2019-08-17 03:16:26 INFO  DAGScheduler:54 - ShuffleMapStage 6 (parquet at StringIndexer.scala:302) finished in 0.067 s\n",
      "[2019-08-17 03:16:26,314] {bash_operator.py:127} INFO - 2019-08-17 03:16:26 INFO  DAGScheduler:54 - looking for newly runnable stages\n",
      "[2019-08-17 03:16:26,315] {bash_operator.py:127} INFO - 2019-08-17 03:16:26 INFO  DAGScheduler:54 - running: Set()\n",
      "[2019-08-17 03:16:26,315] {bash_operator.py:127} INFO - 2019-08-17 03:16:26 INFO  DAGScheduler:54 - waiting: Set(ResultStage 7)\n",
      "[2019-08-17 03:16:26,315] {bash_operator.py:127} INFO - 2019-08-17 03:16:26 INFO  DAGScheduler:54 - failed: Set()\n",
      "[2019-08-17 03:16:26,315] {bash_operator.py:127} INFO - 2019-08-17 03:16:26 INFO  DAGScheduler:54 - Submitting ResultStage 7 (ShuffledRowRDD[26] at parquet at StringIndexer.scala:302), which has no missing parents\n",
      "[2019-08-17 03:16:26,343] {bash_operator.py:127} INFO - 2019-08-17 03:16:26 INFO  MemoryStore:54 - Block broadcast_8 stored as values in memory (estimated size 203.7 KB, free 365.5 MB)\n",
      "[2019-08-17 03:16:26,345] {bash_operator.py:127} INFO - 2019-08-17 03:16:26 INFO  MemoryStore:54 - Block broadcast_8_piece0 stored as bytes in memory (estimated size 69.7 KB, free 365.5 MB)\n",
      "[2019-08-17 03:16:26,347] {bash_operator.py:127} INFO - 2019-08-17 03:16:26 INFO  BlockManagerInfo:54 - Added broadcast_8_piece0 in memory on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 (size: 69.7 KB, free: 366.1 MB)\n",
      "[2019-08-17 03:16:26,348] {bash_operator.py:127} INFO - 2019-08-17 03:16:26 INFO  SparkContext:54 - Created broadcast 8 from broadcast at DAGScheduler.scala:1161\n",
      "[2019-08-17 03:16:26,349] {bash_operator.py:127} INFO - 2019-08-17 03:16:26 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 7 (ShuffledRowRDD[26] at parquet at StringIndexer.scala:302) (first 15 tasks are for partitions Vector(0))\n",
      "[2019-08-17 03:16:26,350] {bash_operator.py:127} INFO - 2019-08-17 03:16:26 INFO  YarnScheduler:54 - Adding task set 7.0 with 1 tasks\n",
      "[2019-08-17 03:16:26,352] {bash_operator.py:127} INFO - 2019-08-17 03:16:26 INFO  TaskSetManager:54 - Starting task 0.0 in stage 7.0 (TID 7, ip-172-31-29-159.ap-northeast-2.compute.internal, executor 2, partition 0, NODE_LOCAL, 7778 bytes)\n",
      "[2019-08-17 03:16:26,370] {bash_operator.py:127} INFO - 2019-08-17 03:16:26 INFO  BlockManagerInfo:54 - Added broadcast_8_piece0 in memory on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 (size: 69.7 KB, free: 9.4 GB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-08-17 03:16:26,422] {bash_operator.py:127} INFO - 2019-08-17 03:16:26 INFO  MapOutputTrackerMasterEndpoint:54 - Asked to send map output locations for shuffle 1 to 172.31.29.159:58394\n",
      "[2019-08-17 03:16:26,592] {bash_operator.py:127} INFO - 2019-08-17 03:16:26 INFO  TaskSetManager:54 - Finished task 0.0 in stage 7.0 (TID 7) in 241 ms on ip-172-31-29-159.ap-northeast-2.compute.internal (executor 2) (1/1)\n",
      "[2019-08-17 03:16:26,592] {bash_operator.py:127} INFO - 2019-08-17 03:16:26 INFO  YarnScheduler:54 - Removed TaskSet 7.0, whose tasks have all completed, from pool\n",
      "[2019-08-17 03:16:26,593] {bash_operator.py:127} INFO - 2019-08-17 03:16:26 INFO  DAGScheduler:54 - ResultStage 7 (parquet at StringIndexer.scala:302) finished in 0.278 s\n",
      "[2019-08-17 03:16:26,594] {bash_operator.py:127} INFO - 2019-08-17 03:16:26 INFO  DAGScheduler:54 - Job 5 finished: parquet at StringIndexer.scala:302, took 0.352093 s\n",
      "[2019-08-17 03:16:26,609] {bash_operator.py:127} INFO - 2019-08-17 03:16:26 INFO  FileFormatWriter:54 - Write Job cbf939fe-9254-4d69-905e-5a24df30bf6c committed.\n",
      "[2019-08-17 03:16:26,615] {bash_operator.py:127} INFO - 2019-08-17 03:16:26 INFO  FileFormatWriter:54 - Finished processing stats for write job cbf939fe-9254-4d69-905e-5a24df30bf6c.\n",
      "[2019-08-17 03:16:26,719] {bash_operator.py:127} INFO - 2019-08-17 03:16:26 INFO  CodeGenerator:54 - Code generated in 15.859371 ms\n",
      "[2019-08-17 03:16:26,734] {bash_operator.py:127} INFO - 2019-08-17 03:16:26 INFO  SparkContext:54 - Starting job: countByValue at StringIndexer.scala:140\n",
      "[2019-08-17 03:16:26,738] {bash_operator.py:127} INFO - 2019-08-17 03:16:26 INFO  DAGScheduler:54 - Registering RDD 34 (countByValue at StringIndexer.scala:140)\n",
      "[2019-08-17 03:16:26,738] {bash_operator.py:127} INFO - 2019-08-17 03:16:26 INFO  DAGScheduler:54 - Got job 6 (countByValue at StringIndexer.scala:140) with 1 output partitions\n",
      "[2019-08-17 03:16:26,738] {bash_operator.py:127} INFO - 2019-08-17 03:16:26 INFO  DAGScheduler:54 - Final stage: ResultStage 9 (countByValue at StringIndexer.scala:140)\n",
      "[2019-08-17 03:16:26,739] {bash_operator.py:127} INFO - 2019-08-17 03:16:26 INFO  DAGScheduler:54 - Parents of final stage: List(ShuffleMapStage 8)\n",
      "[2019-08-17 03:16:26,739] {bash_operator.py:127} INFO - 2019-08-17 03:16:26 INFO  DAGScheduler:54 - Missing parents: List(ShuffleMapStage 8)\n",
      "[2019-08-17 03:16:26,739] {bash_operator.py:127} INFO - 2019-08-17 03:16:26 INFO  DAGScheduler:54 - Submitting ShuffleMapStage 8 (MapPartitionsRDD[34] at countByValue at StringIndexer.scala:140), which has no missing parents\n",
      "[2019-08-17 03:16:26,743] {bash_operator.py:127} INFO - 2019-08-17 03:16:26 INFO  MemoryStore:54 - Block broadcast_9 stored as values in memory (estimated size 39.0 KB, free 365.4 MB)\n",
      "[2019-08-17 03:16:26,745] {bash_operator.py:127} INFO - 2019-08-17 03:16:26 INFO  MemoryStore:54 - Block broadcast_9_piece0 stored as bytes in memory (estimated size 18.7 KB, free 365.4 MB)\n",
      "[2019-08-17 03:16:26,746] {bash_operator.py:127} INFO - 2019-08-17 03:16:26 INFO  BlockManagerInfo:54 - Added broadcast_9_piece0 in memory on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 (size: 18.7 KB, free: 366.1 MB)\n",
      "[2019-08-17 03:16:26,747] {bash_operator.py:127} INFO - 2019-08-17 03:16:26 INFO  SparkContext:54 - Created broadcast 9 from broadcast at DAGScheduler.scala:1161\n",
      "[2019-08-17 03:16:26,748] {bash_operator.py:127} INFO - 2019-08-17 03:16:26 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[34] at countByValue at StringIndexer.scala:140) (first 15 tasks are for partitions Vector(0))\n",
      "[2019-08-17 03:16:26,748] {bash_operator.py:127} INFO - 2019-08-17 03:16:26 INFO  YarnScheduler:54 - Adding task set 8.0 with 1 tasks\n",
      "[2019-08-17 03:16:26,750] {bash_operator.py:127} INFO - 2019-08-17 03:16:26 INFO  TaskSetManager:54 - Starting task 0.0 in stage 8.0 (TID 8, ip-172-31-29-159.ap-northeast-2.compute.internal, executor 2, partition 0, NODE_LOCAL, 8374 bytes)\n",
      "[2019-08-17 03:16:26,766] {bash_operator.py:127} INFO - 2019-08-17 03:16:26 INFO  BlockManagerInfo:54 - Added broadcast_9_piece0 in memory on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 (size: 18.7 KB, free: 9.4 GB)\n",
      "[2019-08-17 03:16:27,097] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  TaskSetManager:54 - Finished task 0.0 in stage 8.0 (TID 8) in 348 ms on ip-172-31-29-159.ap-northeast-2.compute.internal (executor 2) (1/1)\n",
      "[2019-08-17 03:16:27,098] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  YarnScheduler:54 - Removed TaskSet 8.0, whose tasks have all completed, from pool\n",
      "[2019-08-17 03:16:27,099] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  DAGScheduler:54 - ShuffleMapStage 8 (countByValue at StringIndexer.scala:140) finished in 0.361 s\n",
      "[2019-08-17 03:16:27,099] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  DAGScheduler:54 - looking for newly runnable stages\n",
      "[2019-08-17 03:16:27,099] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  DAGScheduler:54 - running: Set()\n",
      "[2019-08-17 03:16:27,099] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  DAGScheduler:54 - waiting: Set(ResultStage 9)\n",
      "[2019-08-17 03:16:27,099] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  DAGScheduler:54 - failed: Set()\n",
      "[2019-08-17 03:16:27,099] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  DAGScheduler:54 - Submitting ResultStage 9 (ShuffledRDD[35] at countByValue at StringIndexer.scala:140), which has no missing parents\n",
      "[2019-08-17 03:16:27,100] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  MemoryStore:54 - Block broadcast_10 stored as values in memory (estimated size 3.3 KB, free 365.4 MB)\n",
      "[2019-08-17 03:16:27,102] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  MemoryStore:54 - Block broadcast_10_piece0 stored as bytes in memory (estimated size 1957.0 B, free 365.4 MB)\n",
      "[2019-08-17 03:16:27,102] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  BlockManagerInfo:54 - Added broadcast_10_piece0 in memory on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 (size: 1957.0 B, free: 366.1 MB)\n",
      "[2019-08-17 03:16:27,103] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  SparkContext:54 - Created broadcast 10 from broadcast at DAGScheduler.scala:1161\n",
      "[2019-08-17 03:16:27,103] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 9 (ShuffledRDD[35] at countByValue at StringIndexer.scala:140) (first 15 tasks are for partitions Vector(0))\n",
      "[2019-08-17 03:16:27,103] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  YarnScheduler:54 - Adding task set 9.0 with 1 tasks\n",
      "[2019-08-17 03:16:27,105] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  TaskSetManager:54 - Starting task 0.0 in stage 9.0 (TID 9, ip-172-31-29-159.ap-northeast-2.compute.internal, executor 2, partition 0, NODE_LOCAL, 7673 bytes)\n",
      "[2019-08-17 03:16:27,121] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  BlockManagerInfo:54 - Added broadcast_10_piece0 in memory on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 (size: 1957.0 B, free: 9.4 GB)\n",
      "[2019-08-17 03:16:27,127] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  MapOutputTrackerMasterEndpoint:54 - Asked to send map output locations for shuffle 2 to 172.31.29.159:58394\n",
      "[2019-08-17 03:16:27,152] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  TaskSetManager:54 - Finished task 0.0 in stage 9.0 (TID 9) in 48 ms on ip-172-31-29-159.ap-northeast-2.compute.internal (executor 2) (1/1)\n",
      "[2019-08-17 03:16:27,153] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  YarnScheduler:54 - Removed TaskSet 9.0, whose tasks have all completed, from pool\n",
      "[2019-08-17 03:16:27,156] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  DAGScheduler:54 - ResultStage 9 (countByValue at StringIndexer.scala:140) finished in 0.054 s\n",
      "[2019-08-17 03:16:27,157] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  DAGScheduler:54 - Job 6 finished: countByValue at StringIndexer.scala:140, took 0.419525 s\n",
      "[2019-08-17 03:16:27,180] {bash_operator.py:127} INFO - /models/string_indexer_model_Origin.bin\n",
      "[2019-08-17 03:16:27,196] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  FileSystemOverwrite:54 - Path /models/string_indexer_model_Origin.bin already exists. It will be overwritten.\n",
      "[2019-08-17 03:16:27,205] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  HadoopMapRedCommitProtocol:54 - Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "[2019-08-17 03:16:27,205] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  FileOutputCommitter:123 - File Output Committer Algorithm version is 1\n",
      "[2019-08-17 03:16:27,205] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  FileOutputCommitter:138 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-08-17 03:16:27,223] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  SparkContext:54 - Starting job: runJob at SparkHadoopWriter.scala:78\n",
      "[2019-08-17 03:16:27,224] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  DAGScheduler:54 - Got job 7 (runJob at SparkHadoopWriter.scala:78) with 1 output partitions\n",
      "[2019-08-17 03:16:27,224] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  DAGScheduler:54 - Final stage: ResultStage 10 (runJob at SparkHadoopWriter.scala:78)\n",
      "[2019-08-17 03:16:27,224] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  DAGScheduler:54 - Parents of final stage: List()\n",
      "[2019-08-17 03:16:27,225] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  DAGScheduler:54 - Missing parents: List()\n",
      "[2019-08-17 03:16:27,225] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  DAGScheduler:54 - Submitting ResultStage 10 (MapPartitionsRDD[37] at saveAsTextFile at ReadWrite.scala:441), which has no missing parents\n",
      "[2019-08-17 03:16:27,235] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  MemoryStore:54 - Block broadcast_11 stored as values in memory (estimated size 86.1 KB, free 365.3 MB)\n",
      "[2019-08-17 03:16:27,237] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  MemoryStore:54 - Block broadcast_11_piece0 stored as bytes in memory (estimated size 30.2 KB, free 365.3 MB)\n",
      "[2019-08-17 03:16:27,240] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  BlockManagerInfo:54 - Added broadcast_11_piece0 in memory on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 (size: 30.2 KB, free: 366.1 MB)\n",
      "[2019-08-17 03:16:27,241] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  SparkContext:54 - Created broadcast 11 from broadcast at DAGScheduler.scala:1161\n",
      "[2019-08-17 03:16:27,244] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[37] at saveAsTextFile at ReadWrite.scala:441) (first 15 tasks are for partitions Vector(0))\n",
      "[2019-08-17 03:16:27,244] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  YarnScheduler:54 - Adding task set 10.0 with 1 tasks\n",
      "[2019-08-17 03:16:27,247] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  TaskSetManager:54 - Starting task 0.0 in stage 10.0 (TID 10, ip-172-31-29-159.ap-northeast-2.compute.internal, executor 2, partition 0, PROCESS_LOCAL, 8211 bytes)\n",
      "[2019-08-17 03:16:27,261] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  BlockManagerInfo:54 - Added broadcast_11_piece0 in memory on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 (size: 30.2 KB, free: 9.4 GB)\n",
      "[2019-08-17 03:16:27,314] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  TaskSetManager:54 - Finished task 0.0 in stage 10.0 (TID 10) in 66 ms on ip-172-31-29-159.ap-northeast-2.compute.internal (executor 2) (1/1)\n",
      "[2019-08-17 03:16:27,314] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  YarnScheduler:54 - Removed TaskSet 10.0, whose tasks have all completed, from pool\n",
      "[2019-08-17 03:16:27,316] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  DAGScheduler:54 - ResultStage 10 (runJob at SparkHadoopWriter.scala:78) finished in 0.091 s\n",
      "[2019-08-17 03:16:27,317] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  DAGScheduler:54 - Job 7 finished: runJob at SparkHadoopWriter.scala:78, took 0.093211 s\n",
      "[2019-08-17 03:16:27,330] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  SparkHadoopWriter:54 - Job job_20190817031627_0037 committed.\n",
      "[2019-08-17 03:16:27,376] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  ParquetFileFormat:54 - Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "[2019-08-17 03:16:27,377] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  FileOutputCommitter:123 - File Output Committer Algorithm version is 1\n",
      "[2019-08-17 03:16:27,377] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  FileOutputCommitter:138 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "[2019-08-17 03:16:27,377] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "[2019-08-17 03:16:27,378] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  FileOutputCommitter:123 - File Output Committer Algorithm version is 1\n",
      "[2019-08-17 03:16:27,378] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  FileOutputCommitter:138 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "[2019-08-17 03:16:27,378] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "[2019-08-17 03:16:27,425] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  SparkContext:54 - Starting job: parquet at StringIndexer.scala:302\n",
      "[2019-08-17 03:16:27,431] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  DAGScheduler:54 - Registering RDD 40 (parquet at StringIndexer.scala:302)\n",
      "[2019-08-17 03:16:27,436] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  DAGScheduler:54 - Got job 8 (parquet at StringIndexer.scala:302) with 1 output partitions\n",
      "[2019-08-17 03:16:27,436] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  DAGScheduler:54 - Final stage: ResultStage 12 (parquet at StringIndexer.scala:302)\n",
      "[2019-08-17 03:16:27,436] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  DAGScheduler:54 - Parents of final stage: List(ShuffleMapStage 11)\n",
      "[2019-08-17 03:16:27,436] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  DAGScheduler:54 - Missing parents: List(ShuffleMapStage 11)\n",
      "[2019-08-17 03:16:27,437] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  DAGScheduler:54 - Submitting ShuffleMapStage 11 (MapPartitionsRDD[40] at parquet at StringIndexer.scala:302), which has no missing parents\n",
      "[2019-08-17 03:16:27,437] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  MemoryStore:54 - Block broadcast_12 stored as values in memory (estimated size 4.8 KB, free 365.3 MB)\n",
      "[2019-08-17 03:16:27,437] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  MemoryStore:54 - Block broadcast_12_piece0 stored as bytes in memory (estimated size 2.9 KB, free 365.3 MB)\n",
      "[2019-08-17 03:16:27,437] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  BlockManagerInfo:54 - Added broadcast_12_piece0 in memory on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 (size: 2.9 KB, free: 366.1 MB)\n",
      "[2019-08-17 03:16:27,438] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  SparkContext:54 - Created broadcast 12 from broadcast at DAGScheduler.scala:1161\n",
      "[2019-08-17 03:16:27,439] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ShuffleMapStage 11 (MapPartitionsRDD[40] at parquet at StringIndexer.scala:302) (first 15 tasks are for partitions Vector(0))\n",
      "[2019-08-17 03:16:27,439] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  YarnScheduler:54 - Adding task set 11.0 with 1 tasks\n",
      "[2019-08-17 03:16:27,441] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  TaskSetManager:54 - Starting task 0.0 in stage 11.0 (TID 11, ip-172-31-29-159.ap-northeast-2.compute.internal, executor 2, partition 0, PROCESS_LOCAL, 8783 bytes)\n",
      "[2019-08-17 03:16:27,454] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  BlockManagerInfo:54 - Added broadcast_12_piece0 in memory on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 (size: 2.9 KB, free: 9.4 GB)\n",
      "[2019-08-17 03:16:27,465] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  TaskSetManager:54 - Finished task 0.0 in stage 11.0 (TID 11) in 23 ms on ip-172-31-29-159.ap-northeast-2.compute.internal (executor 2) (1/1)\n",
      "[2019-08-17 03:16:27,468] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  YarnScheduler:54 - Removed TaskSet 11.0, whose tasks have all completed, from pool\n",
      "[2019-08-17 03:16:27,469] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  DAGScheduler:54 - ShuffleMapStage 11 (parquet at StringIndexer.scala:302) finished in 0.038 s\n",
      "[2019-08-17 03:16:27,469] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  DAGScheduler:54 - looking for newly runnable stages\n",
      "[2019-08-17 03:16:27,469] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  DAGScheduler:54 - running: Set()\n",
      "[2019-08-17 03:16:27,469] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  DAGScheduler:54 - waiting: Set(ResultStage 12)\n",
      "[2019-08-17 03:16:27,469] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  DAGScheduler:54 - failed: Set()\n",
      "[2019-08-17 03:16:27,469] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  DAGScheduler:54 - Submitting ResultStage 12 (ShuffledRowRDD[41] at parquet at StringIndexer.scala:302), which has no missing parents\n",
      "[2019-08-17 03:16:27,500] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  MemoryStore:54 - Block broadcast_13 stored as values in memory (estimated size 203.7 KB, free 365.1 MB)\n",
      "[2019-08-17 03:16:27,506] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  MemoryStore:54 - Block broadcast_13_piece0 stored as bytes in memory (estimated size 69.7 KB, free 365.0 MB)\n",
      "[2019-08-17 03:16:27,507] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  BlockManagerInfo:54 - Added broadcast_13_piece0 in memory on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 (size: 69.7 KB, free: 366.0 MB)\n",
      "[2019-08-17 03:16:27,507] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  SparkContext:54 - Created broadcast 13 from broadcast at DAGScheduler.scala:1161\n",
      "[2019-08-17 03:16:27,509] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 12 (ShuffledRowRDD[41] at parquet at StringIndexer.scala:302) (first 15 tasks are for partitions Vector(0))\n",
      "[2019-08-17 03:16:27,509] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  YarnScheduler:54 - Adding task set 12.0 with 1 tasks\n",
      "[2019-08-17 03:16:27,513] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  TaskSetManager:54 - Starting task 0.0 in stage 12.0 (TID 12, ip-172-31-29-159.ap-northeast-2.compute.internal, executor 2, partition 0, NODE_LOCAL, 7778 bytes)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-08-17 03:16:27,527] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  BlockManagerInfo:54 - Added broadcast_13_piece0 in memory on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 (size: 69.7 KB, free: 9.4 GB)\n",
      "[2019-08-17 03:16:27,552] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  MapOutputTrackerMasterEndpoint:54 - Asked to send map output locations for shuffle 3 to 172.31.29.159:58394\n",
      "[2019-08-17 03:16:27,601] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  TaskSetManager:54 - Finished task 0.0 in stage 12.0 (TID 12) in 91 ms on ip-172-31-29-159.ap-northeast-2.compute.internal (executor 2) (1/1)\n",
      "[2019-08-17 03:16:27,601] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  YarnScheduler:54 - Removed TaskSet 12.0, whose tasks have all completed, from pool\n",
      "[2019-08-17 03:16:27,603] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  DAGScheduler:54 - ResultStage 12 (parquet at StringIndexer.scala:302) finished in 0.133 s\n",
      "[2019-08-17 03:16:27,603] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  DAGScheduler:54 - Job 8 finished: parquet at StringIndexer.scala:302, took 0.177814 s\n",
      "[2019-08-17 03:16:27,613] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  FileFormatWriter:54 - Write Job 7069b015-dbd9-4bd7-b9ed-d6448d67868f committed.\n",
      "[2019-08-17 03:16:27,614] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  FileFormatWriter:54 - Finished processing stats for write job 7069b015-dbd9-4bd7-b9ed-d6448d67868f.\n",
      "[2019-08-17 03:16:27,684] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  CodeGenerator:54 - Code generated in 10.641132 ms\n",
      "[2019-08-17 03:16:27,697] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  SparkContext:54 - Starting job: countByValue at StringIndexer.scala:140\n",
      "[2019-08-17 03:16:27,705] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  DAGScheduler:54 - Registering RDD 49 (countByValue at StringIndexer.scala:140)\n",
      "[2019-08-17 03:16:27,705] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  DAGScheduler:54 - Got job 9 (countByValue at StringIndexer.scala:140) with 1 output partitions\n",
      "[2019-08-17 03:16:27,705] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  DAGScheduler:54 - Final stage: ResultStage 14 (countByValue at StringIndexer.scala:140)\n",
      "[2019-08-17 03:16:27,705] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  DAGScheduler:54 - Parents of final stage: List(ShuffleMapStage 13)\n",
      "[2019-08-17 03:16:27,707] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  DAGScheduler:54 - Missing parents: List(ShuffleMapStage 13)\n",
      "[2019-08-17 03:16:27,707] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  DAGScheduler:54 - Submitting ShuffleMapStage 13 (MapPartitionsRDD[49] at countByValue at StringIndexer.scala:140), which has no missing parents\n",
      "[2019-08-17 03:16:27,710] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  MemoryStore:54 - Block broadcast_14 stored as values in memory (estimated size 39.0 KB, free 365.0 MB)\n",
      "[2019-08-17 03:16:27,712] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  MemoryStore:54 - Block broadcast_14_piece0 stored as bytes in memory (estimated size 18.7 KB, free 364.9 MB)\n",
      "[2019-08-17 03:16:27,719] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  BlockManagerInfo:54 - Added broadcast_14_piece0 in memory on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 (size: 18.7 KB, free: 366.0 MB)\n",
      "[2019-08-17 03:16:27,719] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  SparkContext:54 - Created broadcast 14 from broadcast at DAGScheduler.scala:1161\n",
      "[2019-08-17 03:16:27,720] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ShuffleMapStage 13 (MapPartitionsRDD[49] at countByValue at StringIndexer.scala:140) (first 15 tasks are for partitions Vector(0))\n",
      "[2019-08-17 03:16:27,720] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  YarnScheduler:54 - Adding task set 13.0 with 1 tasks\n",
      "[2019-08-17 03:16:27,721] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  TaskSetManager:54 - Starting task 0.0 in stage 13.0 (TID 13, ip-172-31-29-159.ap-northeast-2.compute.internal, executor 2, partition 0, NODE_LOCAL, 8374 bytes)\n",
      "[2019-08-17 03:16:27,733] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  BlockManagerInfo:54 - Added broadcast_14_piece0 in memory on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 (size: 18.7 KB, free: 9.4 GB)\n",
      "[2019-08-17 03:16:27,991] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  TaskSetManager:54 - Finished task 0.0 in stage 13.0 (TID 13) in 269 ms on ip-172-31-29-159.ap-northeast-2.compute.internal (executor 2) (1/1)\n",
      "[2019-08-17 03:16:27,991] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  YarnScheduler:54 - Removed TaskSet 13.0, whose tasks have all completed, from pool\n",
      "[2019-08-17 03:16:27,992] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  DAGScheduler:54 - ShuffleMapStage 13 (countByValue at StringIndexer.scala:140) finished in 0.285 s\n",
      "[2019-08-17 03:16:27,992] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  DAGScheduler:54 - looking for newly runnable stages\n",
      "[2019-08-17 03:16:27,992] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  DAGScheduler:54 - running: Set()\n",
      "[2019-08-17 03:16:27,992] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  DAGScheduler:54 - waiting: Set(ResultStage 14)\n",
      "[2019-08-17 03:16:27,992] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  DAGScheduler:54 - failed: Set()\n",
      "[2019-08-17 03:16:27,992] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  DAGScheduler:54 - Submitting ResultStage 14 (ShuffledRDD[50] at countByValue at StringIndexer.scala:140), which has no missing parents\n",
      "[2019-08-17 03:16:27,994] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  MemoryStore:54 - Block broadcast_15 stored as values in memory (estimated size 3.3 KB, free 364.9 MB)\n",
      "[2019-08-17 03:16:27,996] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  MemoryStore:54 - Block broadcast_15_piece0 stored as bytes in memory (estimated size 1955.0 B, free 364.9 MB)\n",
      "[2019-08-17 03:16:27,996] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  BlockManagerInfo:54 - Added broadcast_15_piece0 in memory on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 (size: 1955.0 B, free: 366.0 MB)\n",
      "[2019-08-17 03:16:27,997] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  SparkContext:54 - Created broadcast 15 from broadcast at DAGScheduler.scala:1161\n",
      "[2019-08-17 03:16:27,997] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 14 (ShuffledRDD[50] at countByValue at StringIndexer.scala:140) (first 15 tasks are for partitions Vector(0))\n",
      "[2019-08-17 03:16:27,997] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  YarnScheduler:54 - Adding task set 14.0 with 1 tasks\n",
      "[2019-08-17 03:16:27,999] {bash_operator.py:127} INFO - 2019-08-17 03:16:27 INFO  TaskSetManager:54 - Starting task 0.0 in stage 14.0 (TID 14, ip-172-31-29-159.ap-northeast-2.compute.internal, executor 2, partition 0, NODE_LOCAL, 7673 bytes)\n",
      "[2019-08-17 03:16:28,010] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  BlockManagerInfo:54 - Added broadcast_15_piece0 in memory on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 (size: 1955.0 B, free: 9.4 GB)\n",
      "[2019-08-17 03:16:28,014] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  MapOutputTrackerMasterEndpoint:54 - Asked to send map output locations for shuffle 4 to 172.31.29.159:58394\n",
      "[2019-08-17 03:16:28,033] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  TaskSetManager:54 - Finished task 0.0 in stage 14.0 (TID 14) in 35 ms on ip-172-31-29-159.ap-northeast-2.compute.internal (executor 2) (1/1)\n",
      "[2019-08-17 03:16:28,033] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  YarnScheduler:54 - Removed TaskSet 14.0, whose tasks have all completed, from pool\n",
      "[2019-08-17 03:16:28,034] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  DAGScheduler:54 - ResultStage 14 (countByValue at StringIndexer.scala:140) finished in 0.041 s\n",
      "[2019-08-17 03:16:28,035] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  DAGScheduler:54 - Job 9 finished: countByValue at StringIndexer.scala:140, took 0.337054 s\n",
      "[2019-08-17 03:16:28,063] {bash_operator.py:127} INFO - /models/string_indexer_model_Dest.bin\n",
      "[2019-08-17 03:16:28,071] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  FileSystemOverwrite:54 - Path /models/string_indexer_model_Dest.bin already exists. It will be overwritten.\n",
      "[2019-08-17 03:16:28,078] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  HadoopMapRedCommitProtocol:54 - Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "[2019-08-17 03:16:28,078] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  FileOutputCommitter:123 - File Output Committer Algorithm version is 1\n",
      "[2019-08-17 03:16:28,078] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  FileOutputCommitter:138 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-08-17 03:16:28,090] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  SparkContext:54 - Starting job: runJob at SparkHadoopWriter.scala:78\n",
      "[2019-08-17 03:16:28,091] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  DAGScheduler:54 - Got job 10 (runJob at SparkHadoopWriter.scala:78) with 1 output partitions\n",
      "[2019-08-17 03:16:28,091] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  DAGScheduler:54 - Final stage: ResultStage 15 (runJob at SparkHadoopWriter.scala:78)\n",
      "[2019-08-17 03:16:28,091] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  DAGScheduler:54 - Parents of final stage: List()\n",
      "[2019-08-17 03:16:28,094] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  DAGScheduler:54 - Missing parents: List()\n",
      "[2019-08-17 03:16:28,094] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  DAGScheduler:54 - Submitting ResultStage 15 (MapPartitionsRDD[52] at saveAsTextFile at ReadWrite.scala:441), which has no missing parents\n",
      "[2019-08-17 03:16:28,103] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  MemoryStore:54 - Block broadcast_16 stored as values in memory (estimated size 86.1 KB, free 364.9 MB)\n",
      "[2019-08-17 03:16:28,117] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  MemoryStore:54 - Block broadcast_16_piece0 stored as bytes in memory (estimated size 30.3 KB, free 364.8 MB)\n",
      "[2019-08-17 03:16:28,118] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  BlockManagerInfo:54 - Added broadcast_16_piece0 in memory on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 (size: 30.3 KB, free: 366.0 MB)\n",
      "[2019-08-17 03:16:28,118] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 101\n",
      "[2019-08-17 03:16:28,118] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 295\n",
      "[2019-08-17 03:16:28,118] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 236\n",
      "[2019-08-17 03:16:28,118] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 256\n",
      "[2019-08-17 03:16:28,118] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 332\n",
      "[2019-08-17 03:16:28,118] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 380\n",
      "[2019-08-17 03:16:28,118] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 280\n",
      "[2019-08-17 03:16:28,118] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 120\n",
      "[2019-08-17 03:16:28,118] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 302\n",
      "[2019-08-17 03:16:28,119] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 150\n",
      "[2019-08-17 03:16:28,119] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 115\n",
      "[2019-08-17 03:16:28,119] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 350\n",
      "[2019-08-17 03:16:28,119] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 398\n",
      "[2019-08-17 03:16:28,119] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 271\n",
      "[2019-08-17 03:16:28,119] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 117\n",
      "[2019-08-17 03:16:28,119] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 146\n",
      "[2019-08-17 03:16:28,121] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  SparkContext:54 - Created broadcast 16 from broadcast at DAGScheduler.scala:1161\n",
      "[2019-08-17 03:16:28,125] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[52] at saveAsTextFile at ReadWrite.scala:441) (first 15 tasks are for partitions Vector(0))\n",
      "[2019-08-17 03:16:28,125] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  YarnScheduler:54 - Adding task set 15.0 with 1 tasks\n",
      "[2019-08-17 03:16:28,126] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  BlockManagerInfo:54 - Removed broadcast_12_piece0 on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 in memory (size: 2.9 KB, free: 9.4 GB)\n",
      "[2019-08-17 03:16:28,127] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  TaskSetManager:54 - Starting task 0.0 in stage 15.0 (TID 15, ip-172-31-18-11.ap-northeast-2.compute.internal, executor 1, partition 0, PROCESS_LOCAL, 8207 bytes)\n",
      "[2019-08-17 03:16:28,131] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  BlockManagerInfo:54 - Removed broadcast_12_piece0 on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 in memory (size: 2.9 KB, free: 366.0 MB)\n",
      "[2019-08-17 03:16:28,133] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 373\n",
      "[2019-08-17 03:16:28,133] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 190\n",
      "[2019-08-17 03:16:28,133] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 394\n",
      "[2019-08-17 03:16:28,133] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 104\n",
      "[2019-08-17 03:16:28,133] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 113\n",
      "[2019-08-17 03:16:28,133] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 392\n",
      "[2019-08-17 03:16:28,133] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 219\n",
      "[2019-08-17 03:16:28,133] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 224\n",
      "[2019-08-17 03:16:28,133] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 154\n",
      "[2019-08-17 03:16:28,134] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 366\n",
      "[2019-08-17 03:16:28,134] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 171\n",
      "[2019-08-17 03:16:28,134] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 133\n",
      "[2019-08-17 03:16:28,142] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 121\n",
      "[2019-08-17 03:16:28,142] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 388\n",
      "[2019-08-17 03:16:28,143] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 97\n",
      "[2019-08-17 03:16:28,143] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 247\n",
      "[2019-08-17 03:16:28,143] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 111\n",
      "[2019-08-17 03:16:28,143] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 389\n",
      "[2019-08-17 03:16:28,143] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 155\n",
      "[2019-08-17 03:16:28,143] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 100\n",
      "[2019-08-17 03:16:28,143] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 203\n",
      "[2019-08-17 03:16:28,143] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 316\n",
      "[2019-08-17 03:16:28,143] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  BlockManagerInfo:54 - Removed broadcast_13_piece0 on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 in memory (size: 69.7 KB, free: 366.0 MB)\n",
      "[2019-08-17 03:16:28,143] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  BlockManagerInfo:54 - Removed broadcast_13_piece0 on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 in memory (size: 69.7 KB, free: 9.4 GB)\n",
      "[2019-08-17 03:16:28,151] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  BlockManagerInfo:54 - Added broadcast_16_piece0 in memory on ip-172-31-18-11.ap-northeast-2.compute.internal:38933 (size: 30.3 KB, free: 9.4 GB)\n",
      "[2019-08-17 03:16:28,153] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 210\n",
      "[2019-08-17 03:16:28,153] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 369\n",
      "[2019-08-17 03:16:28,153] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 291\n",
      "[2019-08-17 03:16:28,153] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 301\n",
      "[2019-08-17 03:16:28,153] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 84\n",
      "[2019-08-17 03:16:28,158] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned shuffle 1\n",
      "[2019-08-17 03:16:28,158] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 168\n",
      "[2019-08-17 03:16:28,158] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 396\n",
      "[2019-08-17 03:16:28,159] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 358\n",
      "[2019-08-17 03:16:28,159] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 337\n",
      "[2019-08-17 03:16:28,159] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 201\n",
      "[2019-08-17 03:16:28,160] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 211\n",
      "[2019-08-17 03:16:28,160] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 312\n",
      "[2019-08-17 03:16:28,160] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 347\n",
      "[2019-08-17 03:16:28,160] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 212\n",
      "[2019-08-17 03:16:28,161] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 164\n",
      "[2019-08-17 03:16:28,161] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 384\n",
      "[2019-08-17 03:16:28,161] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 177\n",
      "[2019-08-17 03:16:28,162] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned shuffle 3\n",
      "[2019-08-17 03:16:28,162] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 327\n",
      "[2019-08-17 03:16:28,162] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 141\n",
      "[2019-08-17 03:16:28,163] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 127\n",
      "[2019-08-17 03:16:28,163] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 108\n",
      "[2019-08-17 03:16:28,163] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 135\n",
      "[2019-08-17 03:16:28,163] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 270\n",
      "[2019-08-17 03:16:28,163] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 173\n",
      "[2019-08-17 03:16:28,163] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 217\n",
      "[2019-08-17 03:16:28,163] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 264\n",
      "[2019-08-17 03:16:28,163] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 330\n",
      "[2019-08-17 03:16:28,163] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 125\n",
      "[2019-08-17 03:16:28,163] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 147\n",
      "[2019-08-17 03:16:28,163] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 307\n",
      "[2019-08-17 03:16:28,163] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 353\n",
      "[2019-08-17 03:16:28,163] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 368\n",
      "[2019-08-17 03:16:28,163] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 138\n",
      "[2019-08-17 03:16:28,163] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 139\n",
      "[2019-08-17 03:16:28,163] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 144\n",
      "[2019-08-17 03:16:28,163] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 309\n",
      "[2019-08-17 03:16:28,163] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 360\n",
      "[2019-08-17 03:16:28,163] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 179\n",
      "[2019-08-17 03:16:28,163] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 383\n",
      "[2019-08-17 03:16:28,163] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 297\n",
      "[2019-08-17 03:16:28,163] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 254\n",
      "[2019-08-17 03:16:28,164] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 371\n",
      "[2019-08-17 03:16:28,164] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 92\n",
      "[2019-08-17 03:16:28,164] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 243\n",
      "[2019-08-17 03:16:28,164] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 169\n",
      "[2019-08-17 03:16:28,164] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 400\n",
      "[2019-08-17 03:16:28,164] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 185\n",
      "[2019-08-17 03:16:28,164] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 321\n",
      "[2019-08-17 03:16:28,165] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 122\n",
      "[2019-08-17 03:16:28,165] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 310\n",
      "[2019-08-17 03:16:28,165] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 205\n",
      "[2019-08-17 03:16:28,166] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 143\n",
      "[2019-08-17 03:16:28,166] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 308\n",
      "[2019-08-17 03:16:28,166] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 109\n",
      "[2019-08-17 03:16:28,166] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 276\n",
      "[2019-08-17 03:16:28,166] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 348\n",
      "[2019-08-17 03:16:28,166] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 136\n",
      "[2019-08-17 03:16:28,166] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 156\n",
      "[2019-08-17 03:16:28,167] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 153\n",
      "[2019-08-17 03:16:28,167] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 364\n",
      "[2019-08-17 03:16:28,167] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 377\n",
      "[2019-08-17 03:16:28,167] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 126\n",
      "[2019-08-17 03:16:28,167] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 235\n",
      "[2019-08-17 03:16:28,167] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 249\n",
      "[2019-08-17 03:16:28,168] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  BlockManagerInfo:54 - Removed broadcast_6_piece0 on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 in memory (size: 30.3 KB, free: 366.1 MB)\n",
      "[2019-08-17 03:16:28,168] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  BlockManagerInfo:54 - Removed broadcast_6_piece0 on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 in memory (size: 30.3 KB, free: 9.4 GB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-08-17 03:16:28,208] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 220\n",
      "[2019-08-17 03:16:28,208] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 166\n",
      "[2019-08-17 03:16:28,210] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 170\n",
      "[2019-08-17 03:16:28,210] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 231\n",
      "[2019-08-17 03:16:28,210] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 335\n",
      "[2019-08-17 03:16:28,210] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 176\n",
      "[2019-08-17 03:16:28,210] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 299\n",
      "[2019-08-17 03:16:28,210] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 88\n",
      "[2019-08-17 03:16:28,210] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 163\n",
      "[2019-08-17 03:16:28,211] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 387\n",
      "[2019-08-17 03:16:28,211] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 341\n",
      "[2019-08-17 03:16:28,211] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 209\n",
      "[2019-08-17 03:16:28,211] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 119\n",
      "[2019-08-17 03:16:28,211] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 158\n",
      "[2019-08-17 03:16:28,211] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 95\n",
      "[2019-08-17 03:16:28,211] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 385\n",
      "[2019-08-17 03:16:28,211] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 192\n",
      "[2019-08-17 03:16:28,211] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 206\n",
      "[2019-08-17 03:16:28,211] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 137\n",
      "[2019-08-17 03:16:28,211] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 343\n",
      "[2019-08-17 03:16:28,211] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 372\n",
      "[2019-08-17 03:16:28,211] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 213\n",
      "[2019-08-17 03:16:28,211] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 151\n",
      "[2019-08-17 03:16:28,211] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 272\n",
      "[2019-08-17 03:16:28,211] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 328\n",
      "[2019-08-17 03:16:28,211] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 306\n",
      "[2019-08-17 03:16:28,212] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 93\n",
      "[2019-08-17 03:16:28,212] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 284\n",
      "[2019-08-17 03:16:28,212] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 110\n",
      "[2019-08-17 03:16:28,212] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 346\n",
      "[2019-08-17 03:16:28,212] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 344\n",
      "[2019-08-17 03:16:28,212] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 188\n",
      "[2019-08-17 03:16:28,213] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  BlockManagerInfo:54 - Removed broadcast_8_piece0 on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 in memory (size: 69.7 KB, free: 366.1 MB)\n",
      "[2019-08-17 03:16:28,216] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  BlockManagerInfo:54 - Removed broadcast_8_piece0 on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 in memory (size: 69.7 KB, free: 9.4 GB)\n",
      "[2019-08-17 03:16:28,223] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 245\n",
      "[2019-08-17 03:16:28,223] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 148\n",
      "[2019-08-17 03:16:28,223] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 278\n",
      "[2019-08-17 03:16:28,225] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  BlockManagerInfo:54 - Removed broadcast_5_piece0 on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 in memory (size: 1959.0 B, free: 366.1 MB)\n",
      "[2019-08-17 03:16:28,227] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  BlockManagerInfo:54 - Removed broadcast_5_piece0 on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 in memory (size: 1959.0 B, free: 9.4 GB)\n",
      "[2019-08-17 03:16:28,238] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 105\n",
      "[2019-08-17 03:16:28,238] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 161\n",
      "[2019-08-17 03:16:28,238] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 331\n",
      "[2019-08-17 03:16:28,242] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned shuffle 4\n",
      "[2019-08-17 03:16:28,242] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 313\n",
      "[2019-08-17 03:16:28,242] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 334\n",
      "[2019-08-17 03:16:28,242] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 124\n",
      "[2019-08-17 03:16:28,242] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 269\n",
      "[2019-08-17 03:16:28,242] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 242\n",
      "[2019-08-17 03:16:28,242] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 87\n",
      "[2019-08-17 03:16:28,243] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 252\n",
      "[2019-08-17 03:16:28,243] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 200\n",
      "[2019-08-17 03:16:28,243] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 289\n",
      "[2019-08-17 03:16:28,243] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 338\n",
      "[2019-08-17 03:16:28,245] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 305\n",
      "[2019-08-17 03:16:28,245] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 162\n",
      "[2019-08-17 03:16:28,245] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 268\n",
      "[2019-08-17 03:16:28,245] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 197\n",
      "[2019-08-17 03:16:28,245] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 395\n",
      "[2019-08-17 03:16:28,245] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  BlockManagerInfo:54 - Removed broadcast_11_piece0 on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 in memory (size: 30.2 KB, free: 366.2 MB)\n",
      "[2019-08-17 03:16:28,245] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  BlockManagerInfo:54 - Removed broadcast_11_piece0 on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 in memory (size: 30.2 KB, free: 9.4 GB)\n",
      "[2019-08-17 03:16:28,261] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 304\n",
      "[2019-08-17 03:16:28,261] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 221\n",
      "[2019-08-17 03:16:28,261] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 397\n",
      "[2019-08-17 03:16:28,261] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 263\n",
      "[2019-08-17 03:16:28,261] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 365\n",
      "[2019-08-17 03:16:28,261] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 102\n",
      "[2019-08-17 03:16:28,261] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 390\n",
      "[2019-08-17 03:16:28,261] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 106\n",
      "[2019-08-17 03:16:28,264] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  BlockManagerInfo:54 - Removed broadcast_4_piece0 on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 in memory (size: 18.7 KB, free: 366.2 MB)\n",
      "[2019-08-17 03:16:28,265] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  BlockManagerInfo:54 - Removed broadcast_4_piece0 on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 in memory (size: 18.7 KB, free: 9.4 GB)\n",
      "[2019-08-17 03:16:28,278] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  TaskSetManager:54 - Finished task 0.0 in stage 15.0 (TID 15) in 150 ms on ip-172-31-18-11.ap-northeast-2.compute.internal (executor 1) (1/1)\n",
      "[2019-08-17 03:16:28,278] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  YarnScheduler:54 - Removed TaskSet 15.0, whose tasks have all completed, from pool\n",
      "[2019-08-17 03:16:28,279] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  DAGScheduler:54 - ResultStage 15 (runJob at SparkHadoopWriter.scala:78) finished in 0.185 s\n",
      "[2019-08-17 03:16:28,279] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  DAGScheduler:54 - Job 10 finished: runJob at SparkHadoopWriter.scala:78, took 0.186879 s\n",
      "[2019-08-17 03:16:28,279] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 128\n",
      "[2019-08-17 03:16:28,279] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 354\n",
      "[2019-08-17 03:16:28,282] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  BlockManagerInfo:54 - Removed broadcast_14_piece0 on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 in memory (size: 18.7 KB, free: 366.2 MB)\n",
      "[2019-08-17 03:16:28,282] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  BlockManagerInfo:54 - Removed broadcast_14_piece0 on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 in memory (size: 18.7 KB, free: 9.4 GB)\n",
      "[2019-08-17 03:16:28,284] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 94\n",
      "[2019-08-17 03:16:28,284] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 375\n",
      "[2019-08-17 03:16:28,284] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 250\n",
      "[2019-08-17 03:16:28,284] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 303\n",
      "[2019-08-17 03:16:28,284] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-08-17 03:16:28,287] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 285\r\n",
      "[2019-08-17 03:16:28,288] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 277\r\n",
      "[2019-08-17 03:16:28,288] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 253\r\n",
      "[2019-08-17 03:16:28,288] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 290\r\n",
      "[2019-08-17 03:16:28,288] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 132\r\n",
      "[2019-08-17 03:16:28,288] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 361\r\n",
      "[2019-08-17 03:16:28,288] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 403\r\n",
      "[2019-08-17 03:16:28,291] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  BlockManagerInfo:54 - Removed broadcast_9_piece0 on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 in memory (size: 18.7 KB, free: 9.4 GB)\r\n",
      "[2019-08-17 03:16:28,294] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  BlockManagerInfo:54 - Removed broadcast_9_piece0 on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 in memory (size: 18.7 KB, free: 366.2 MB)\r\n",
      "[2019-08-17 03:16:28,297] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 266\r\n",
      "[2019-08-17 03:16:28,297] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 172\r\n",
      "[2019-08-17 03:16:28,297] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 195\r\n",
      "[2019-08-17 03:16:28,297] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 363\r\n",
      "[2019-08-17 03:16:28,297] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 202\r\n",
      "[2019-08-17 03:16:28,300] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  SparkHadoopWriter:54 - Job job_20190817031628_0052 committed.\r\n",
      "[2019-08-17 03:16:28,300] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  BlockManagerInfo:54 - Removed broadcast_15_piece0 on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 in memory (size: 1955.0 B, free: 366.2 MB)\r\n",
      "[2019-08-17 03:16:28,301] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  BlockManagerInfo:54 - Removed broadcast_15_piece0 on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 in memory (size: 1955.0 B, free: 9.4 GB)\r\n",
      "[2019-08-17 03:16:28,305] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 265\r\n",
      "[2019-08-17 03:16:28,305] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 326\r\n",
      "[2019-08-17 03:16:28,305] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 183\r\n",
      "[2019-08-17 03:16:28,305] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 237\r\n",
      "[2019-08-17 03:16:28,305] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 228\r\n",
      "[2019-08-17 03:16:28,305] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 359\r\n",
      "[2019-08-17 03:16:28,305] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 314\r\n",
      "[2019-08-17 03:16:28,305] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 259\r\n",
      "[2019-08-17 03:16:28,305] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 367\r\n",
      "[2019-08-17 03:16:28,305] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 222\r\n",
      "[2019-08-17 03:16:28,305] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 317\r\n",
      "[2019-08-17 03:16:28,305] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 86\r\n",
      "[2019-08-17 03:16:28,306] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 225\r\n",
      "[2019-08-17 03:16:28,306] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 145\r\n",
      "[2019-08-17 03:16:28,306] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 157\r\n",
      "[2019-08-17 03:16:28,306] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 246\r\n",
      "[2019-08-17 03:16:28,306] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 98\r\n",
      "[2019-08-17 03:16:28,306] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 239\r\n",
      "[2019-08-17 03:16:28,311] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  BlockManagerInfo:54 - Removed broadcast_10_piece0 on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 in memory (size: 1957.0 B, free: 9.4 GB)\r\n",
      "[2019-08-17 03:16:28,312] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  BlockManagerInfo:54 - Removed broadcast_10_piece0 on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 in memory (size: 1957.0 B, free: 366.2 MB)\r\n",
      "[2019-08-17 03:16:28,330] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 186\r\n",
      "[2019-08-17 03:16:28,330] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 241\r\n",
      "[2019-08-17 03:16:28,330] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 255\r\n",
      "[2019-08-17 03:16:28,330] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 322\r\n",
      "[2019-08-17 03:16:28,331] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 315\r\n",
      "[2019-08-17 03:16:28,331] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 118\r\n",
      "[2019-08-17 03:16:28,331] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 288\r\n",
      "[2019-08-17 03:16:28,331] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 198\r\n",
      "[2019-08-17 03:16:28,331] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 296\r\n",
      "[2019-08-17 03:16:28,331] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 244\r\n",
      "[2019-08-17 03:16:28,331] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 356\r\n",
      "[2019-08-17 03:16:28,331] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 99\r\n",
      "[2019-08-17 03:16:28,331] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 261\r\n",
      "[2019-08-17 03:16:28,331] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 180\r\n",
      "[2019-08-17 03:16:28,331] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 382\r\n",
      "[2019-08-17 03:16:28,331] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 90\r\n",
      "[2019-08-17 03:16:28,331] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 355\r\n",
      "[2019-08-17 03:16:28,331] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 129\r\n",
      "[2019-08-17 03:16:28,331] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 130\r\n",
      "[2019-08-17 03:16:28,331] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 251\r\n",
      "[2019-08-17 03:16:28,332] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 134\r\n",
      "[2019-08-17 03:16:28,332] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 320\r\n",
      "[2019-08-17 03:16:28,332] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 401\r\n",
      "[2019-08-17 03:16:28,332] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 107\r\n",
      "[2019-08-17 03:16:28,332] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 215\r\n",
      "[2019-08-17 03:16:28,332] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 357\r\n",
      "[2019-08-17 03:16:28,332] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 279\r\n",
      "[2019-08-17 03:16:28,332] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 152\r\n",
      "[2019-08-17 03:16:28,332] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 352\r\n",
      "[2019-08-17 03:16:28,332] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 283\r\n",
      "[2019-08-17 03:16:28,332] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 287\r\n",
      "[2019-08-17 03:16:28,332] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 112\r\n",
      "[2019-08-17 03:16:28,332] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 325\r\n",
      "[2019-08-17 03:16:28,332] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 362\r\n",
      "[2019-08-17 03:16:28,333] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 91\r\n",
      "[2019-08-17 03:16:28,333] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 240\r\n",
      "[2019-08-17 03:16:28,333] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 374\r\n",
      "[2019-08-17 03:16:28,333] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 175\r\n",
      "[2019-08-17 03:16:28,333] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 103\r\n",
      "[2019-08-17 03:16:28,333] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 189\r\n",
      "[2019-08-17 03:16:28,333] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 370\r\n",
      "[2019-08-17 03:16:28,333] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 391\r\n",
      "[2019-08-17 03:16:28,333] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 339\r\n",
      "[2019-08-17 03:16:28,333] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 123\r\n",
      "[2019-08-17 03:16:28,333] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 234\r\n",
      "[2019-08-17 03:16:28,333] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 399\r\n",
      "[2019-08-17 03:16:28,333] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 85\r\n",
      "[2019-08-17 03:16:28,334] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 393\r\n",
      "[2019-08-17 03:16:28,334] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 216\r\n",
      "[2019-08-17 03:16:28,334] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 174\r\n",
      "[2019-08-17 03:16:28,334] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 275\r\n",
      "[2019-08-17 03:16:28,334] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 376\r\n",
      "[2019-08-17 03:16:28,334] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 238\r\n",
      "[2019-08-17 03:16:28,334] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 226\r\n",
      "[2019-08-17 03:16:28,334] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 300\r\n",
      "[2019-08-17 03:16:28,334] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 319\r\n",
      "[2019-08-17 03:16:28,334] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 379\r\n",
      "[2019-08-17 03:16:28,334] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 187\r\n",
      "[2019-08-17 03:16:28,334] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 227\r\n",
      "[2019-08-17 03:16:28,334] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 207\r\n",
      "[2019-08-17 03:16:28,334] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 160\r\n",
      "[2019-08-17 03:16:28,334] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 96\r\n",
      "[2019-08-17 03:16:28,335] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 194\r\n",
      "[2019-08-17 03:16:28,336] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 282\r\n",
      "[2019-08-17 03:16:28,336] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 336\r\n",
      "[2019-08-17 03:16:28,337] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 184\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-08-17 03:16:28,337] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 257\n",
      "[2019-08-17 03:16:28,337] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 232\n",
      "[2019-08-17 03:16:28,341] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 167\n",
      "[2019-08-17 03:16:28,341] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 214\n",
      "[2019-08-17 03:16:28,342] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 165\n",
      "[2019-08-17 03:16:28,343] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 230\n",
      "[2019-08-17 03:16:28,343] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 342\n",
      "[2019-08-17 03:16:28,343] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 260\n",
      "[2019-08-17 03:16:28,344] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 323\n",
      "[2019-08-17 03:16:28,344] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 298\n",
      "[2019-08-17 03:16:28,344] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 333\n",
      "[2019-08-17 03:16:28,344] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned shuffle 2\n",
      "[2019-08-17 03:16:28,344] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 182\n",
      "[2019-08-17 03:16:28,344] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 140\n",
      "[2019-08-17 03:16:28,344] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 381\n",
      "[2019-08-17 03:16:28,367] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  BlockManagerInfo:54 - Removed broadcast_7_piece0 on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 in memory (size: 2.9 KB, free: 9.4 GB)\n",
      "[2019-08-17 03:16:28,371] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  BlockManagerInfo:54 - Removed broadcast_7_piece0 on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 in memory (size: 2.9 KB, free: 366.2 MB)\n",
      "[2019-08-17 03:16:28,373] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 267\n",
      "[2019-08-17 03:16:28,373] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 114\n",
      "[2019-08-17 03:16:28,375] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned shuffle 0\n",
      "[2019-08-17 03:16:28,375] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 324\n",
      "[2019-08-17 03:16:28,375] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 329\n",
      "[2019-08-17 03:16:28,375] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 193\n",
      "[2019-08-17 03:16:28,375] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 273\n",
      "[2019-08-17 03:16:28,376] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 142\n",
      "[2019-08-17 03:16:28,376] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 233\n",
      "[2019-08-17 03:16:28,376] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 311\n",
      "[2019-08-17 03:16:28,377] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 402\n",
      "[2019-08-17 03:16:28,377] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 286\n",
      "[2019-08-17 03:16:28,377] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 274\n",
      "[2019-08-17 03:16:28,377] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 292\n",
      "[2019-08-17 03:16:28,377] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 294\n",
      "[2019-08-17 03:16:28,377] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 181\n",
      "[2019-08-17 03:16:28,377] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 349\n",
      "[2019-08-17 03:16:28,377] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 191\n",
      "[2019-08-17 03:16:28,377] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 218\n",
      "[2019-08-17 03:16:28,377] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 318\n",
      "[2019-08-17 03:16:28,377] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 159\n",
      "[2019-08-17 03:16:28,377] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 340\n",
      "[2019-08-17 03:16:28,377] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 149\n",
      "[2019-08-17 03:16:28,377] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 116\n",
      "[2019-08-17 03:16:28,378] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 199\n",
      "[2019-08-17 03:16:28,378] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 208\n",
      "[2019-08-17 03:16:28,378] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 386\n",
      "[2019-08-17 03:16:28,378] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 293\n",
      "[2019-08-17 03:16:28,378] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 229\n",
      "[2019-08-17 03:16:28,378] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 281\n",
      "[2019-08-17 03:16:28,378] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 345\n",
      "[2019-08-17 03:16:28,378] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 89\n",
      "[2019-08-17 03:16:28,378] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 223\n",
      "[2019-08-17 03:16:28,378] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 248\n",
      "[2019-08-17 03:16:28,378] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 378\n",
      "[2019-08-17 03:16:28,378] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 178\n",
      "[2019-08-17 03:16:28,378] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 262\n",
      "[2019-08-17 03:16:28,378] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 258\n",
      "[2019-08-17 03:16:28,378] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 131\n",
      "[2019-08-17 03:16:28,379] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 351\n",
      "[2019-08-17 03:16:28,379] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ContextCleaner:54 - Cleaned accumulator 204\n",
      "[2019-08-17 03:16:28,420] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  ParquetFileFormat:54 - Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "[2019-08-17 03:16:28,421] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  FileOutputCommitter:123 - File Output Committer Algorithm version is 1\n",
      "[2019-08-17 03:16:28,421] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  FileOutputCommitter:138 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "[2019-08-17 03:16:28,422] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "[2019-08-17 03:16:28,422] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  FileOutputCommitter:123 - File Output Committer Algorithm version is 1\n",
      "[2019-08-17 03:16:28,423] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  FileOutputCommitter:138 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "[2019-08-17 03:16:28,423] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-08-17 03:16:28,454] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  SparkContext:54 - Starting job: parquet at StringIndexer.scala:302\n",
      "[2019-08-17 03:16:28,457] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  DAGScheduler:54 - Registering RDD 55 (parquet at StringIndexer.scala:302)\n",
      "[2019-08-17 03:16:28,458] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  DAGScheduler:54 - Got job 11 (parquet at StringIndexer.scala:302) with 1 output partitions\n",
      "[2019-08-17 03:16:28,458] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  DAGScheduler:54 - Final stage: ResultStage 17 (parquet at StringIndexer.scala:302)\n",
      "[2019-08-17 03:16:28,458] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  DAGScheduler:54 - Parents of final stage: List(ShuffleMapStage 16)\n",
      "[2019-08-17 03:16:28,458] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  DAGScheduler:54 - Missing parents: List(ShuffleMapStage 16)\n",
      "[2019-08-17 03:16:28,459] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  DAGScheduler:54 - Submitting ShuffleMapStage 16 (MapPartitionsRDD[55] at parquet at StringIndexer.scala:302), which has no missing parents\n",
      "[2019-08-17 03:16:28,460] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  MemoryStore:54 - Block broadcast_17 stored as values in memory (estimated size 4.8 KB, free 365.8 MB)\n",
      "[2019-08-17 03:16:28,462] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  MemoryStore:54 - Block broadcast_17_piece0 stored as bytes in memory (estimated size 2.9 KB, free 365.8 MB)\n",
      "[2019-08-17 03:16:28,463] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  BlockManagerInfo:54 - Added broadcast_17_piece0 in memory on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 (size: 2.9 KB, free: 366.2 MB)\n",
      "[2019-08-17 03:16:28,465] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  SparkContext:54 - Created broadcast 17 from broadcast at DAGScheduler.scala:1161\n",
      "[2019-08-17 03:16:28,465] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ShuffleMapStage 16 (MapPartitionsRDD[55] at parquet at StringIndexer.scala:302) (first 15 tasks are for partitions Vector(0))\n",
      "[2019-08-17 03:16:28,465] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  YarnScheduler:54 - Adding task set 16.0 with 1 tasks\n",
      "[2019-08-17 03:16:28,467] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  TaskSetManager:54 - Starting task 0.0 in stage 16.0 (TID 16, ip-172-31-29-159.ap-northeast-2.compute.internal, executor 2, partition 0, PROCESS_LOCAL, 8847 bytes)\n",
      "[2019-08-17 03:16:28,481] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  BlockManagerInfo:54 - Added broadcast_17_piece0 in memory on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 (size: 2.9 KB, free: 9.4 GB)\n",
      "[2019-08-17 03:16:28,490] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  TaskSetManager:54 - Finished task 0.0 in stage 16.0 (TID 16) in 23 ms on ip-172-31-29-159.ap-northeast-2.compute.internal (executor 2) (1/1)\n",
      "[2019-08-17 03:16:28,490] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  YarnScheduler:54 - Removed TaskSet 16.0, whose tasks have all completed, from pool\n",
      "[2019-08-17 03:16:28,491] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  DAGScheduler:54 - ShuffleMapStage 16 (parquet at StringIndexer.scala:302) finished in 0.034 s\n",
      "[2019-08-17 03:16:28,491] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  DAGScheduler:54 - looking for newly runnable stages\n",
      "[2019-08-17 03:16:28,491] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  DAGScheduler:54 - running: Set()\n",
      "[2019-08-17 03:16:28,491] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  DAGScheduler:54 - waiting: Set(ResultStage 17)\n",
      "[2019-08-17 03:16:28,491] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  DAGScheduler:54 - failed: Set()\n",
      "[2019-08-17 03:16:28,492] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  DAGScheduler:54 - Submitting ResultStage 17 (ShuffledRowRDD[56] at parquet at StringIndexer.scala:302), which has no missing parents\n",
      "[2019-08-17 03:16:28,520] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  MemoryStore:54 - Block broadcast_18 stored as values in memory (estimated size 203.7 KB, free 365.6 MB)\n",
      "[2019-08-17 03:16:28,523] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  MemoryStore:54 - Block broadcast_18_piece0 stored as bytes in memory (estimated size 69.6 KB, free 365.5 MB)\n",
      "[2019-08-17 03:16:28,523] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  BlockManagerInfo:54 - Added broadcast_18_piece0 in memory on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 (size: 69.6 KB, free: 366.2 MB)\n",
      "[2019-08-17 03:16:28,524] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  SparkContext:54 - Created broadcast 18 from broadcast at DAGScheduler.scala:1161\n",
      "[2019-08-17 03:16:28,525] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 17 (ShuffledRowRDD[56] at parquet at StringIndexer.scala:302) (first 15 tasks are for partitions Vector(0))\n",
      "[2019-08-17 03:16:28,525] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  YarnScheduler:54 - Adding task set 17.0 with 1 tasks\n",
      "[2019-08-17 03:16:28,527] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  TaskSetManager:54 - Starting task 0.0 in stage 17.0 (TID 17, ip-172-31-29-159.ap-northeast-2.compute.internal, executor 2, partition 0, NODE_LOCAL, 7778 bytes)\n",
      "[2019-08-17 03:16:28,543] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  BlockManagerInfo:54 - Added broadcast_18_piece0 in memory on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 (size: 69.6 KB, free: 9.4 GB)\n",
      "[2019-08-17 03:16:28,576] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  MapOutputTrackerMasterEndpoint:54 - Asked to send map output locations for shuffle 5 to 172.31.29.159:58394\n",
      "[2019-08-17 03:16:28,616] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  TaskSetManager:54 - Finished task 0.0 in stage 17.0 (TID 17) in 90 ms on ip-172-31-29-159.ap-northeast-2.compute.internal (executor 2) (1/1)\n",
      "[2019-08-17 03:16:28,616] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  YarnScheduler:54 - Removed TaskSet 17.0, whose tasks have all completed, from pool\n",
      "[2019-08-17 03:16:28,619] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  DAGScheduler:54 - ResultStage 17 (parquet at StringIndexer.scala:302) finished in 0.127 s\n",
      "[2019-08-17 03:16:28,621] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  DAGScheduler:54 - Job 11 finished: parquet at StringIndexer.scala:302, took 0.166177 s\n",
      "[2019-08-17 03:16:28,631] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  FileFormatWriter:54 - Write Job 4c9fcfa0-3ac7-40a3-a01e-3237eb87d7bd committed.\n",
      "[2019-08-17 03:16:28,632] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  FileFormatWriter:54 - Finished processing stats for write job 4c9fcfa0-3ac7-40a3-a01e-3237eb87d7bd.\n",
      "[2019-08-17 03:16:28,747] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  CodeGenerator:54 - Code generated in 13.936102 ms\n",
      "[2019-08-17 03:16:28,763] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  SparkContext:54 - Starting job: countByValue at StringIndexer.scala:140\n",
      "[2019-08-17 03:16:28,766] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  DAGScheduler:54 - Registering RDD 64 (countByValue at StringIndexer.scala:140)\n",
      "[2019-08-17 03:16:28,766] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  DAGScheduler:54 - Got job 12 (countByValue at StringIndexer.scala:140) with 1 output partitions\n",
      "[2019-08-17 03:16:28,766] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  DAGScheduler:54 - Final stage: ResultStage 19 (countByValue at StringIndexer.scala:140)\n",
      "[2019-08-17 03:16:28,767] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  DAGScheduler:54 - Parents of final stage: List(ShuffleMapStage 18)\n",
      "[2019-08-17 03:16:28,767] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  DAGScheduler:54 - Missing parents: List(ShuffleMapStage 18)\n",
      "[2019-08-17 03:16:28,767] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  DAGScheduler:54 - Submitting ShuffleMapStage 18 (MapPartitionsRDD[64] at countByValue at StringIndexer.scala:140), which has no missing parents\n",
      "[2019-08-17 03:16:28,784] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  MemoryStore:54 - Block broadcast_19 stored as values in memory (estimated size 40.9 KB, free 365.5 MB)\n",
      "[2019-08-17 03:16:28,787] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  MemoryStore:54 - Block broadcast_19_piece0 stored as bytes in memory (estimated size 19.3 KB, free 365.5 MB)\n",
      "[2019-08-17 03:16:28,788] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  BlockManagerInfo:54 - Added broadcast_19_piece0 in memory on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 (size: 19.3 KB, free: 366.1 MB)\n",
      "[2019-08-17 03:16:28,789] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  SparkContext:54 - Created broadcast 19 from broadcast at DAGScheduler.scala:1161\n",
      "[2019-08-17 03:16:28,790] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ShuffleMapStage 18 (MapPartitionsRDD[64] at countByValue at StringIndexer.scala:140) (first 15 tasks are for partitions Vector(0))\n",
      "[2019-08-17 03:16:28,790] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  YarnScheduler:54 - Adding task set 18.0 with 1 tasks\n",
      "[2019-08-17 03:16:28,792] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  TaskSetManager:54 - Starting task 0.0 in stage 18.0 (TID 18, ip-172-31-29-159.ap-northeast-2.compute.internal, executor 2, partition 0, NODE_LOCAL, 8374 bytes)\n",
      "[2019-08-17 03:16:28,803] {bash_operator.py:127} INFO - 2019-08-17 03:16:28 INFO  BlockManagerInfo:54 - Added broadcast_19_piece0 in memory on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 (size: 19.3 KB, free: 9.4 GB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-08-17 03:16:29,181] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  TaskSetManager:54 - Finished task 0.0 in stage 18.0 (TID 18) in 389 ms on ip-172-31-29-159.ap-northeast-2.compute.internal (executor 2) (1/1)\n",
      "[2019-08-17 03:16:29,181] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  YarnScheduler:54 - Removed TaskSet 18.0, whose tasks have all completed, from pool\n",
      "[2019-08-17 03:16:29,183] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  DAGScheduler:54 - ShuffleMapStage 18 (countByValue at StringIndexer.scala:140) finished in 0.413 s\n",
      "[2019-08-17 03:16:29,183] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  DAGScheduler:54 - looking for newly runnable stages\n",
      "[2019-08-17 03:16:29,183] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  DAGScheduler:54 - running: Set()\n",
      "[2019-08-17 03:16:29,183] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  DAGScheduler:54 - waiting: Set(ResultStage 19)\n",
      "[2019-08-17 03:16:29,183] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  DAGScheduler:54 - failed: Set()\n",
      "[2019-08-17 03:16:29,183] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  DAGScheduler:54 - Submitting ResultStage 19 (ShuffledRDD[65] at countByValue at StringIndexer.scala:140), which has no missing parents\n",
      "[2019-08-17 03:16:29,184] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  MemoryStore:54 - Block broadcast_20 stored as values in memory (estimated size 3.3 KB, free 365.5 MB)\n",
      "[2019-08-17 03:16:29,185] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  MemoryStore:54 - Block broadcast_20_piece0 stored as bytes in memory (estimated size 1955.0 B, free 365.5 MB)\n",
      "[2019-08-17 03:16:29,186] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  BlockManagerInfo:54 - Added broadcast_20_piece0 in memory on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 (size: 1955.0 B, free: 366.1 MB)\n",
      "[2019-08-17 03:16:29,186] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  SparkContext:54 - Created broadcast 20 from broadcast at DAGScheduler.scala:1161\n",
      "[2019-08-17 03:16:29,188] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 19 (ShuffledRDD[65] at countByValue at StringIndexer.scala:140) (first 15 tasks are for partitions Vector(0))\n",
      "[2019-08-17 03:16:29,189] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  YarnScheduler:54 - Adding task set 19.0 with 1 tasks\n",
      "[2019-08-17 03:16:29,190] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  TaskSetManager:54 - Starting task 0.0 in stage 19.0 (TID 19, ip-172-31-29-159.ap-northeast-2.compute.internal, executor 2, partition 0, NODE_LOCAL, 7673 bytes)\n",
      "[2019-08-17 03:16:29,202] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  BlockManagerInfo:54 - Added broadcast_20_piece0 in memory on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 (size: 1955.0 B, free: 9.4 GB)\n",
      "[2019-08-17 03:16:29,207] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  MapOutputTrackerMasterEndpoint:54 - Asked to send map output locations for shuffle 6 to 172.31.29.159:58394\n",
      "[2019-08-17 03:16:29,236] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  TaskSetManager:54 - Finished task 0.0 in stage 19.0 (TID 19) in 46 ms on ip-172-31-29-159.ap-northeast-2.compute.internal (executor 2) (1/1)\n",
      "[2019-08-17 03:16:29,238] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  YarnScheduler:54 - Removed TaskSet 19.0, whose tasks have all completed, from pool\n",
      "[2019-08-17 03:16:29,238] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  DAGScheduler:54 - ResultStage 19 (countByValue at StringIndexer.scala:140) finished in 0.053 s\n",
      "[2019-08-17 03:16:29,238] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  DAGScheduler:54 - Job 12 finished: countByValue at StringIndexer.scala:140, took 0.473190 s\n",
      "[2019-08-17 03:16:29,266] {bash_operator.py:127} INFO - /models/string_indexer_model_Route.bin\n",
      "[2019-08-17 03:16:29,283] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  FileSystemOverwrite:54 - Path /models/string_indexer_model_Route.bin already exists. It will be overwritten.\n",
      "[2019-08-17 03:16:29,295] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  HadoopMapRedCommitProtocol:54 - Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "[2019-08-17 03:16:29,296] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  FileOutputCommitter:123 - File Output Committer Algorithm version is 1\n",
      "[2019-08-17 03:16:29,296] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  FileOutputCommitter:138 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "[2019-08-17 03:16:29,307] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  SparkContext:54 - Starting job: runJob at SparkHadoopWriter.scala:78\n",
      "[2019-08-17 03:16:29,309] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  DAGScheduler:54 - Got job 13 (runJob at SparkHadoopWriter.scala:78) with 1 output partitions\n",
      "[2019-08-17 03:16:29,309] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  DAGScheduler:54 - Final stage: ResultStage 20 (runJob at SparkHadoopWriter.scala:78)\n",
      "[2019-08-17 03:16:29,309] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  DAGScheduler:54 - Parents of final stage: List()\n",
      "[2019-08-17 03:16:29,309] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  DAGScheduler:54 - Missing parents: List()\n",
      "[2019-08-17 03:16:29,309] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  DAGScheduler:54 - Submitting ResultStage 20 (MapPartitionsRDD[67] at saveAsTextFile at ReadWrite.scala:441), which has no missing parents\n",
      "[2019-08-17 03:16:29,318] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  MemoryStore:54 - Block broadcast_21 stored as values in memory (estimated size 86.1 KB, free 365.4 MB)\n",
      "[2019-08-17 03:16:29,323] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  MemoryStore:54 - Block broadcast_21_piece0 stored as bytes in memory (estimated size 30.2 KB, free 365.3 MB)\n",
      "[2019-08-17 03:16:29,324] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  BlockManagerInfo:54 - Added broadcast_21_piece0 in memory on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 (size: 30.2 KB, free: 366.1 MB)\n",
      "[2019-08-17 03:16:29,325] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  SparkContext:54 - Created broadcast 21 from broadcast at DAGScheduler.scala:1161\n",
      "[2019-08-17 03:16:29,326] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 20 (MapPartitionsRDD[67] at saveAsTextFile at ReadWrite.scala:441) (first 15 tasks are for partitions Vector(0))\n",
      "[2019-08-17 03:16:29,326] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  YarnScheduler:54 - Adding task set 20.0 with 1 tasks\n",
      "[2019-08-17 03:16:29,328] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  TaskSetManager:54 - Starting task 0.0 in stage 20.0 (TID 20, ip-172-31-18-11.ap-northeast-2.compute.internal, executor 1, partition 0, PROCESS_LOCAL, 8209 bytes)\n",
      "[2019-08-17 03:16:29,344] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  BlockManagerInfo:54 - Added broadcast_21_piece0 in memory on ip-172-31-18-11.ap-northeast-2.compute.internal:38933 (size: 30.2 KB, free: 9.4 GB)\n",
      "[2019-08-17 03:16:29,409] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  TaskSetManager:54 - Finished task 0.0 in stage 20.0 (TID 20) in 80 ms on ip-172-31-18-11.ap-northeast-2.compute.internal (executor 1) (1/1)\n",
      "[2019-08-17 03:16:29,410] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  YarnScheduler:54 - Removed TaskSet 20.0, whose tasks have all completed, from pool\n",
      "[2019-08-17 03:16:29,411] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  DAGScheduler:54 - ResultStage 20 (runJob at SparkHadoopWriter.scala:78) finished in 0.101 s\n",
      "[2019-08-17 03:16:29,411] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  DAGScheduler:54 - Job 13 finished: runJob at SparkHadoopWriter.scala:78, took 0.102970 s\n",
      "[2019-08-17 03:16:29,421] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  SparkHadoopWriter:54 - Job job_20190817031629_0067 committed.\n",
      "[2019-08-17 03:16:29,453] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  ParquetFileFormat:54 - Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "[2019-08-17 03:16:29,459] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  FileOutputCommitter:123 - File Output Committer Algorithm version is 1\n",
      "[2019-08-17 03:16:29,460] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  FileOutputCommitter:138 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "[2019-08-17 03:16:29,460] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "[2019-08-17 03:16:29,460] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  FileOutputCommitter:123 - File Output Committer Algorithm version is 1\n",
      "[2019-08-17 03:16:29,460] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  FileOutputCommitter:138 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "[2019-08-17 03:16:29,461] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-08-17 03:16:29,486] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  SparkContext:54 - Starting job: parquet at StringIndexer.scala:302\n",
      "[2019-08-17 03:16:29,488] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  DAGScheduler:54 - Registering RDD 70 (parquet at StringIndexer.scala:302)\n",
      "[2019-08-17 03:16:29,488] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  DAGScheduler:54 - Got job 14 (parquet at StringIndexer.scala:302) with 1 output partitions\n",
      "[2019-08-17 03:16:29,488] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  DAGScheduler:54 - Final stage: ResultStage 22 (parquet at StringIndexer.scala:302)\n",
      "[2019-08-17 03:16:29,488] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  DAGScheduler:54 - Parents of final stage: List(ShuffleMapStage 21)\n",
      "[2019-08-17 03:16:29,490] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  DAGScheduler:54 - Missing parents: List(ShuffleMapStage 21)\n",
      "[2019-08-17 03:16:29,490] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  DAGScheduler:54 - Submitting ShuffleMapStage 21 (MapPartitionsRDD[70] at parquet at StringIndexer.scala:302), which has no missing parents\n",
      "[2019-08-17 03:16:29,490] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  MemoryStore:54 - Block broadcast_22 stored as values in memory (estimated size 4.8 KB, free 365.3 MB)\n",
      "[2019-08-17 03:16:29,492] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  MemoryStore:54 - Block broadcast_22_piece0 stored as bytes in memory (estimated size 2.9 KB, free 365.3 MB)\n",
      "[2019-08-17 03:16:29,492] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  BlockManagerInfo:54 - Added broadcast_22_piece0 in memory on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 (size: 2.9 KB, free: 366.1 MB)\n",
      "[2019-08-17 03:16:29,493] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  SparkContext:54 - Created broadcast 22 from broadcast at DAGScheduler.scala:1161\n",
      "[2019-08-17 03:16:29,493] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ShuffleMapStage 21 (MapPartitionsRDD[70] at parquet at StringIndexer.scala:302) (first 15 tasks are for partitions Vector(0))\n",
      "[2019-08-17 03:16:29,494] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  YarnScheduler:54 - Adding task set 21.0 with 1 tasks\n",
      "[2019-08-17 03:16:29,495] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  TaskSetManager:54 - Starting task 0.0 in stage 21.0 (TID 21, ip-172-31-29-159.ap-northeast-2.compute.internal, executor 2, partition 0, PROCESS_LOCAL, 10649 bytes)\n",
      "[2019-08-17 03:16:29,507] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  BlockManagerInfo:54 - Added broadcast_22_piece0 in memory on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 (size: 2.9 KB, free: 9.4 GB)\n",
      "[2019-08-17 03:16:29,515] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  TaskSetManager:54 - Finished task 0.0 in stage 21.0 (TID 21) in 21 ms on ip-172-31-29-159.ap-northeast-2.compute.internal (executor 2) (1/1)\n",
      "[2019-08-17 03:16:29,516] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  YarnScheduler:54 - Removed TaskSet 21.0, whose tasks have all completed, from pool\n",
      "[2019-08-17 03:16:29,517] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  DAGScheduler:54 - ShuffleMapStage 21 (parquet at StringIndexer.scala:302) finished in 0.027 s\n",
      "[2019-08-17 03:16:29,517] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  DAGScheduler:54 - looking for newly runnable stages\n",
      "[2019-08-17 03:16:29,517] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  DAGScheduler:54 - running: Set()\n",
      "[2019-08-17 03:16:29,517] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  DAGScheduler:54 - waiting: Set(ResultStage 22)\n",
      "[2019-08-17 03:16:29,517] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  DAGScheduler:54 - failed: Set()\n",
      "[2019-08-17 03:16:29,517] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  DAGScheduler:54 - Submitting ResultStage 22 (ShuffledRowRDD[71] at parquet at StringIndexer.scala:302), which has no missing parents\n",
      "[2019-08-17 03:16:29,538] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  MemoryStore:54 - Block broadcast_23 stored as values in memory (estimated size 203.7 KB, free 365.1 MB)\n",
      "[2019-08-17 03:16:29,540] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  MemoryStore:54 - Block broadcast_23_piece0 stored as bytes in memory (estimated size 69.6 KB, free 365.1 MB)\n",
      "[2019-08-17 03:16:29,541] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  BlockManagerInfo:54 - Added broadcast_23_piece0 in memory on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 (size: 69.6 KB, free: 366.0 MB)\n",
      "[2019-08-17 03:16:29,542] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  SparkContext:54 - Created broadcast 23 from broadcast at DAGScheduler.scala:1161\n",
      "[2019-08-17 03:16:29,543] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 22 (ShuffledRowRDD[71] at parquet at StringIndexer.scala:302) (first 15 tasks are for partitions Vector(0))\n",
      "[2019-08-17 03:16:29,543] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  YarnScheduler:54 - Adding task set 22.0 with 1 tasks\n",
      "[2019-08-17 03:16:29,544] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  TaskSetManager:54 - Starting task 0.0 in stage 22.0 (TID 22, ip-172-31-29-159.ap-northeast-2.compute.internal, executor 2, partition 0, NODE_LOCAL, 7778 bytes)\n",
      "[2019-08-17 03:16:29,559] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  BlockManagerInfo:54 - Added broadcast_23_piece0 in memory on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 (size: 69.6 KB, free: 9.4 GB)\n",
      "[2019-08-17 03:16:29,586] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  MapOutputTrackerMasterEndpoint:54 - Asked to send map output locations for shuffle 7 to 172.31.29.159:58394\n",
      "[2019-08-17 03:16:29,629] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  TaskSetManager:54 - Finished task 0.0 in stage 22.0 (TID 22) in 84 ms on ip-172-31-29-159.ap-northeast-2.compute.internal (executor 2) (1/1)\n",
      "[2019-08-17 03:16:29,629] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  YarnScheduler:54 - Removed TaskSet 22.0, whose tasks have all completed, from pool\n",
      "[2019-08-17 03:16:29,630] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  DAGScheduler:54 - ResultStage 22 (parquet at StringIndexer.scala:302) finished in 0.113 s\n",
      "[2019-08-17 03:16:29,631] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  DAGScheduler:54 - Job 14 finished: parquet at StringIndexer.scala:302, took 0.143766 s\n",
      "[2019-08-17 03:16:29,641] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  FileFormatWriter:54 - Write Job 0b68516f-35b0-45f1-9884-e2729e311734 committed.\n",
      "[2019-08-17 03:16:29,642] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  FileFormatWriter:54 - Finished processing stats for write job 0b68516f-35b0-45f1-9884-e2729e311734.\n",
      "[2019-08-17 03:16:29,713] {bash_operator.py:127} INFO - /models/numeric_vector_assembler.bin\n",
      "[2019-08-17 03:16:29,723] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  FileSystemOverwrite:54 - Path /models/numeric_vector_assembler.bin already exists. It will be overwritten.\n",
      "[2019-08-17 03:16:29,731] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  HadoopMapRedCommitProtocol:54 - Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "[2019-08-17 03:16:29,731] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  FileOutputCommitter:123 - File Output Committer Algorithm version is 1\n",
      "[2019-08-17 03:16:29,731] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  FileOutputCommitter:138 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "[2019-08-17 03:16:29,744] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  SparkContext:54 - Starting job: runJob at SparkHadoopWriter.scala:78\n",
      "[2019-08-17 03:16:29,744] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  DAGScheduler:54 - Got job 15 (runJob at SparkHadoopWriter.scala:78) with 1 output partitions\n",
      "[2019-08-17 03:16:29,744] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  DAGScheduler:54 - Final stage: ResultStage 23 (runJob at SparkHadoopWriter.scala:78)\n",
      "[2019-08-17 03:16:29,745] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  DAGScheduler:54 - Parents of final stage: List()\n",
      "[2019-08-17 03:16:29,745] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  DAGScheduler:54 - Missing parents: List()\n",
      "[2019-08-17 03:16:29,745] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  DAGScheduler:54 - Submitting ResultStage 23 (MapPartitionsRDD[74] at saveAsTextFile at ReadWrite.scala:441), which has no missing parents\n",
      "[2019-08-17 03:16:29,754] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  MemoryStore:54 - Block broadcast_24 stored as values in memory (estimated size 86.1 KB, free 365.0 MB)\n",
      "[2019-08-17 03:16:29,756] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  MemoryStore:54 - Block broadcast_24_piece0 stored as bytes in memory (estimated size 30.3 KB, free 364.9 MB)\n",
      "[2019-08-17 03:16:29,756] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  BlockManagerInfo:54 - Added broadcast_24_piece0 in memory on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 (size: 30.3 KB, free: 366.0 MB)\n",
      "[2019-08-17 03:16:29,757] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  SparkContext:54 - Created broadcast 24 from broadcast at DAGScheduler.scala:1161\n",
      "[2019-08-17 03:16:29,757] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[74] at saveAsTextFile at ReadWrite.scala:441) (first 15 tasks are for partitions Vector(0))\n",
      "[2019-08-17 03:16:29,758] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  YarnScheduler:54 - Adding task set 23.0 with 1 tasks\n",
      "[2019-08-17 03:16:29,761] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  TaskSetManager:54 - Starting task 0.0 in stage 23.0 (TID 23, ip-172-31-29-159.ap-northeast-2.compute.internal, executor 2, partition 0, PROCESS_LOCAL, 8289 bytes)\n",
      "[2019-08-17 03:16:29,771] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  BlockManagerInfo:54 - Added broadcast_24_piece0 in memory on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 (size: 30.3 KB, free: 9.4 GB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-08-17 03:16:29,818] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  TaskSetManager:54 - Finished task 0.0 in stage 23.0 (TID 23) in 60 ms on ip-172-31-29-159.ap-northeast-2.compute.internal (executor 2) (1/1)\n",
      "[2019-08-17 03:16:29,818] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  YarnScheduler:54 - Removed TaskSet 23.0, whose tasks have all completed, from pool\n",
      "[2019-08-17 03:16:29,819] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  DAGScheduler:54 - ResultStage 23 (runJob at SparkHadoopWriter.scala:78) finished in 0.073 s\n",
      "[2019-08-17 03:16:29,820] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  DAGScheduler:54 - Job 15 finished: runJob at SparkHadoopWriter.scala:78, took 0.076082 s\n",
      "[2019-08-17 03:16:29,829] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  SparkHadoopWriter:54 - Job job_20190817031629_0074 committed.\n",
      "[2019-08-17 03:16:29,936] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  Instrumentation:54 - [a07343fc] Stage class: RandomForestClassifier\n",
      "[2019-08-17 03:16:29,936] {bash_operator.py:127} INFO - 2019-08-17 03:16:29 INFO  Instrumentation:54 - [a07343fc] Stage uid: RandomForestClassifier_92c6dbf0836a\n",
      "[2019-08-17 03:16:30,166] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  CodeGenerator:54 - Code generated in 66.674915 ms\n",
      "[2019-08-17 03:16:30,200] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  Instrumentation:54 - [a07343fc] training: numPartitions=1 storageLevel=StorageLevel(1 replicas)\n",
      "[2019-08-17 03:16:30,374] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 504\n",
      "[2019-08-17 03:16:30,374] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 638\n",
      "[2019-08-17 03:16:30,375] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 416\n",
      "[2019-08-17 03:16:30,376] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 467\n",
      "[2019-08-17 03:16:30,376] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 538\n",
      "[2019-08-17 03:16:30,376] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 619\n",
      "[2019-08-17 03:16:30,376] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 430\n",
      "[2019-08-17 03:16:30,376] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 443\n",
      "[2019-08-17 03:16:30,376] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 601\n",
      "[2019-08-17 03:16:30,376] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 642\n",
      "[2019-08-17 03:16:30,376] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 498\n",
      "[2019-08-17 03:16:30,376] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 516\n",
      "[2019-08-17 03:16:30,376] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 589\n",
      "[2019-08-17 03:16:30,376] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 639\n",
      "[2019-08-17 03:16:30,378] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned shuffle 6\n",
      "[2019-08-17 03:16:30,378] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 547\n",
      "[2019-08-17 03:16:30,378] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 636\n",
      "[2019-08-17 03:16:30,378] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 553\n",
      "[2019-08-17 03:16:30,378] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 634\n",
      "[2019-08-17 03:16:30,378] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 490\n",
      "[2019-08-17 03:16:30,378] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 523\n",
      "[2019-08-17 03:16:30,380] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 434\n",
      "[2019-08-17 03:16:30,380] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 564\n",
      "[2019-08-17 03:16:30,380] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 566\n",
      "[2019-08-17 03:16:30,380] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 542\n",
      "[2019-08-17 03:16:30,380] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 534\n",
      "[2019-08-17 03:16:30,381] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 485\n",
      "[2019-08-17 03:16:30,381] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 478\n",
      "[2019-08-17 03:16:30,381] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 596\n",
      "[2019-08-17 03:16:30,381] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 464\n",
      "[2019-08-17 03:16:30,381] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 594\n",
      "[2019-08-17 03:16:30,381] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 494\n",
      "[2019-08-17 03:16:30,381] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 549\n",
      "[2019-08-17 03:16:30,381] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 626\n",
      "[2019-08-17 03:16:30,381] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 567\n",
      "[2019-08-17 03:16:30,381] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 608\n",
      "[2019-08-17 03:16:30,381] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 519\n",
      "[2019-08-17 03:16:30,381] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 590\n",
      "[2019-08-17 03:16:30,381] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 408\n",
      "[2019-08-17 03:16:30,381] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 449\n",
      "[2019-08-17 03:16:30,381] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 581\n",
      "[2019-08-17 03:16:30,382] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 419\n",
      "[2019-08-17 03:16:30,382] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 438\n",
      "[2019-08-17 03:16:30,382] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 426\n",
      "[2019-08-17 03:16:30,382] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 441\n",
      "[2019-08-17 03:16:30,382] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 537\n",
      "[2019-08-17 03:16:30,382] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 406\n",
      "[2019-08-17 03:16:30,383] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 472\n",
      "[2019-08-17 03:16:30,383] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 514\n",
      "[2019-08-17 03:16:30,383] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 454\n",
      "[2019-08-17 03:16:30,383] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 571\n",
      "[2019-08-17 03:16:30,383] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 645\n",
      "[2019-08-17 03:16:30,383] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 622\n",
      "[2019-08-17 03:16:30,401] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  BlockManagerInfo:54 - Removed broadcast_24_piece0 on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 in memory (size: 30.3 KB, free: 366.0 MB)\n",
      "[2019-08-17 03:16:30,404] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  BlockManagerInfo:54 - Removed broadcast_24_piece0 on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 in memory (size: 30.3 KB, free: 9.4 GB)\n",
      "[2019-08-17 03:16:30,415] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 422\n",
      "[2019-08-17 03:16:30,415] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 592\n",
      "[2019-08-17 03:16:30,415] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 562\n",
      "[2019-08-17 03:16:30,415] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 625\n",
      "[2019-08-17 03:16:30,415] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 551\n",
      "[2019-08-17 03:16:30,415] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 503\n",
      "[2019-08-17 03:16:30,417] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 528\n",
      "[2019-08-17 03:16:30,417] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 417\n",
      "[2019-08-17 03:16:30,417] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 593\n",
      "[2019-08-17 03:16:30,418] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 584\n",
      "[2019-08-17 03:16:30,418] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 453\n",
      "[2019-08-17 03:16:30,418] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 541\n",
      "[2019-08-17 03:16:30,418] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 525\n",
      "[2019-08-17 03:16:30,418] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 539\n",
      "[2019-08-17 03:16:30,418] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 616\n",
      "[2019-08-17 03:16:30,418] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 558\n",
      "[2019-08-17 03:16:30,418] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned shuffle 7\n",
      "[2019-08-17 03:16:30,418] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 492\n",
      "[2019-08-17 03:16:30,418] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 502\n",
      "[2019-08-17 03:16:30,420] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 527\n",
      "[2019-08-17 03:16:30,420] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 588\n",
      "[2019-08-17 03:16:30,420] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 545\n",
      "[2019-08-17 03:16:30,420] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 575\n",
      "[2019-08-17 03:16:30,421] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 518\n",
      "[2019-08-17 03:16:30,421] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 535\n",
      "[2019-08-17 03:16:30,421] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 429\n",
      "[2019-08-17 03:16:30,421] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  BlockManagerInfo:54 - Removed broadcast_17_piece0 on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 in memory (size: 2.9 KB, free: 9.4 GB)\n",
      "[2019-08-17 03:16:30,432] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  BlockManagerInfo:54 - Removed broadcast_17_piece0 on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 in memory (size: 2.9 KB, free: 366.0 MB)\n",
      "[2019-08-17 03:16:30,451] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 423\n",
      "[2019-08-17 03:16:30,451] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 543\n",
      "[2019-08-17 03:16:30,451] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 501\n",
      "[2019-08-17 03:16:30,451] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 420\n",
      "[2019-08-17 03:16:30,451] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-08-17 03:16:30,455] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  BlockManagerInfo:54 - Removed broadcast_21_piece0 on ip-172-31-18-11.ap-northeast-2.compute.internal:38933 in memory (size: 30.2 KB, free: 9.4 GB)\r\n",
      "[2019-08-17 03:16:30,459] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  CodeGenerator:54 - Code generated in 123.660389 ms\r\n",
      "[2019-08-17 03:16:30,460] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  BlockManagerInfo:54 - Removed broadcast_21_piece0 on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 in memory (size: 30.2 KB, free: 366.1 MB)\r\n",
      "[2019-08-17 03:16:30,476] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 595\r\n",
      "[2019-08-17 03:16:30,477] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 591\r\n",
      "[2019-08-17 03:16:30,477] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 486\r\n",
      "[2019-08-17 03:16:30,477] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 442\r\n",
      "[2019-08-17 03:16:30,477] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 578\r\n",
      "[2019-08-17 03:16:30,477] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 496\r\n",
      "[2019-08-17 03:16:30,479] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 533\r\n",
      "[2019-08-17 03:16:30,479] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 631\r\n",
      "[2019-08-17 03:16:30,479] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 635\r\n",
      "[2019-08-17 03:16:30,479] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 424\r\n",
      "[2019-08-17 03:16:30,479] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 618\r\n",
      "[2019-08-17 03:16:30,479] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 439\r\n",
      "[2019-08-17 03:16:30,479] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 561\r\n",
      "[2019-08-17 03:16:30,479] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 421\r\n",
      "[2019-08-17 03:16:30,479] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 435\r\n",
      "[2019-08-17 03:16:30,479] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 415\r\n",
      "[2019-08-17 03:16:30,479] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 515\r\n",
      "[2019-08-17 03:16:30,479] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 572\r\n",
      "[2019-08-17 03:16:30,480] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 548\r\n",
      "[2019-08-17 03:16:30,480] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 630\r\n",
      "[2019-08-17 03:16:30,480] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 511\r\n",
      "[2019-08-17 03:16:30,480] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 607\r\n",
      "[2019-08-17 03:16:30,480] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 433\r\n",
      "[2019-08-17 03:16:30,480] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 579\r\n",
      "[2019-08-17 03:16:30,480] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 411\r\n",
      "[2019-08-17 03:16:30,480] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 632\r\n",
      "[2019-08-17 03:16:30,480] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 612\r\n",
      "[2019-08-17 03:16:30,480] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 615\r\n",
      "[2019-08-17 03:16:30,480] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 620\r\n",
      "[2019-08-17 03:16:30,480] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 517\r\n",
      "[2019-08-17 03:16:30,482] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  BlockManagerInfo:54 - Removed broadcast_19_piece0 on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 in memory (size: 19.3 KB, free: 9.4 GB)\r\n",
      "[2019-08-17 03:16:30,482] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  BlockManagerInfo:54 - Removed broadcast_19_piece0 on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 in memory (size: 19.3 KB, free: 366.1 MB)\r\n",
      "[2019-08-17 03:16:30,484] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  Instrumentation:54 - [a07343fc] {\"featuresCol\":\"Features_vec\",\"maxDepth\":10,\"labelCol\":\"ArrDelayBucket\",\"maxMemoryInMB\":1024,\"maxBins\":4657,\"numTrees\":10}\r\n",
      "[2019-08-17 03:16:30,495] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 603\r\n",
      "[2019-08-17 03:16:30,496] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 536\r\n",
      "[2019-08-17 03:16:30,496] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 465\r\n",
      "[2019-08-17 03:16:30,496] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 432\r\n",
      "[2019-08-17 03:16:30,499] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned shuffle 5\r\n",
      "[2019-08-17 03:16:30,499] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 530\r\n",
      "[2019-08-17 03:16:30,499] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 452\r\n",
      "[2019-08-17 03:16:30,499] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 455\r\n",
      "[2019-08-17 03:16:30,499] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 544\r\n",
      "[2019-08-17 03:16:30,499] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 437\r\n",
      "[2019-08-17 03:16:30,499] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 410\r\n",
      "[2019-08-17 03:16:30,499] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 414\r\n",
      "[2019-08-17 03:16:30,500] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 613\r\n",
      "[2019-08-17 03:16:30,500] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 468\r\n",
      "[2019-08-17 03:16:30,500] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 574\r\n",
      "[2019-08-17 03:16:30,500] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 482\r\n",
      "[2019-08-17 03:16:30,500] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 627\r\n",
      "[2019-08-17 03:16:30,500] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 457\r\n",
      "[2019-08-17 03:16:30,500] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 466\r\n",
      "[2019-08-17 03:16:30,500] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 493\r\n",
      "[2019-08-17 03:16:30,500] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 513\r\n",
      "[2019-08-17 03:16:30,500] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 597\r\n",
      "[2019-08-17 03:16:30,500] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 568\r\n",
      "[2019-08-17 03:16:30,500] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 555\r\n",
      "[2019-08-17 03:16:30,500] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 586\r\n",
      "[2019-08-17 03:16:30,500] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 444\r\n",
      "[2019-08-17 03:16:30,500] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 456\r\n",
      "[2019-08-17 03:16:30,501] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 629\r\n",
      "[2019-08-17 03:16:30,501] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 451\r\n",
      "[2019-08-17 03:16:30,501] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 479\r\n",
      "[2019-08-17 03:16:30,501] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 471\r\n",
      "[2019-08-17 03:16:30,501] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 470\r\n",
      "[2019-08-17 03:16:30,501] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 458\r\n",
      "[2019-08-17 03:16:30,501] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 448\r\n",
      "[2019-08-17 03:16:30,501] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 621\r\n",
      "[2019-08-17 03:16:30,501] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 552\r\n",
      "[2019-08-17 03:16:30,501] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 560\r\n",
      "[2019-08-17 03:16:30,501] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 497\r\n",
      "[2019-08-17 03:16:30,502] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 559\r\n",
      "[2019-08-17 03:16:30,502] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 436\r\n",
      "[2019-08-17 03:16:30,502] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 500\r\n",
      "[2019-08-17 03:16:30,503] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 576\r\n",
      "[2019-08-17 03:16:30,503] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 473\r\n",
      "[2019-08-17 03:16:30,503] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 491\r\n",
      "[2019-08-17 03:16:30,503] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 462\r\n",
      "[2019-08-17 03:16:30,503] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 475\r\n",
      "[2019-08-17 03:16:30,503] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 624\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-08-17 03:16:30,503] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 573\r\n",
      "[2019-08-17 03:16:30,503] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 460\r\n",
      "[2019-08-17 03:16:30,503] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 450\r\n",
      "[2019-08-17 03:16:30,503] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 484\r\n",
      "[2019-08-17 03:16:30,503] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 585\r\n",
      "[2019-08-17 03:16:30,507] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 481\r\n",
      "[2019-08-17 03:16:30,507] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 488\r\n",
      "[2019-08-17 03:16:30,508] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 602\r\n",
      "[2019-08-17 03:16:30,508] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 522\r\n",
      "[2019-08-17 03:16:30,508] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 476\r\n",
      "[2019-08-17 03:16:30,508] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 428\r\n",
      "[2019-08-17 03:16:30,508] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 463\r\n",
      "[2019-08-17 03:16:30,508] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 577\r\n",
      "[2019-08-17 03:16:30,509] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 483\r\n",
      "[2019-08-17 03:16:30,509] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 412\r\n",
      "[2019-08-17 03:16:30,509] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 507\r\n",
      "[2019-08-17 03:16:30,509] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 510\r\n",
      "[2019-08-17 03:16:30,509] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 524\r\n",
      "[2019-08-17 03:16:30,509] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 582\r\n",
      "[2019-08-17 03:16:30,511] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  BlockManagerInfo:54 - Removed broadcast_22_piece0 on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 in memory (size: 2.9 KB, free: 9.4 GB)\r\n",
      "[2019-08-17 03:16:30,516] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  BlockManagerInfo:54 - Removed broadcast_22_piece0 on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 in memory (size: 2.9 KB, free: 366.1 MB)\r\n",
      "[2019-08-17 03:16:30,535] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  SparkContext:54 - Starting job: take at DecisionTreeMetadata.scala:112\r\n",
      "[2019-08-17 03:16:30,537] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 529\r\n",
      "[2019-08-17 03:16:30,538] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 610\r\n",
      "[2019-08-17 03:16:30,538] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 508\r\n",
      "[2019-08-17 03:16:30,538] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 526\r\n",
      "[2019-08-17 03:16:30,538] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 605\r\n",
      "[2019-08-17 03:16:30,539] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 606\r\n",
      "[2019-08-17 03:16:30,539] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 633\r\n",
      "[2019-08-17 03:16:30,539] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 554\r\n",
      "[2019-08-17 03:16:30,539] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 640\r\n",
      "[2019-08-17 03:16:30,539] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 405\r\n",
      "[2019-08-17 03:16:30,539] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 480\r\n",
      "[2019-08-17 03:16:30,539] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 512\r\n",
      "[2019-08-17 03:16:30,539] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 583\r\n",
      "[2019-08-17 03:16:30,539] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 427\r\n",
      "[2019-08-17 03:16:30,539] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 580\r\n",
      "[2019-08-17 03:16:30,540] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  DAGScheduler:54 - Got job 16 (take at DecisionTreeMetadata.scala:112) with 1 output partitions\r\n",
      "[2019-08-17 03:16:30,540] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  DAGScheduler:54 - Final stage: ResultStage 24 (take at DecisionTreeMetadata.scala:112)\r\n",
      "[2019-08-17 03:16:30,540] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  DAGScheduler:54 - Parents of final stage: List()\r\n",
      "[2019-08-17 03:16:30,540] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  DAGScheduler:54 - Missing parents: List()\r\n",
      "[2019-08-17 03:16:30,541] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  DAGScheduler:54 - Submitting ResultStage 24 (MapPartitionsRDD[85] at map at DecisionTreeMetadata.scala:112), which has no missing parents\r\n",
      "[2019-08-17 03:16:30,542] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  BlockManagerInfo:54 - Removed broadcast_20_piece0 on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 in memory (size: 1955.0 B, free: 366.1 MB)\r\n",
      "[2019-08-17 03:16:30,543] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  BlockManagerInfo:54 - Removed broadcast_20_piece0 on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 in memory (size: 1955.0 B, free: 9.4 GB)\r\n",
      "[2019-08-17 03:16:30,545] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  MemoryStore:54 - Block broadcast_25 stored as values in memory (estimated size 74.3 KB, free 365.2 MB)\r\n",
      "[2019-08-17 03:16:30,546] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 557\r\n",
      "[2019-08-17 03:16:30,546] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 628\r\n",
      "[2019-08-17 03:16:30,546] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 505\r\n",
      "[2019-08-17 03:16:30,547] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 623\r\n",
      "[2019-08-17 03:16:30,547] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 600\r\n",
      "[2019-08-17 03:16:30,547] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  MemoryStore:54 - Block broadcast_25_piece0 stored as bytes in memory (estimated size 31.0 KB, free 365.2 MB)\r\n",
      "[2019-08-17 03:16:30,550] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  BlockManagerInfo:54 - Added broadcast_25_piece0 in memory on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 (size: 31.0 KB, free: 366.1 MB)\r\n",
      "[2019-08-17 03:16:30,551] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  BlockManagerInfo:54 - Removed broadcast_23_piece0 on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 in memory (size: 69.6 KB, free: 9.4 GB)\r\n",
      "[2019-08-17 03:16:30,552] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  SparkContext:54 - Created broadcast 25 from broadcast at DAGScheduler.scala:1161\r\n",
      "[2019-08-17 03:16:30,552] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 24 (MapPartitionsRDD[85] at map at DecisionTreeMetadata.scala:112) (first 15 tasks are for partitions Vector(0))\r\n",
      "[2019-08-17 03:16:30,553] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  YarnScheduler:54 - Adding task set 24.0 with 1 tasks\r\n",
      "[2019-08-17 03:16:30,554] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  BlockManagerInfo:54 - Removed broadcast_23_piece0 on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 in memory (size: 69.6 KB, free: 366.1 MB)\r\n",
      "[2019-08-17 03:16:30,556] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  TaskSetManager:54 - Starting task 0.0 in stage 24.0 (TID 24, ip-172-31-29-159.ap-northeast-2.compute.internal, executor 2, partition 0, NODE_LOCAL, 8385 bytes)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-08-17 03:16:30,560] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 637\n",
      "[2019-08-17 03:16:30,560] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 418\n",
      "[2019-08-17 03:16:30,560] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 609\n",
      "[2019-08-17 03:16:30,560] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 495\n",
      "[2019-08-17 03:16:30,560] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 531\n",
      "[2019-08-17 03:16:30,561] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 614\n",
      "[2019-08-17 03:16:30,561] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 425\n",
      "[2019-08-17 03:16:30,561] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 506\n",
      "[2019-08-17 03:16:30,561] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 565\n",
      "[2019-08-17 03:16:30,561] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 440\n",
      "[2019-08-17 03:16:30,561] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 599\n",
      "[2019-08-17 03:16:30,561] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 617\n",
      "[2019-08-17 03:16:30,561] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 469\n",
      "[2019-08-17 03:16:30,561] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 487\n",
      "[2019-08-17 03:16:30,561] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 461\n",
      "[2019-08-17 03:16:30,561] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 563\n",
      "[2019-08-17 03:16:30,561] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 404\n",
      "[2019-08-17 03:16:30,561] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 532\n",
      "[2019-08-17 03:16:30,561] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 644\n",
      "[2019-08-17 03:16:30,565] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 474\n",
      "[2019-08-17 03:16:30,566] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 407\n",
      "[2019-08-17 03:16:30,566] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 431\n",
      "[2019-08-17 03:16:30,566] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 520\n",
      "[2019-08-17 03:16:30,566] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 570\n",
      "[2019-08-17 03:16:30,566] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 445\n",
      "[2019-08-17 03:16:30,566] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 409\n",
      "[2019-08-17 03:16:30,568] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 598\n",
      "[2019-08-17 03:16:30,568] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 611\n",
      "[2019-08-17 03:16:30,569] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 459\n",
      "[2019-08-17 03:16:30,569] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 499\n",
      "[2019-08-17 03:16:30,570] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 446\n",
      "[2019-08-17 03:16:30,570] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 489\n",
      "[2019-08-17 03:16:30,570] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 550\n",
      "[2019-08-17 03:16:30,570] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 569\n",
      "[2019-08-17 03:16:30,570] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 509\n",
      "[2019-08-17 03:16:30,570] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 641\n",
      "[2019-08-17 03:16:30,570] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 587\n",
      "[2019-08-17 03:16:30,570] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 540\n",
      "[2019-08-17 03:16:30,570] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 556\n",
      "[2019-08-17 03:16:30,570] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  BlockManagerInfo:54 - Removed broadcast_18_piece0 on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 in memory (size: 69.6 KB, free: 9.4 GB)\n",
      "[2019-08-17 03:16:30,572] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  BlockManagerInfo:54 - Removed broadcast_18_piece0 on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 in memory (size: 69.6 KB, free: 366.2 MB)\n",
      "[2019-08-17 03:16:30,573] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  BlockManagerInfo:54 - Added broadcast_25_piece0 in memory on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 (size: 31.0 KB, free: 9.4 GB)\n",
      "[2019-08-17 03:16:30,574] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 477\n",
      "[2019-08-17 03:16:30,575] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 604\n",
      "[2019-08-17 03:16:30,575] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 643\n",
      "[2019-08-17 03:16:30,576] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  BlockManagerInfo:54 - Removed broadcast_16_piece0 on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 in memory (size: 30.3 KB, free: 366.2 MB)\n",
      "[2019-08-17 03:16:30,578] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  BlockManagerInfo:54 - Removed broadcast_16_piece0 on ip-172-31-18-11.ap-northeast-2.compute.internal:38933 in memory (size: 30.3 KB, free: 9.4 GB)\n",
      "[2019-08-17 03:16:30,580] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 521\n",
      "[2019-08-17 03:16:30,580] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 447\n",
      "[2019-08-17 03:16:30,580] {bash_operator.py:127} INFO - 2019-08-17 03:16:30 INFO  ContextCleaner:54 - Cleaned accumulator 413\n",
      "[2019-08-17 03:16:31,064] {bash_operator.py:127} INFO - 2019-08-17 03:16:31 INFO  TaskSetManager:54 - Finished task 0.0 in stage 24.0 (TID 24) in 509 ms on ip-172-31-29-159.ap-northeast-2.compute.internal (executor 2) (1/1)\n",
      "[2019-08-17 03:16:31,064] {bash_operator.py:127} INFO - 2019-08-17 03:16:31 INFO  YarnScheduler:54 - Removed TaskSet 24.0, whose tasks have all completed, from pool\n",
      "[2019-08-17 03:16:31,069] {bash_operator.py:127} INFO - 2019-08-17 03:16:31 INFO  DAGScheduler:54 - ResultStage 24 (take at DecisionTreeMetadata.scala:112) finished in 0.527 s\n",
      "[2019-08-17 03:16:31,075] {bash_operator.py:127} INFO - 2019-08-17 03:16:31 INFO  DAGScheduler:54 - Job 16 finished: take at DecisionTreeMetadata.scala:112, took 0.534153 s\n",
      "[2019-08-17 03:16:31,076] {bash_operator.py:127} INFO - 2019-08-17 03:16:31 INFO  SparkContext:54 - Starting job: count at DecisionTreeMetadata.scala:118\n",
      "[2019-08-17 03:16:31,076] {bash_operator.py:127} INFO - 2019-08-17 03:16:31 INFO  DAGScheduler:54 - Got job 17 (count at DecisionTreeMetadata.scala:118) with 1 output partitions\n",
      "[2019-08-17 03:16:31,076] {bash_operator.py:127} INFO - 2019-08-17 03:16:31 INFO  DAGScheduler:54 - Final stage: ResultStage 25 (count at DecisionTreeMetadata.scala:118)\n",
      "[2019-08-17 03:16:31,076] {bash_operator.py:127} INFO - 2019-08-17 03:16:31 INFO  DAGScheduler:54 - Parents of final stage: List()\n",
      "[2019-08-17 03:16:31,076] {bash_operator.py:127} INFO - 2019-08-17 03:16:31 INFO  DAGScheduler:54 - Missing parents: List()\n",
      "[2019-08-17 03:16:31,076] {bash_operator.py:127} INFO - 2019-08-17 03:16:31 INFO  DAGScheduler:54 - Submitting ResultStage 25 (MapPartitionsRDD[84] at retag at RandomForest.scala:104), which has no missing parents\n",
      "[2019-08-17 03:16:31,078] {bash_operator.py:127} INFO - 2019-08-17 03:16:31 INFO  MemoryStore:54 - Block broadcast_26 stored as values in memory (estimated size 74.0 KB, free 365.7 MB)\n",
      "[2019-08-17 03:16:31,080] {bash_operator.py:127} INFO - 2019-08-17 03:16:31 INFO  MemoryStore:54 - Block broadcast_26_piece0 stored as bytes in memory (estimated size 30.8 KB, free 365.7 MB)\n",
      "[2019-08-17 03:16:31,080] {bash_operator.py:127} INFO - 2019-08-17 03:16:31 INFO  BlockManagerInfo:54 - Added broadcast_26_piece0 in memory on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 (size: 30.8 KB, free: 366.2 MB)\n",
      "[2019-08-17 03:16:31,081] {bash_operator.py:127} INFO - 2019-08-17 03:16:31 INFO  SparkContext:54 - Created broadcast 26 from broadcast at DAGScheduler.scala:1161\n",
      "[2019-08-17 03:16:31,081] {bash_operator.py:127} INFO - 2019-08-17 03:16:31 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 25 (MapPartitionsRDD[84] at retag at RandomForest.scala:104) (first 15 tasks are for partitions Vector(0))\n",
      "[2019-08-17 03:16:31,082] {bash_operator.py:127} INFO - 2019-08-17 03:16:31 INFO  YarnScheduler:54 - Adding task set 25.0 with 1 tasks\n",
      "[2019-08-17 03:16:31,084] {bash_operator.py:127} INFO - 2019-08-17 03:16:31 INFO  TaskSetManager:54 - Starting task 0.0 in stage 25.0 (TID 25, ip-172-31-29-159.ap-northeast-2.compute.internal, executor 2, partition 0, NODE_LOCAL, 8385 bytes)\n",
      "[2019-08-17 03:16:31,094] {bash_operator.py:127} INFO - 2019-08-17 03:16:31 INFO  BlockManagerInfo:54 - Added broadcast_26_piece0 in memory on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 (size: 30.8 KB, free: 9.4 GB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-08-17 03:16:31,449] {bash_operator.py:127} INFO - 2019-08-17 03:16:31 INFO  TaskSetManager:54 - Finished task 0.0 in stage 25.0 (TID 25) in 365 ms on ip-172-31-29-159.ap-northeast-2.compute.internal (executor 2) (1/1)\n",
      "[2019-08-17 03:16:31,449] {bash_operator.py:127} INFO - 2019-08-17 03:16:31 INFO  YarnScheduler:54 - Removed TaskSet 25.0, whose tasks have all completed, from pool\n",
      "[2019-08-17 03:16:31,450] {bash_operator.py:127} INFO - 2019-08-17 03:16:31 INFO  DAGScheduler:54 - ResultStage 25 (count at DecisionTreeMetadata.scala:118) finished in 0.375 s\n",
      "[2019-08-17 03:16:31,451] {bash_operator.py:127} INFO - 2019-08-17 03:16:31 INFO  DAGScheduler:54 - Job 17 finished: count at DecisionTreeMetadata.scala:118, took 0.376958 s\n",
      "[2019-08-17 03:16:31,451] {bash_operator.py:127} INFO - 2019-08-17 03:16:31 WARN  DecisionTreeMetadata:66 - DecisionTree reducing maxBins from 4657 to 873 (= number of training instances)\n",
      "[2019-08-17 03:16:31,455] {bash_operator.py:127} INFO - 2019-08-17 03:16:31 INFO  Instrumentation:54 - [a07343fc] {\"numFeatures\":9}\n",
      "[2019-08-17 03:16:31,455] {bash_operator.py:127} INFO - 2019-08-17 03:16:31 INFO  Instrumentation:54 - [a07343fc] {\"numClasses\":4}\n",
      "[2019-08-17 03:16:31,455] {bash_operator.py:127} INFO - 2019-08-17 03:16:31 INFO  Instrumentation:54 - [a07343fc] {\"numExamples\":873}\n",
      "[2019-08-17 03:16:31,494] {bash_operator.py:127} INFO - 2019-08-17 03:16:31 INFO  SparkContext:54 - Starting job: collectAsMap at RandomForest.scala:927\n",
      "[2019-08-17 03:16:31,498] {bash_operator.py:127} INFO - 2019-08-17 03:16:31 INFO  DAGScheduler:54 - Registering RDD 87 (flatMap at RandomForest.scala:919)\n",
      "[2019-08-17 03:16:31,499] {bash_operator.py:127} INFO - 2019-08-17 03:16:31 INFO  DAGScheduler:54 - Got job 18 (collectAsMap at RandomForest.scala:927) with 1 output partitions\n",
      "[2019-08-17 03:16:31,499] {bash_operator.py:127} INFO - 2019-08-17 03:16:31 INFO  DAGScheduler:54 - Final stage: ResultStage 27 (collectAsMap at RandomForest.scala:927)\n",
      "[2019-08-17 03:16:31,499] {bash_operator.py:127} INFO - 2019-08-17 03:16:31 INFO  DAGScheduler:54 - Parents of final stage: List(ShuffleMapStage 26)\n",
      "[2019-08-17 03:16:31,499] {bash_operator.py:127} INFO - 2019-08-17 03:16:31 INFO  DAGScheduler:54 - Missing parents: List(ShuffleMapStage 26)\n",
      "[2019-08-17 03:16:31,499] {bash_operator.py:127} INFO - 2019-08-17 03:16:31 INFO  DAGScheduler:54 - Submitting ShuffleMapStage 26 (MapPartitionsRDD[87] at flatMap at RandomForest.scala:919), which has no missing parents\n",
      "[2019-08-17 03:16:31,499] {bash_operator.py:127} INFO - 2019-08-17 03:16:31 INFO  MemoryStore:54 - Block broadcast_27 stored as values in memory (estimated size 77.2 KB, free 365.6 MB)\n",
      "[2019-08-17 03:16:31,500] {bash_operator.py:127} INFO - 2019-08-17 03:16:31 INFO  MemoryStore:54 - Block broadcast_27_piece0 stored as bytes in memory (estimated size 32.4 KB, free 365.6 MB)\n",
      "[2019-08-17 03:16:31,501] {bash_operator.py:127} INFO - 2019-08-17 03:16:31 INFO  BlockManagerInfo:54 - Added broadcast_27_piece0 in memory on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 (size: 32.4 KB, free: 366.2 MB)\n",
      "[2019-08-17 03:16:31,501] {bash_operator.py:127} INFO - 2019-08-17 03:16:31 INFO  SparkContext:54 - Created broadcast 27 from broadcast at DAGScheduler.scala:1161\n",
      "[2019-08-17 03:16:31,507] {bash_operator.py:127} INFO - 2019-08-17 03:16:31 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ShuffleMapStage 26 (MapPartitionsRDD[87] at flatMap at RandomForest.scala:919) (first 15 tasks are for partitions Vector(0))\n",
      "[2019-08-17 03:16:31,507] {bash_operator.py:127} INFO - 2019-08-17 03:16:31 INFO  YarnScheduler:54 - Adding task set 26.0 with 1 tasks\n",
      "[2019-08-17 03:16:31,508] {bash_operator.py:127} INFO - 2019-08-17 03:16:31 INFO  TaskSetManager:54 - Starting task 0.0 in stage 26.0 (TID 26, ip-172-31-29-159.ap-northeast-2.compute.internal, executor 2, partition 0, NODE_LOCAL, 8483 bytes)\n",
      "[2019-08-17 03:16:31,522] {bash_operator.py:127} INFO - 2019-08-17 03:16:31 INFO  BlockManagerInfo:54 - Added broadcast_27_piece0 in memory on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 (size: 32.4 KB, free: 9.4 GB)\n",
      "[2019-08-17 03:16:31,880] {bash_operator.py:127} INFO - 2019-08-17 03:16:31 INFO  TaskSetManager:54 - Finished task 0.0 in stage 26.0 (TID 26) in 373 ms on ip-172-31-29-159.ap-northeast-2.compute.internal (executor 2) (1/1)\n",
      "[2019-08-17 03:16:31,881] {bash_operator.py:127} INFO - 2019-08-17 03:16:31 INFO  YarnScheduler:54 - Removed TaskSet 26.0, whose tasks have all completed, from pool\n",
      "[2019-08-17 03:16:31,882] {bash_operator.py:127} INFO - 2019-08-17 03:16:31 INFO  DAGScheduler:54 - ShuffleMapStage 26 (flatMap at RandomForest.scala:919) finished in 0.386 s\n",
      "[2019-08-17 03:16:31,882] {bash_operator.py:127} INFO - 2019-08-17 03:16:31 INFO  DAGScheduler:54 - looking for newly runnable stages\n",
      "[2019-08-17 03:16:31,882] {bash_operator.py:127} INFO - 2019-08-17 03:16:31 INFO  DAGScheduler:54 - running: Set()\n",
      "[2019-08-17 03:16:31,883] {bash_operator.py:127} INFO - 2019-08-17 03:16:31 INFO  DAGScheduler:54 - waiting: Set(ResultStage 27)\n",
      "[2019-08-17 03:16:31,883] {bash_operator.py:127} INFO - 2019-08-17 03:16:31 INFO  DAGScheduler:54 - failed: Set()\n",
      "[2019-08-17 03:16:31,883] {bash_operator.py:127} INFO - 2019-08-17 03:16:31 INFO  DAGScheduler:54 - Submitting ResultStage 27 (MapPartitionsRDD[89] at map at RandomForest.scala:922), which has no missing parents\n",
      "[2019-08-17 03:16:31,887] {bash_operator.py:127} INFO - 2019-08-17 03:16:31 INFO  MemoryStore:54 - Block broadcast_28 stored as values in memory (estimated size 79.8 KB, free 365.5 MB)\n",
      "[2019-08-17 03:16:31,890] {bash_operator.py:127} INFO - 2019-08-17 03:16:31 INFO  MemoryStore:54 - Block broadcast_28_piece0 stored as bytes in memory (estimated size 33.6 KB, free 365.5 MB)\n",
      "[2019-08-17 03:16:31,890] {bash_operator.py:127} INFO - 2019-08-17 03:16:31 INFO  BlockManagerInfo:54 - Added broadcast_28_piece0 in memory on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 (size: 33.6 KB, free: 366.1 MB)\n",
      "[2019-08-17 03:16:31,891] {bash_operator.py:127} INFO - 2019-08-17 03:16:31 INFO  SparkContext:54 - Created broadcast 28 from broadcast at DAGScheduler.scala:1161\n",
      "[2019-08-17 03:16:31,892] {bash_operator.py:127} INFO - 2019-08-17 03:16:31 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 27 (MapPartitionsRDD[89] at map at RandomForest.scala:922) (first 15 tasks are for partitions Vector(0))\n",
      "[2019-08-17 03:16:31,892] {bash_operator.py:127} INFO - 2019-08-17 03:16:31 INFO  YarnScheduler:54 - Adding task set 27.0 with 1 tasks\n",
      "[2019-08-17 03:16:31,893] {bash_operator.py:127} INFO - 2019-08-17 03:16:31 INFO  TaskSetManager:54 - Starting task 0.0 in stage 27.0 (TID 27, ip-172-31-29-159.ap-northeast-2.compute.internal, executor 2, partition 0, NODE_LOCAL, 7673 bytes)\n",
      "[2019-08-17 03:16:31,904] {bash_operator.py:127} INFO - 2019-08-17 03:16:31 INFO  BlockManagerInfo:54 - Added broadcast_28_piece0 in memory on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 (size: 33.6 KB, free: 9.4 GB)\n",
      "[2019-08-17 03:16:31,923] {bash_operator.py:127} INFO - 2019-08-17 03:16:31 INFO  MapOutputTrackerMasterEndpoint:54 - Asked to send map output locations for shuffle 8 to 172.31.29.159:58394\n",
      "[2019-08-17 03:16:31,994] {bash_operator.py:127} INFO - 2019-08-17 03:16:31 INFO  TaskSetManager:54 - Finished task 0.0 in stage 27.0 (TID 27) in 101 ms on ip-172-31-29-159.ap-northeast-2.compute.internal (executor 2) (1/1)\n",
      "[2019-08-17 03:16:31,995] {bash_operator.py:127} INFO - 2019-08-17 03:16:31 INFO  YarnScheduler:54 - Removed TaskSet 27.0, whose tasks have all completed, from pool\n",
      "[2019-08-17 03:16:31,995] {bash_operator.py:127} INFO - 2019-08-17 03:16:31 INFO  DAGScheduler:54 - ResultStage 27 (collectAsMap at RandomForest.scala:927) finished in 0.111 s\n",
      "[2019-08-17 03:16:31,995] {bash_operator.py:127} INFO - 2019-08-17 03:16:31 INFO  DAGScheduler:54 - Job 18 finished: collectAsMap at RandomForest.scala:927, took 0.501483 s\n",
      "[2019-08-17 03:16:32,022] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  MemoryStore:54 - Block broadcast_29 stored as values in memory (estimated size 1160.0 B, free 365.5 MB)\n",
      "[2019-08-17 03:16:32,029] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  MemoryStore:54 - Block broadcast_29_piece0 stored as bytes in memory (estimated size 394.0 B, free 365.5 MB)\n",
      "[2019-08-17 03:16:32,030] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  BlockManagerInfo:54 - Added broadcast_29_piece0 in memory on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 (size: 394.0 B, free: 366.1 MB)\n",
      "[2019-08-17 03:16:32,030] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  SparkContext:54 - Created broadcast 29 from broadcast at RandomForest.scala:517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-08-17 03:16:32,059] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  SparkContext:54 - Starting job: collectAsMap at RandomForest.scala:567\n",
      "[2019-08-17 03:16:32,060] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  DAGScheduler:54 - Registering RDD 92 (mapPartitions at RandomForest.scala:538)\n",
      "[2019-08-17 03:16:32,061] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  DAGScheduler:54 - Got job 19 (collectAsMap at RandomForest.scala:567) with 1 output partitions\n",
      "[2019-08-17 03:16:32,062] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  DAGScheduler:54 - Final stage: ResultStage 29 (collectAsMap at RandomForest.scala:567)\n",
      "[2019-08-17 03:16:32,062] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  DAGScheduler:54 - Parents of final stage: List(ShuffleMapStage 28)\n",
      "[2019-08-17 03:16:32,062] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  DAGScheduler:54 - Missing parents: List(ShuffleMapStage 28)\n",
      "[2019-08-17 03:16:32,063] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  DAGScheduler:54 - Submitting ShuffleMapStage 28 (MapPartitionsRDD[92] at mapPartitions at RandomForest.scala:538), which has no missing parents\n",
      "[2019-08-17 03:16:32,068] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  MemoryStore:54 - Block broadcast_30 stored as values in memory (estimated size 83.9 KB, free 365.4 MB)\n",
      "[2019-08-17 03:16:32,079] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  MemoryStore:54 - Block broadcast_30_piece0 stored as bytes in memory (estimated size 35.1 KB, free 365.4 MB)\n",
      "[2019-08-17 03:16:32,080] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  BlockManagerInfo:54 - Added broadcast_30_piece0 in memory on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 (size: 35.1 KB, free: 366.1 MB)\n",
      "[2019-08-17 03:16:32,080] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  SparkContext:54 - Created broadcast 30 from broadcast at DAGScheduler.scala:1161\n",
      "[2019-08-17 03:16:32,081] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ShuffleMapStage 28 (MapPartitionsRDD[92] at mapPartitions at RandomForest.scala:538) (first 15 tasks are for partitions Vector(0))\n",
      "[2019-08-17 03:16:32,081] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  YarnScheduler:54 - Adding task set 28.0 with 1 tasks\n",
      "[2019-08-17 03:16:32,082] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  TaskSetManager:54 - Starting task 0.0 in stage 28.0 (TID 28, ip-172-31-29-159.ap-northeast-2.compute.internal, executor 2, partition 0, NODE_LOCAL, 8374 bytes)\n",
      "[2019-08-17 03:16:32,099] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  BlockManagerInfo:54 - Added broadcast_30_piece0 in memory on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 (size: 35.1 KB, free: 9.4 GB)\n",
      "[2019-08-17 03:16:32,462] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  BlockManagerInfo:54 - Added rdd_91_0 in memory on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 (size: 159.7 KB, free: 9.4 GB)\n",
      "[2019-08-17 03:16:32,486] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  BlockManagerInfo:54 - Added broadcast_29_piece0 in memory on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 (size: 394.0 B, free: 9.4 GB)\n",
      "[2019-08-17 03:16:32,556] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  TaskSetManager:54 - Finished task 0.0 in stage 28.0 (TID 28) in 475 ms on ip-172-31-29-159.ap-northeast-2.compute.internal (executor 2) (1/1)\n",
      "[2019-08-17 03:16:32,556] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  YarnScheduler:54 - Removed TaskSet 28.0, whose tasks have all completed, from pool\n",
      "[2019-08-17 03:16:32,557] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  DAGScheduler:54 - ShuffleMapStage 28 (mapPartitions at RandomForest.scala:538) finished in 0.493 s\n",
      "[2019-08-17 03:16:32,557] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  DAGScheduler:54 - looking for newly runnable stages\n",
      "[2019-08-17 03:16:32,557] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  DAGScheduler:54 - running: Set()\n",
      "[2019-08-17 03:16:32,557] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  DAGScheduler:54 - waiting: Set(ResultStage 29)\n",
      "[2019-08-17 03:16:32,557] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  DAGScheduler:54 - failed: Set()\n",
      "[2019-08-17 03:16:32,558] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  DAGScheduler:54 - Submitting ResultStage 29 (MapPartitionsRDD[94] at map at RandomForest.scala:557), which has no missing parents\n",
      "[2019-08-17 03:16:32,559] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  MemoryStore:54 - Block broadcast_31 stored as values in memory (estimated size 8.6 KB, free 365.4 MB)\n",
      "[2019-08-17 03:16:32,560] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  MemoryStore:54 - Block broadcast_31_piece0 stored as bytes in memory (estimated size 3.7 KB, free 365.4 MB)\n",
      "[2019-08-17 03:16:32,561] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  BlockManagerInfo:54 - Added broadcast_31_piece0 in memory on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 (size: 3.7 KB, free: 366.1 MB)\n",
      "[2019-08-17 03:16:32,564] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  SparkContext:54 - Created broadcast 31 from broadcast at DAGScheduler.scala:1161\n",
      "[2019-08-17 03:16:32,565] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 29 (MapPartitionsRDD[94] at map at RandomForest.scala:557) (first 15 tasks are for partitions Vector(0))\n",
      "[2019-08-17 03:16:32,565] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  YarnScheduler:54 - Adding task set 29.0 with 1 tasks\n",
      "[2019-08-17 03:16:32,566] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  TaskSetManager:54 - Starting task 0.0 in stage 29.0 (TID 29, ip-172-31-29-159.ap-northeast-2.compute.internal, executor 2, partition 0, NODE_LOCAL, 7673 bytes)\n",
      "[2019-08-17 03:16:32,579] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  BlockManagerInfo:54 - Added broadcast_31_piece0 in memory on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 (size: 3.7 KB, free: 9.4 GB)\n",
      "[2019-08-17 03:16:32,586] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  MapOutputTrackerMasterEndpoint:54 - Asked to send map output locations for shuffle 9 to 172.31.29.159:58394\n",
      "[2019-08-17 03:16:32,727] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  TaskSetManager:54 - Finished task 0.0 in stage 29.0 (TID 29) in 161 ms on ip-172-31-29-159.ap-northeast-2.compute.internal (executor 2) (1/1)\n",
      "[2019-08-17 03:16:32,728] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  DAGScheduler:54 - ResultStage 29 (collectAsMap at RandomForest.scala:567) finished in 0.169 s\n",
      "[2019-08-17 03:16:32,730] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  DAGScheduler:54 - Job 19 finished: collectAsMap at RandomForest.scala:567, took 0.669268 s\n",
      "[2019-08-17 03:16:32,734] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  MemoryStore:54 - Block broadcast_32 stored as values in memory (estimated size 1592.0 B, free 365.3 MB)\n",
      "[2019-08-17 03:16:32,736] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  MemoryStore:54 - Block broadcast_32_piece0 stored as bytes in memory (estimated size 440.0 B, free 365.3 MB)\n",
      "[2019-08-17 03:16:32,738] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  YarnScheduler:54 - Removed TaskSet 29.0, whose tasks have all completed, from pool\n",
      "[2019-08-17 03:16:32,739] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  BlockManagerInfo:54 - Added broadcast_32_piece0 in memory on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 (size: 440.0 B, free: 366.1 MB)\n",
      "[2019-08-17 03:16:32,739] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  SparkContext:54 - Created broadcast 32 from broadcast at RandomForest.scala:517\n",
      "[2019-08-17 03:16:32,750] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  SparkContext:54 - Starting job: collectAsMap at RandomForest.scala:567\n",
      "[2019-08-17 03:16:32,754] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  DAGScheduler:54 - Registering RDD 95 (mapPartitions at RandomForest.scala:538)\n",
      "[2019-08-17 03:16:32,755] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  DAGScheduler:54 - Got job 20 (collectAsMap at RandomForest.scala:567) with 1 output partitions\n",
      "[2019-08-17 03:16:32,756] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  DAGScheduler:54 - Final stage: ResultStage 31 (collectAsMap at RandomForest.scala:567)\n",
      "[2019-08-17 03:16:32,756] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  DAGScheduler:54 - Parents of final stage: List(ShuffleMapStage 30)\n",
      "[2019-08-17 03:16:32,756] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  DAGScheduler:54 - Missing parents: List(ShuffleMapStage 30)\n",
      "[2019-08-17 03:16:32,758] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  DAGScheduler:54 - Submitting ShuffleMapStage 30 (MapPartitionsRDD[95] at mapPartitions at RandomForest.scala:538), which has no missing parents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-08-17 03:16:32,764] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  MemoryStore:54 - Block broadcast_33 stored as values in memory (estimated size 87.6 KB, free 365.3 MB)\n",
      "[2019-08-17 03:16:32,771] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  MemoryStore:54 - Block broadcast_33_piece0 stored as bytes in memory (estimated size 36.5 KB, free 365.2 MB)\n",
      "[2019-08-17 03:16:32,772] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  BlockManagerInfo:54 - Added broadcast_33_piece0 in memory on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 (size: 36.5 KB, free: 366.1 MB)\n",
      "[2019-08-17 03:16:32,773] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  SparkContext:54 - Created broadcast 33 from broadcast at DAGScheduler.scala:1161\n",
      "[2019-08-17 03:16:32,774] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ShuffleMapStage 30 (MapPartitionsRDD[95] at mapPartitions at RandomForest.scala:538) (first 15 tasks are for partitions Vector(0))\n",
      "[2019-08-17 03:16:32,774] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  YarnScheduler:54 - Adding task set 30.0 with 1 tasks\n",
      "[2019-08-17 03:16:32,776] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  TaskSetManager:54 - Starting task 0.0 in stage 30.0 (TID 30, ip-172-31-29-159.ap-northeast-2.compute.internal, executor 2, partition 0, PROCESS_LOCAL, 8374 bytes)\n",
      "[2019-08-17 03:16:32,787] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  BlockManagerInfo:54 - Added broadcast_33_piece0 in memory on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 (size: 36.5 KB, free: 9.4 GB)\n",
      "[2019-08-17 03:16:32,804] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  BlockManagerInfo:54 - Added broadcast_32_piece0 in memory on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 (size: 440.0 B, free: 9.4 GB)\n",
      "[2019-08-17 03:16:32,831] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  TaskSetManager:54 - Finished task 0.0 in stage 30.0 (TID 30) in 55 ms on ip-172-31-29-159.ap-northeast-2.compute.internal (executor 2) (1/1)\n",
      "[2019-08-17 03:16:32,831] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  YarnScheduler:54 - Removed TaskSet 30.0, whose tasks have all completed, from pool\n",
      "[2019-08-17 03:16:32,833] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  DAGScheduler:54 - ShuffleMapStage 30 (mapPartitions at RandomForest.scala:538) finished in 0.075 s\n",
      "[2019-08-17 03:16:32,833] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  DAGScheduler:54 - looking for newly runnable stages\n",
      "[2019-08-17 03:16:32,834] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  DAGScheduler:54 - running: Set()\n",
      "[2019-08-17 03:16:32,834] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  DAGScheduler:54 - waiting: Set(ResultStage 31)\n",
      "[2019-08-17 03:16:32,834] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  DAGScheduler:54 - failed: Set()\n",
      "[2019-08-17 03:16:32,835] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  DAGScheduler:54 - Submitting ResultStage 31 (MapPartitionsRDD[97] at map at RandomForest.scala:557), which has no missing parents\n",
      "[2019-08-17 03:16:32,836] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  MemoryStore:54 - Block broadcast_34 stored as values in memory (estimated size 10.0 KB, free 365.2 MB)\n",
      "[2019-08-17 03:16:32,838] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  MemoryStore:54 - Block broadcast_34_piece0 stored as bytes in memory (estimated size 4.4 KB, free 365.2 MB)\n",
      "[2019-08-17 03:16:32,839] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  BlockManagerInfo:54 - Added broadcast_34_piece0 in memory on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 (size: 4.4 KB, free: 366.1 MB)\n",
      "[2019-08-17 03:16:32,844] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  SparkContext:54 - Created broadcast 34 from broadcast at DAGScheduler.scala:1161\n",
      "[2019-08-17 03:16:32,845] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 31 (MapPartitionsRDD[97] at map at RandomForest.scala:557) (first 15 tasks are for partitions Vector(0))\n",
      "[2019-08-17 03:16:32,845] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  YarnScheduler:54 - Adding task set 31.0 with 1 tasks\n",
      "[2019-08-17 03:16:32,847] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  TaskSetManager:54 - Starting task 0.0 in stage 31.0 (TID 31, ip-172-31-29-159.ap-northeast-2.compute.internal, executor 2, partition 0, NODE_LOCAL, 7673 bytes)\n",
      "[2019-08-17 03:16:32,863] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  BlockManagerInfo:54 - Added broadcast_34_piece0 in memory on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 (size: 4.4 KB, free: 9.4 GB)\n",
      "[2019-08-17 03:16:32,869] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  MapOutputTrackerMasterEndpoint:54 - Asked to send map output locations for shuffle 10 to 172.31.29.159:58394\n",
      "[2019-08-17 03:16:32,926] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  TaskSetManager:54 - Finished task 0.0 in stage 31.0 (TID 31) in 79 ms on ip-172-31-29-159.ap-northeast-2.compute.internal (executor 2) (1/1)\n",
      "[2019-08-17 03:16:32,926] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  DAGScheduler:54 - ResultStage 31 (collectAsMap at RandomForest.scala:567) finished in 0.091 s\n",
      "[2019-08-17 03:16:32,927] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  DAGScheduler:54 - Job 20 finished: collectAsMap at RandomForest.scala:567, took 0.174469 s\n",
      "[2019-08-17 03:16:32,930] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  MemoryStore:54 - Block broadcast_35 stored as values in memory (estimated size 2.3 KB, free 365.2 MB)\n",
      "[2019-08-17 03:16:32,931] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  YarnScheduler:54 - Removed TaskSet 31.0, whose tasks have all completed, from pool\n",
      "[2019-08-17 03:16:32,931] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  MemoryStore:54 - Block broadcast_35_piece0 stored as bytes in memory (estimated size 510.0 B, free 365.2 MB)\n",
      "[2019-08-17 03:16:32,932] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  BlockManagerInfo:54 - Added broadcast_35_piece0 in memory on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 (size: 510.0 B, free: 366.1 MB)\n",
      "[2019-08-17 03:16:32,932] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  SparkContext:54 - Created broadcast 35 from broadcast at RandomForest.scala:517\n",
      "[2019-08-17 03:16:32,948] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  SparkContext:54 - Starting job: collectAsMap at RandomForest.scala:567\n",
      "[2019-08-17 03:16:32,950] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  DAGScheduler:54 - Registering RDD 98 (mapPartitions at RandomForest.scala:538)\n",
      "[2019-08-17 03:16:32,952] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  DAGScheduler:54 - Got job 21 (collectAsMap at RandomForest.scala:567) with 1 output partitions\n",
      "[2019-08-17 03:16:32,952] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  DAGScheduler:54 - Final stage: ResultStage 33 (collectAsMap at RandomForest.scala:567)\n",
      "[2019-08-17 03:16:32,952] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  DAGScheduler:54 - Parents of final stage: List(ShuffleMapStage 32)\n",
      "[2019-08-17 03:16:32,952] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  DAGScheduler:54 - Missing parents: List(ShuffleMapStage 32)\n",
      "[2019-08-17 03:16:32,952] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  DAGScheduler:54 - Submitting ShuffleMapStage 32 (MapPartitionsRDD[98] at mapPartitions at RandomForest.scala:538), which has no missing parents\n",
      "[2019-08-17 03:16:32,955] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  MemoryStore:54 - Block broadcast_36 stored as values in memory (estimated size 92.2 KB, free 365.1 MB)\n",
      "[2019-08-17 03:16:32,955] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  MemoryStore:54 - Block broadcast_36_piece0 stored as bytes in memory (estimated size 37.9 KB, free 365.1 MB)\n",
      "[2019-08-17 03:16:32,955] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  BlockManagerInfo:54 - Added broadcast_36_piece0 in memory on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 (size: 37.9 KB, free: 366.0 MB)\n",
      "[2019-08-17 03:16:32,956] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  SparkContext:54 - Created broadcast 36 from broadcast at DAGScheduler.scala:1161\n",
      "[2019-08-17 03:16:32,957] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ShuffleMapStage 32 (MapPartitionsRDD[98] at mapPartitions at RandomForest.scala:538) (first 15 tasks are for partitions Vector(0))\n",
      "[2019-08-17 03:16:32,957] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  YarnScheduler:54 - Adding task set 32.0 with 1 tasks\n",
      "[2019-08-17 03:16:32,959] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  TaskSetManager:54 - Starting task 0.0 in stage 32.0 (TID 32, ip-172-31-29-159.ap-northeast-2.compute.internal, executor 2, partition 0, PROCESS_LOCAL, 8374 bytes)\n",
      "[2019-08-17 03:16:32,970] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  BlockManagerInfo:54 - Added broadcast_36_piece0 in memory on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 (size: 37.9 KB, free: 9.4 GB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-08-17 03:16:32,992] {bash_operator.py:127} INFO - 2019-08-17 03:16:32 INFO  BlockManagerInfo:54 - Added broadcast_35_piece0 in memory on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 (size: 510.0 B, free: 9.4 GB)\n",
      "[2019-08-17 03:16:33,016] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  TaskSetManager:54 - Finished task 0.0 in stage 32.0 (TID 32) in 57 ms on ip-172-31-29-159.ap-northeast-2.compute.internal (executor 2) (1/1)\n",
      "[2019-08-17 03:16:33,016] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  YarnScheduler:54 - Removed TaskSet 32.0, whose tasks have all completed, from pool\n",
      "[2019-08-17 03:16:33,016] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  DAGScheduler:54 - ShuffleMapStage 32 (mapPartitions at RandomForest.scala:538) finished in 0.066 s\n",
      "[2019-08-17 03:16:33,017] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  DAGScheduler:54 - looking for newly runnable stages\n",
      "[2019-08-17 03:16:33,017] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  DAGScheduler:54 - running: Set()\n",
      "[2019-08-17 03:16:33,017] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  DAGScheduler:54 - waiting: Set(ResultStage 33)\n",
      "[2019-08-17 03:16:33,017] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  DAGScheduler:54 - failed: Set()\n",
      "[2019-08-17 03:16:33,017] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  DAGScheduler:54 - Submitting ResultStage 33 (MapPartitionsRDD[100] at map at RandomForest.scala:557), which has no missing parents\n",
      "[2019-08-17 03:16:33,018] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  MemoryStore:54 - Block broadcast_37 stored as values in memory (estimated size 10.7 KB, free 365.1 MB)\n",
      "[2019-08-17 03:16:33,020] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  MemoryStore:54 - Block broadcast_37_piece0 stored as bytes in memory (estimated size 4.6 KB, free 365.1 MB)\n",
      "[2019-08-17 03:16:33,021] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  BlockManagerInfo:54 - Added broadcast_37_piece0 in memory on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 (size: 4.6 KB, free: 366.0 MB)\n",
      "[2019-08-17 03:16:33,021] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  SparkContext:54 - Created broadcast 37 from broadcast at DAGScheduler.scala:1161\n",
      "[2019-08-17 03:16:33,021] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 33 (MapPartitionsRDD[100] at map at RandomForest.scala:557) (first 15 tasks are for partitions Vector(0))\n",
      "[2019-08-17 03:16:33,021] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  YarnScheduler:54 - Adding task set 33.0 with 1 tasks\n",
      "[2019-08-17 03:16:33,022] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  TaskSetManager:54 - Starting task 0.0 in stage 33.0 (TID 33, ip-172-31-29-159.ap-northeast-2.compute.internal, executor 2, partition 0, NODE_LOCAL, 7673 bytes)\n",
      "[2019-08-17 03:16:33,033] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  BlockManagerInfo:54 - Added broadcast_37_piece0 in memory on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 (size: 4.6 KB, free: 9.4 GB)\n",
      "[2019-08-17 03:16:33,038] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  MapOutputTrackerMasterEndpoint:54 - Asked to send map output locations for shuffle 11 to 172.31.29.159:58394\n",
      "[2019-08-17 03:16:33,112] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  TaskSetManager:54 - Finished task 0.0 in stage 33.0 (TID 33) in 90 ms on ip-172-31-29-159.ap-northeast-2.compute.internal (executor 2) (1/1)\n",
      "[2019-08-17 03:16:33,112] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  YarnScheduler:54 - Removed TaskSet 33.0, whose tasks have all completed, from pool\n",
      "[2019-08-17 03:16:33,113] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  DAGScheduler:54 - ResultStage 33 (collectAsMap at RandomForest.scala:567) finished in 0.096 s\n",
      "[2019-08-17 03:16:33,115] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  DAGScheduler:54 - Job 21 finished: collectAsMap at RandomForest.scala:567, took 0.165219 s\n",
      "[2019-08-17 03:16:33,117] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  MemoryStore:54 - Block broadcast_38 stored as values in memory (estimated size 3.6 KB, free 365.1 MB)\n",
      "[2019-08-17 03:16:33,119] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  MemoryStore:54 - Block broadcast_38_piece0 stored as bytes in memory (estimated size 612.0 B, free 365.1 MB)\n",
      "[2019-08-17 03:16:33,120] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  BlockManagerInfo:54 - Added broadcast_38_piece0 in memory on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 (size: 612.0 B, free: 366.0 MB)\n",
      "[2019-08-17 03:16:33,120] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  SparkContext:54 - Created broadcast 38 from broadcast at RandomForest.scala:517\n",
      "[2019-08-17 03:16:33,133] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  SparkContext:54 - Starting job: collectAsMap at RandomForest.scala:567\n",
      "[2019-08-17 03:16:33,135] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  DAGScheduler:54 - Registering RDD 101 (mapPartitions at RandomForest.scala:538)\n",
      "[2019-08-17 03:16:33,135] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  DAGScheduler:54 - Got job 22 (collectAsMap at RandomForest.scala:567) with 1 output partitions\n",
      "[2019-08-17 03:16:33,135] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  DAGScheduler:54 - Final stage: ResultStage 35 (collectAsMap at RandomForest.scala:567)\n",
      "[2019-08-17 03:16:33,135] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  DAGScheduler:54 - Parents of final stage: List(ShuffleMapStage 34)\n",
      "[2019-08-17 03:16:33,136] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  DAGScheduler:54 - Missing parents: List(ShuffleMapStage 34)\n",
      "[2019-08-17 03:16:33,136] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  DAGScheduler:54 - Submitting ShuffleMapStage 34 (MapPartitionsRDD[101] at mapPartitions at RandomForest.scala:538), which has no missing parents\n",
      "[2019-08-17 03:16:33,139] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  MemoryStore:54 - Block broadcast_39 stored as values in memory (estimated size 100.1 KB, free 365.0 MB)\n",
      "[2019-08-17 03:16:33,141] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  MemoryStore:54 - Block broadcast_39_piece0 stored as bytes in memory (estimated size 40.5 KB, free 364.9 MB)\n",
      "[2019-08-17 03:16:33,141] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  BlockManagerInfo:54 - Added broadcast_39_piece0 in memory on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 (size: 40.5 KB, free: 366.0 MB)\n",
      "[2019-08-17 03:16:33,142] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  SparkContext:54 - Created broadcast 39 from broadcast at DAGScheduler.scala:1161\n",
      "[2019-08-17 03:16:33,142] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ShuffleMapStage 34 (MapPartitionsRDD[101] at mapPartitions at RandomForest.scala:538) (first 15 tasks are for partitions Vector(0))\n",
      "[2019-08-17 03:16:33,143] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  YarnScheduler:54 - Adding task set 34.0 with 1 tasks\n",
      "[2019-08-17 03:16:33,144] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  TaskSetManager:54 - Starting task 0.0 in stage 34.0 (TID 34, ip-172-31-29-159.ap-northeast-2.compute.internal, executor 2, partition 0, PROCESS_LOCAL, 8374 bytes)\n",
      "[2019-08-17 03:16:33,158] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  BlockManagerInfo:54 - Added broadcast_39_piece0 in memory on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 (size: 40.5 KB, free: 9.4 GB)\n",
      "[2019-08-17 03:16:33,181] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  BlockManagerInfo:54 - Added broadcast_38_piece0 in memory on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 (size: 612.0 B, free: 9.4 GB)\n",
      "[2019-08-17 03:16:33,226] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  TaskSetManager:54 - Finished task 0.0 in stage 34.0 (TID 34) in 83 ms on ip-172-31-29-159.ap-northeast-2.compute.internal (executor 2) (1/1)\n",
      "[2019-08-17 03:16:33,226] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  YarnScheduler:54 - Removed TaskSet 34.0, whose tasks have all completed, from pool\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-08-17 03:16:33,227] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  DAGScheduler:54 - ShuffleMapStage 34 (mapPartitions at RandomForest.scala:538) finished in 0.091 s\n",
      "[2019-08-17 03:16:33,228] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  DAGScheduler:54 - looking for newly runnable stages\n",
      "[2019-08-17 03:16:33,228] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  DAGScheduler:54 - running: Set()\n",
      "[2019-08-17 03:16:33,228] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  DAGScheduler:54 - waiting: Set(ResultStage 35)\n",
      "[2019-08-17 03:16:33,228] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  DAGScheduler:54 - failed: Set()\n",
      "[2019-08-17 03:16:33,228] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  DAGScheduler:54 - Submitting ResultStage 35 (MapPartitionsRDD[103] at map at RandomForest.scala:557), which has no missing parents\n",
      "[2019-08-17 03:16:33,231] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  MemoryStore:54 - Block broadcast_40 stored as values in memory (estimated size 11.7 KB, free 364.9 MB)\n",
      "[2019-08-17 03:16:33,232] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  MemoryStore:54 - Block broadcast_40_piece0 stored as bytes in memory (estimated size 4.8 KB, free 364.9 MB)\n",
      "[2019-08-17 03:16:33,233] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  BlockManagerInfo:54 - Added broadcast_40_piece0 in memory on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 (size: 4.8 KB, free: 366.0 MB)\n",
      "[2019-08-17 03:16:33,233] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  SparkContext:54 - Created broadcast 40 from broadcast at DAGScheduler.scala:1161\n",
      "[2019-08-17 03:16:33,234] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 35 (MapPartitionsRDD[103] at map at RandomForest.scala:557) (first 15 tasks are for partitions Vector(0))\n",
      "[2019-08-17 03:16:33,234] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  YarnScheduler:54 - Adding task set 35.0 with 1 tasks\n",
      "[2019-08-17 03:16:33,235] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  TaskSetManager:54 - Starting task 0.0 in stage 35.0 (TID 35, ip-172-31-29-159.ap-northeast-2.compute.internal, executor 2, partition 0, NODE_LOCAL, 7673 bytes)\n",
      "[2019-08-17 03:16:33,246] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  BlockManagerInfo:54 - Added broadcast_40_piece0 in memory on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 (size: 4.8 KB, free: 9.4 GB)\n",
      "[2019-08-17 03:16:33,251] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  MapOutputTrackerMasterEndpoint:54 - Asked to send map output locations for shuffle 12 to 172.31.29.159:58394\n",
      "[2019-08-17 03:16:33,378] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  TaskSetManager:54 - Finished task 0.0 in stage 35.0 (TID 35) in 139 ms on ip-172-31-29-159.ap-northeast-2.compute.internal (executor 2) (1/1)\n",
      "[2019-08-17 03:16:33,379] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  YarnScheduler:54 - Removed TaskSet 35.0, whose tasks have all completed, from pool\n",
      "[2019-08-17 03:16:33,379] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  DAGScheduler:54 - ResultStage 35 (collectAsMap at RandomForest.scala:567) finished in 0.146 s\n",
      "[2019-08-17 03:16:33,379] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  DAGScheduler:54 - Job 22 finished: collectAsMap at RandomForest.scala:567, took 0.241644 s\n",
      "[2019-08-17 03:16:33,380] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  MemoryStore:54 - Block broadcast_41 stored as values in memory (estimated size 4.7 KB, free 364.9 MB)\n",
      "[2019-08-17 03:16:33,382] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  MemoryStore:54 - Block broadcast_41_piece0 stored as bytes in memory (estimated size 697.0 B, free 364.9 MB)\n",
      "[2019-08-17 03:16:33,382] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  BlockManagerInfo:54 - Added broadcast_41_piece0 in memory on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 (size: 697.0 B, free: 366.0 MB)\n",
      "[2019-08-17 03:16:33,383] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  SparkContext:54 - Created broadcast 41 from broadcast at RandomForest.scala:517\n",
      "[2019-08-17 03:16:33,396] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  SparkContext:54 - Starting job: collectAsMap at RandomForest.scala:567\n",
      "[2019-08-17 03:16:33,398] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  DAGScheduler:54 - Registering RDD 104 (mapPartitions at RandomForest.scala:538)\n",
      "[2019-08-17 03:16:33,398] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  DAGScheduler:54 - Got job 23 (collectAsMap at RandomForest.scala:567) with 1 output partitions\n",
      "[2019-08-17 03:16:33,398] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  DAGScheduler:54 - Final stage: ResultStage 37 (collectAsMap at RandomForest.scala:567)\n",
      "[2019-08-17 03:16:33,398] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  DAGScheduler:54 - Parents of final stage: List(ShuffleMapStage 36)\n",
      "[2019-08-17 03:16:33,398] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  DAGScheduler:54 - Missing parents: List(ShuffleMapStage 36)\n",
      "[2019-08-17 03:16:33,400] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  DAGScheduler:54 - Submitting ShuffleMapStage 36 (MapPartitionsRDD[104] at mapPartitions at RandomForest.scala:538), which has no missing parents\n",
      "[2019-08-17 03:16:33,402] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  MemoryStore:54 - Block broadcast_42 stored as values in memory (estimated size 111.5 KB, free 364.8 MB)\n",
      "[2019-08-17 03:16:33,404] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  MemoryStore:54 - Block broadcast_42_piece0 stored as bytes in memory (estimated size 43.9 KB, free 364.8 MB)\n",
      "[2019-08-17 03:16:33,405] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  BlockManagerInfo:54 - Added broadcast_42_piece0 in memory on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 (size: 43.9 KB, free: 365.9 MB)\n",
      "[2019-08-17 03:16:33,406] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  SparkContext:54 - Created broadcast 42 from broadcast at DAGScheduler.scala:1161\n",
      "[2019-08-17 03:16:33,406] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ShuffleMapStage 36 (MapPartitionsRDD[104] at mapPartitions at RandomForest.scala:538) (first 15 tasks are for partitions Vector(0))\n",
      "[2019-08-17 03:16:33,407] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  YarnScheduler:54 - Adding task set 36.0 with 1 tasks\n",
      "[2019-08-17 03:16:33,408] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  TaskSetManager:54 - Starting task 0.0 in stage 36.0 (TID 36, ip-172-31-29-159.ap-northeast-2.compute.internal, executor 2, partition 0, PROCESS_LOCAL, 8374 bytes)\n",
      "[2019-08-17 03:16:33,420] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  BlockManagerInfo:54 - Added broadcast_42_piece0 in memory on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 (size: 43.9 KB, free: 9.4 GB)\n",
      "[2019-08-17 03:16:33,442] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  BlockManagerInfo:54 - Added broadcast_41_piece0 in memory on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 (size: 697.0 B, free: 9.4 GB)\n",
      "[2019-08-17 03:16:33,479] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  TaskSetManager:54 - Finished task 0.0 in stage 36.0 (TID 36) in 70 ms on ip-172-31-29-159.ap-northeast-2.compute.internal (executor 2) (1/1)\n",
      "[2019-08-17 03:16:33,481] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  YarnScheduler:54 - Removed TaskSet 36.0, whose tasks have all completed, from pool\n",
      "[2019-08-17 03:16:33,481] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  DAGScheduler:54 - ShuffleMapStage 36 (mapPartitions at RandomForest.scala:538) finished in 0.081 s\n",
      "[2019-08-17 03:16:33,481] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  DAGScheduler:54 - looking for newly runnable stages\n",
      "[2019-08-17 03:16:33,481] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  DAGScheduler:54 - running: Set()\n",
      "[2019-08-17 03:16:33,481] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  DAGScheduler:54 - waiting: Set(ResultStage 37)\n",
      "[2019-08-17 03:16:33,481] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  DAGScheduler:54 - failed: Set()\n",
      "[2019-08-17 03:16:33,485] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  DAGScheduler:54 - Submitting ResultStage 37 (MapPartitionsRDD[106] at map at RandomForest.scala:557), which has no missing parents\n",
      "[2019-08-17 03:16:33,485] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  MemoryStore:54 - Block broadcast_43 stored as values in memory (estimated size 12.5 KB, free 364.7 MB)\n",
      "[2019-08-17 03:16:33,485] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  MemoryStore:54 - Block broadcast_43_piece0 stored as bytes in memory (estimated size 5.0 KB, free 364.7 MB)\n",
      "[2019-08-17 03:16:33,485] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  BlockManagerInfo:54 - Added broadcast_43_piece0 in memory on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 (size: 5.0 KB, free: 365.9 MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-08-17 03:16:33,485] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  SparkContext:54 - Created broadcast 43 from broadcast at DAGScheduler.scala:1161\n",
      "[2019-08-17 03:16:33,486] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 37 (MapPartitionsRDD[106] at map at RandomForest.scala:557) (first 15 tasks are for partitions Vector(0))\n",
      "[2019-08-17 03:16:33,486] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  YarnScheduler:54 - Adding task set 37.0 with 1 tasks\n",
      "[2019-08-17 03:16:33,490] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  TaskSetManager:54 - Starting task 0.0 in stage 37.0 (TID 37, ip-172-31-29-159.ap-northeast-2.compute.internal, executor 2, partition 0, NODE_LOCAL, 7673 bytes)\n",
      "[2019-08-17 03:16:33,499] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  BlockManagerInfo:54 - Added broadcast_43_piece0 in memory on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 (size: 5.0 KB, free: 9.4 GB)\n",
      "[2019-08-17 03:16:33,505] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  MapOutputTrackerMasterEndpoint:54 - Asked to send map output locations for shuffle 13 to 172.31.29.159:58394\n",
      "[2019-08-17 03:16:33,590] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  TaskSetManager:54 - Finished task 0.0 in stage 37.0 (TID 37) in 101 ms on ip-172-31-29-159.ap-northeast-2.compute.internal (executor 2) (1/1)\n",
      "[2019-08-17 03:16:33,590] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  YarnScheduler:54 - Removed TaskSet 37.0, whose tasks have all completed, from pool\n",
      "[2019-08-17 03:16:33,592] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  DAGScheduler:54 - ResultStage 37 (collectAsMap at RandomForest.scala:567) finished in 0.109 s\n",
      "[2019-08-17 03:16:33,592] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  DAGScheduler:54 - Job 23 finished: collectAsMap at RandomForest.scala:567, took 0.195156 s\n",
      "[2019-08-17 03:16:33,599] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  MemoryStore:54 - Block broadcast_44 stored as values in memory (estimated size 5.3 KB, free 364.7 MB)\n",
      "[2019-08-17 03:16:33,601] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  MemoryStore:54 - Block broadcast_44_piece0 stored as bytes in memory (estimated size 756.0 B, free 364.7 MB)\n",
      "[2019-08-17 03:16:33,601] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  BlockManagerInfo:54 - Added broadcast_44_piece0 in memory on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 (size: 756.0 B, free: 365.9 MB)\n",
      "[2019-08-17 03:16:33,602] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  SparkContext:54 - Created broadcast 44 from broadcast at RandomForest.scala:517\n",
      "[2019-08-17 03:16:33,611] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  SparkContext:54 - Starting job: collectAsMap at RandomForest.scala:567\n",
      "[2019-08-17 03:16:33,613] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  DAGScheduler:54 - Registering RDD 107 (mapPartitions at RandomForest.scala:538)\n",
      "[2019-08-17 03:16:33,613] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  DAGScheduler:54 - Got job 24 (collectAsMap at RandomForest.scala:567) with 1 output partitions\n",
      "[2019-08-17 03:16:33,613] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  DAGScheduler:54 - Final stage: ResultStage 39 (collectAsMap at RandomForest.scala:567)\n",
      "[2019-08-17 03:16:33,613] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  DAGScheduler:54 - Parents of final stage: List(ShuffleMapStage 38)\n",
      "[2019-08-17 03:16:33,613] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  DAGScheduler:54 - Missing parents: List(ShuffleMapStage 38)\n",
      "[2019-08-17 03:16:33,618] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  DAGScheduler:54 - Submitting ShuffleMapStage 38 (MapPartitionsRDD[107] at mapPartitions at RandomForest.scala:538), which has no missing parents\n",
      "[2019-08-17 03:16:33,619] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  MemoryStore:54 - Block broadcast_45 stored as values in memory (estimated size 122.6 KB, free 364.6 MB)\n",
      "[2019-08-17 03:16:33,619] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  MemoryStore:54 - Block broadcast_45_piece0 stored as bytes in memory (estimated size 47.0 KB, free 364.6 MB)\n",
      "[2019-08-17 03:16:33,620] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  BlockManagerInfo:54 - Added broadcast_45_piece0 in memory on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 (size: 47.0 KB, free: 365.9 MB)\n",
      "[2019-08-17 03:16:33,621] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  SparkContext:54 - Created broadcast 45 from broadcast at DAGScheduler.scala:1161\n",
      "[2019-08-17 03:16:33,621] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ShuffleMapStage 38 (MapPartitionsRDD[107] at mapPartitions at RandomForest.scala:538) (first 15 tasks are for partitions Vector(0))\n",
      "[2019-08-17 03:16:33,622] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  YarnScheduler:54 - Adding task set 38.0 with 1 tasks\n",
      "[2019-08-17 03:16:33,624] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  TaskSetManager:54 - Starting task 0.0 in stage 38.0 (TID 38, ip-172-31-29-159.ap-northeast-2.compute.internal, executor 2, partition 0, PROCESS_LOCAL, 8374 bytes)\n",
      "[2019-08-17 03:16:33,634] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  BlockManagerInfo:54 - Added broadcast_45_piece0 in memory on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 (size: 47.0 KB, free: 9.4 GB)\n",
      "[2019-08-17 03:16:33,655] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  BlockManagerInfo:54 - Added broadcast_44_piece0 in memory on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 (size: 756.0 B, free: 9.4 GB)\n",
      "[2019-08-17 03:16:33,681] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  TaskSetManager:54 - Finished task 0.0 in stage 38.0 (TID 38) in 55 ms on ip-172-31-29-159.ap-northeast-2.compute.internal (executor 2) (1/1)\n",
      "[2019-08-17 03:16:33,681] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  YarnScheduler:54 - Removed TaskSet 38.0, whose tasks have all completed, from pool\n",
      "[2019-08-17 03:16:33,681] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  DAGScheduler:54 - ShuffleMapStage 38 (mapPartitions at RandomForest.scala:538) finished in 0.065 s\n",
      "[2019-08-17 03:16:33,681] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  DAGScheduler:54 - looking for newly runnable stages\n",
      "[2019-08-17 03:16:33,682] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  DAGScheduler:54 - running: Set()\n",
      "[2019-08-17 03:16:33,682] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  DAGScheduler:54 - waiting: Set(ResultStage 39)\n",
      "[2019-08-17 03:16:33,682] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  DAGScheduler:54 - failed: Set()\n",
      "[2019-08-17 03:16:33,682] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  DAGScheduler:54 - Submitting ResultStage 39 (MapPartitionsRDD[109] at map at RandomForest.scala:557), which has no missing parents\n",
      "[2019-08-17 03:16:33,684] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  MemoryStore:54 - Block broadcast_46 stored as values in memory (estimated size 12.9 KB, free 364.6 MB)\n",
      "[2019-08-17 03:16:33,684] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  MemoryStore:54 - Block broadcast_46_piece0 stored as bytes in memory (estimated size 5.1 KB, free 364.5 MB)\n",
      "[2019-08-17 03:16:33,685] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  BlockManagerInfo:54 - Added broadcast_46_piece0 in memory on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 (size: 5.1 KB, free: 365.9 MB)\n",
      "[2019-08-17 03:16:33,685] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  SparkContext:54 - Created broadcast 46 from broadcast at DAGScheduler.scala:1161\n",
      "[2019-08-17 03:16:33,687] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 39 (MapPartitionsRDD[109] at map at RandomForest.scala:557) (first 15 tasks are for partitions Vector(0))\n",
      "[2019-08-17 03:16:33,688] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  YarnScheduler:54 - Adding task set 39.0 with 1 tasks\n",
      "[2019-08-17 03:16:33,689] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  TaskSetManager:54 - Starting task 0.0 in stage 39.0 (TID 39, ip-172-31-29-159.ap-northeast-2.compute.internal, executor 2, partition 0, NODE_LOCAL, 7673 bytes)\n",
      "[2019-08-17 03:16:33,698] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  BlockManagerInfo:54 - Added broadcast_46_piece0 in memory on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 (size: 5.1 KB, free: 9.4 GB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-08-17 03:16:33,703] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  MapOutputTrackerMasterEndpoint:54 - Asked to send map output locations for shuffle 14 to 172.31.29.159:58394\n",
      "[2019-08-17 03:16:33,796] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  TaskSetManager:54 - Finished task 0.0 in stage 39.0 (TID 39) in 108 ms on ip-172-31-29-159.ap-northeast-2.compute.internal (executor 2) (1/1)\n",
      "[2019-08-17 03:16:33,796] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  YarnScheduler:54 - Removed TaskSet 39.0, whose tasks have all completed, from pool\n",
      "[2019-08-17 03:16:33,797] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  DAGScheduler:54 - ResultStage 39 (collectAsMap at RandomForest.scala:567) finished in 0.117 s\n",
      "[2019-08-17 03:16:33,802] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  DAGScheduler:54 - Job 24 finished: collectAsMap at RandomForest.scala:567, took 0.185659 s\n",
      "[2019-08-17 03:16:33,802] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  MemoryStore:54 - Block broadcast_47 stored as values in memory (estimated size 6.6 KB, free 364.5 MB)\n",
      "[2019-08-17 03:16:33,803] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  MemoryStore:54 - Block broadcast_47_piece0 stored as bytes in memory (estimated size 868.0 B, free 364.5 MB)\n",
      "[2019-08-17 03:16:33,804] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  BlockManagerInfo:54 - Added broadcast_47_piece0 in memory on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 (size: 868.0 B, free: 365.9 MB)\n",
      "[2019-08-17 03:16:33,807] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  SparkContext:54 - Created broadcast 47 from broadcast at RandomForest.scala:517\n",
      "[2019-08-17 03:16:33,816] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  SparkContext:54 - Starting job: collectAsMap at RandomForest.scala:567\n",
      "[2019-08-17 03:16:33,818] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  DAGScheduler:54 - Registering RDD 110 (mapPartitions at RandomForest.scala:538)\n",
      "[2019-08-17 03:16:33,818] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  DAGScheduler:54 - Got job 25 (collectAsMap at RandomForest.scala:567) with 1 output partitions\n",
      "[2019-08-17 03:16:33,819] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  DAGScheduler:54 - Final stage: ResultStage 41 (collectAsMap at RandomForest.scala:567)\n",
      "[2019-08-17 03:16:33,819] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  DAGScheduler:54 - Parents of final stage: List(ShuffleMapStage 40)\n",
      "[2019-08-17 03:16:33,819] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  DAGScheduler:54 - Missing parents: List(ShuffleMapStage 40)\n",
      "[2019-08-17 03:16:33,821] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  DAGScheduler:54 - Submitting ShuffleMapStage 40 (MapPartitionsRDD[110] at mapPartitions at RandomForest.scala:538), which has no missing parents\n",
      "[2019-08-17 03:16:33,823] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  MemoryStore:54 - Block broadcast_48 stored as values in memory (estimated size 137.6 KB, free 364.4 MB)\n",
      "[2019-08-17 03:16:33,825] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  MemoryStore:54 - Block broadcast_48_piece0 stored as bytes in memory (estimated size 51.2 KB, free 364.4 MB)\n",
      "[2019-08-17 03:16:33,826] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  BlockManagerInfo:54 - Added broadcast_48_piece0 in memory on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 (size: 51.2 KB, free: 365.8 MB)\n",
      "[2019-08-17 03:16:33,826] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  SparkContext:54 - Created broadcast 48 from broadcast at DAGScheduler.scala:1161\n",
      "[2019-08-17 03:16:33,827] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ShuffleMapStage 40 (MapPartitionsRDD[110] at mapPartitions at RandomForest.scala:538) (first 15 tasks are for partitions Vector(0))\n",
      "[2019-08-17 03:16:33,827] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  YarnScheduler:54 - Adding task set 40.0 with 1 tasks\n",
      "[2019-08-17 03:16:33,828] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  TaskSetManager:54 - Starting task 0.0 in stage 40.0 (TID 40, ip-172-31-29-159.ap-northeast-2.compute.internal, executor 2, partition 0, PROCESS_LOCAL, 8374 bytes)\n",
      "[2019-08-17 03:16:33,837] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  BlockManagerInfo:54 - Added broadcast_48_piece0 in memory on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 (size: 51.2 KB, free: 9.4 GB)\n",
      "[2019-08-17 03:16:33,858] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  BlockManagerInfo:54 - Added broadcast_47_piece0 in memory on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 (size: 868.0 B, free: 9.4 GB)\n",
      "[2019-08-17 03:16:33,883] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  TaskSetManager:54 - Finished task 0.0 in stage 40.0 (TID 40) in 55 ms on ip-172-31-29-159.ap-northeast-2.compute.internal (executor 2) (1/1)\n",
      "[2019-08-17 03:16:33,883] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  YarnScheduler:54 - Removed TaskSet 40.0, whose tasks have all completed, from pool\n",
      "[2019-08-17 03:16:33,884] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  DAGScheduler:54 - ShuffleMapStage 40 (mapPartitions at RandomForest.scala:538) finished in 0.063 s\n",
      "[2019-08-17 03:16:33,884] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  DAGScheduler:54 - looking for newly runnable stages\n",
      "[2019-08-17 03:16:33,884] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  DAGScheduler:54 - running: Set()\n",
      "[2019-08-17 03:16:33,884] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  DAGScheduler:54 - waiting: Set(ResultStage 41)\n",
      "[2019-08-17 03:16:33,884] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  DAGScheduler:54 - failed: Set()\n",
      "[2019-08-17 03:16:33,884] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  DAGScheduler:54 - Submitting ResultStage 41 (MapPartitionsRDD[112] at map at RandomForest.scala:557), which has no missing parents\n",
      "[2019-08-17 03:16:33,886] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  MemoryStore:54 - Block broadcast_49 stored as values in memory (estimated size 13.9 KB, free 364.3 MB)\n",
      "[2019-08-17 03:16:33,888] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  MemoryStore:54 - Block broadcast_49_piece0 stored as bytes in memory (estimated size 5.3 KB, free 364.3 MB)\n",
      "[2019-08-17 03:16:33,888] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  BlockManagerInfo:54 - Added broadcast_49_piece0 in memory on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 (size: 5.3 KB, free: 365.8 MB)\n",
      "[2019-08-17 03:16:33,888] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  SparkContext:54 - Created broadcast 49 from broadcast at DAGScheduler.scala:1161\n",
      "[2019-08-17 03:16:33,889] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 41 (MapPartitionsRDD[112] at map at RandomForest.scala:557) (first 15 tasks are for partitions Vector(0))\n",
      "[2019-08-17 03:16:33,889] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  YarnScheduler:54 - Adding task set 41.0 with 1 tasks\n",
      "[2019-08-17 03:16:33,890] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  TaskSetManager:54 - Starting task 0.0 in stage 41.0 (TID 41, ip-172-31-29-159.ap-northeast-2.compute.internal, executor 2, partition 0, NODE_LOCAL, 7673 bytes)\n",
      "[2019-08-17 03:16:33,899] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  BlockManagerInfo:54 - Added broadcast_49_piece0 in memory on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 (size: 5.3 KB, free: 9.4 GB)\n",
      "[2019-08-17 03:16:33,905] {bash_operator.py:127} INFO - 2019-08-17 03:16:33 INFO  MapOutputTrackerMasterEndpoint:54 - Asked to send map output locations for shuffle 15 to 172.31.29.159:58394\n",
      "[2019-08-17 03:16:34,063] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  TaskSetManager:54 - Finished task 0.0 in stage 41.0 (TID 41) in 172 ms on ip-172-31-29-159.ap-northeast-2.compute.internal (executor 2) (1/1)\n",
      "[2019-08-17 03:16:34,064] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  YarnScheduler:54 - Removed TaskSet 41.0, whose tasks have all completed, from pool\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-08-17 03:16:34,066] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  DAGScheduler:54 - ResultStage 41 (collectAsMap at RandomForest.scala:567) finished in 0.178 s\n",
      "[2019-08-17 03:16:34,066] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  DAGScheduler:54 - Job 25 finished: collectAsMap at RandomForest.scala:567, took 0.247221 s\n",
      "[2019-08-17 03:16:34,069] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  MemoryStore:54 - Block broadcast_50 stored as values in memory (estimated size 6.7 KB, free 364.3 MB)\n",
      "[2019-08-17 03:16:34,074] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  MemoryStore:54 - Block broadcast_50_piece0 stored as bytes in memory (estimated size 863.0 B, free 364.3 MB)\n",
      "[2019-08-17 03:16:34,075] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Added broadcast_50_piece0 in memory on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 (size: 863.0 B, free: 365.8 MB)\n",
      "[2019-08-17 03:16:34,077] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  SparkContext:54 - Created broadcast 50 from broadcast at RandomForest.scala:517\n",
      "[2019-08-17 03:16:34,090] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  SparkContext:54 - Starting job: collectAsMap at RandomForest.scala:567\n",
      "[2019-08-17 03:16:34,093] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  DAGScheduler:54 - Registering RDD 113 (mapPartitions at RandomForest.scala:538)\n",
      "[2019-08-17 03:16:34,093] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  DAGScheduler:54 - Got job 26 (collectAsMap at RandomForest.scala:567) with 1 output partitions\n",
      "[2019-08-17 03:16:34,093] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  DAGScheduler:54 - Final stage: ResultStage 43 (collectAsMap at RandomForest.scala:567)\n",
      "[2019-08-17 03:16:34,094] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  DAGScheduler:54 - Parents of final stage: List(ShuffleMapStage 42)\n",
      "[2019-08-17 03:16:34,094] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  DAGScheduler:54 - Missing parents: List(ShuffleMapStage 42)\n",
      "[2019-08-17 03:16:34,097] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  DAGScheduler:54 - Submitting ShuffleMapStage 42 (MapPartitionsRDD[113] at mapPartitions at RandomForest.scala:538), which has no missing parents\n",
      "[2019-08-17 03:16:34,100] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  MemoryStore:54 - Block broadcast_51 stored as values in memory (estimated size 151.6 KB, free 364.2 MB)\n",
      "[2019-08-17 03:16:34,104] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  MemoryStore:54 - Block broadcast_51_piece0 stored as bytes in memory (estimated size 55.3 KB, free 364.1 MB)\n",
      "[2019-08-17 03:16:34,105] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Added broadcast_51_piece0 in memory on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 (size: 55.3 KB, free: 365.8 MB)\n",
      "[2019-08-17 03:16:34,105] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  SparkContext:54 - Created broadcast 51 from broadcast at DAGScheduler.scala:1161\n",
      "[2019-08-17 03:16:34,106] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ShuffleMapStage 42 (MapPartitionsRDD[113] at mapPartitions at RandomForest.scala:538) (first 15 tasks are for partitions Vector(0))\n",
      "[2019-08-17 03:16:34,106] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  YarnScheduler:54 - Adding task set 42.0 with 1 tasks\n",
      "[2019-08-17 03:16:34,108] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  TaskSetManager:54 - Starting task 0.0 in stage 42.0 (TID 42, ip-172-31-29-159.ap-northeast-2.compute.internal, executor 2, partition 0, PROCESS_LOCAL, 8374 bytes)\n",
      "[2019-08-17 03:16:34,122] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Added broadcast_51_piece0 in memory on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 (size: 55.3 KB, free: 9.4 GB)\n",
      "[2019-08-17 03:16:34,149] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Added broadcast_50_piece0 in memory on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 (size: 863.0 B, free: 9.4 GB)\n",
      "[2019-08-17 03:16:34,172] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  TaskSetManager:54 - Finished task 0.0 in stage 42.0 (TID 42) in 65 ms on ip-172-31-29-159.ap-northeast-2.compute.internal (executor 2) (1/1)\n",
      "[2019-08-17 03:16:34,172] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  YarnScheduler:54 - Removed TaskSet 42.0, whose tasks have all completed, from pool\n",
      "[2019-08-17 03:16:34,174] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  DAGScheduler:54 - ShuffleMapStage 42 (mapPartitions at RandomForest.scala:538) finished in 0.079 s\n",
      "[2019-08-17 03:16:34,174] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  DAGScheduler:54 - looking for newly runnable stages\n",
      "[2019-08-17 03:16:34,174] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  DAGScheduler:54 - running: Set()\n",
      "[2019-08-17 03:16:34,175] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  DAGScheduler:54 - waiting: Set(ResultStage 43)\n",
      "[2019-08-17 03:16:34,175] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  DAGScheduler:54 - failed: Set()\n",
      "[2019-08-17 03:16:34,175] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  DAGScheduler:54 - Submitting ResultStage 43 (MapPartitionsRDD[115] at map at RandomForest.scala:557), which has no missing parents\n",
      "[2019-08-17 03:16:34,180] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  MemoryStore:54 - Block broadcast_52 stored as values in memory (estimated size 14.0 KB, free 364.1 MB)\n",
      "[2019-08-17 03:16:34,183] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  MemoryStore:54 - Block broadcast_52_piece0 stored as bytes in memory (estimated size 5.4 KB, free 364.1 MB)\n",
      "[2019-08-17 03:16:34,184] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Added broadcast_52_piece0 in memory on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 (size: 5.4 KB, free: 365.8 MB)\n",
      "[2019-08-17 03:16:34,184] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  SparkContext:54 - Created broadcast 52 from broadcast at DAGScheduler.scala:1161\n",
      "[2019-08-17 03:16:34,185] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 43 (MapPartitionsRDD[115] at map at RandomForest.scala:557) (first 15 tasks are for partitions Vector(0))\n",
      "[2019-08-17 03:16:34,185] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  YarnScheduler:54 - Adding task set 43.0 with 1 tasks\n",
      "[2019-08-17 03:16:34,187] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  TaskSetManager:54 - Starting task 0.0 in stage 43.0 (TID 43, ip-172-31-29-159.ap-northeast-2.compute.internal, executor 2, partition 0, NODE_LOCAL, 7673 bytes)\n",
      "[2019-08-17 03:16:34,196] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Added broadcast_52_piece0 in memory on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 (size: 5.4 KB, free: 9.4 GB)\n",
      "[2019-08-17 03:16:34,202] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  MapOutputTrackerMasterEndpoint:54 - Asked to send map output locations for shuffle 16 to 172.31.29.159:58394\n",
      "[2019-08-17 03:16:34,312] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  TaskSetManager:54 - Finished task 0.0 in stage 43.0 (TID 43) in 126 ms on ip-172-31-29-159.ap-northeast-2.compute.internal (executor 2) (1/1)\n",
      "[2019-08-17 03:16:34,313] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  YarnScheduler:54 - Removed TaskSet 43.0, whose tasks have all completed, from pool\n",
      "[2019-08-17 03:16:34,314] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  DAGScheduler:54 - ResultStage 43 (collectAsMap at RandomForest.scala:567) finished in 0.135 s\n",
      "[2019-08-17 03:16:34,318] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  DAGScheduler:54 - Job 26 finished: collectAsMap at RandomForest.scala:567, took 0.224900 s\n",
      "[2019-08-17 03:16:34,319] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  MemoryStore:54 - Block broadcast_53 stored as values in memory (estimated size 7.0 KB, free 364.1 MB)\n",
      "[2019-08-17 03:16:34,320] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  MemoryStore:54 - Block broadcast_53_piece0 stored as bytes in memory (estimated size 897.0 B, free 364.1 MB)\n",
      "[2019-08-17 03:16:34,321] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Added broadcast_53_piece0 in memory on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 (size: 897.0 B, free: 365.8 MB)\n",
      "[2019-08-17 03:16:34,322] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  SparkContext:54 - Created broadcast 53 from broadcast at RandomForest.scala:517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-08-17 03:16:34,342] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  SparkContext:54 - Starting job: collectAsMap at RandomForest.scala:567\n",
      "[2019-08-17 03:16:34,347] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  DAGScheduler:54 - Registering RDD 116 (mapPartitions at RandomForest.scala:538)\n",
      "[2019-08-17 03:16:34,348] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  DAGScheduler:54 - Got job 27 (collectAsMap at RandomForest.scala:567) with 1 output partitions\n",
      "[2019-08-17 03:16:34,348] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  DAGScheduler:54 - Final stage: ResultStage 45 (collectAsMap at RandomForest.scala:567)\n",
      "[2019-08-17 03:16:34,348] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  DAGScheduler:54 - Parents of final stage: List(ShuffleMapStage 44)\n",
      "[2019-08-17 03:16:34,348] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  DAGScheduler:54 - Missing parents: List(ShuffleMapStage 44)\n",
      "[2019-08-17 03:16:34,350] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  DAGScheduler:54 - Submitting ShuffleMapStage 44 (MapPartitionsRDD[116] at mapPartitions at RandomForest.scala:538), which has no missing parents\n",
      "[2019-08-17 03:16:34,353] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  MemoryStore:54 - Block broadcast_54 stored as values in memory (estimated size 169.3 KB, free 363.9 MB)\n",
      "[2019-08-17 03:16:34,358] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  MemoryStore:54 - Block broadcast_54_piece0 stored as bytes in memory (estimated size 60.2 KB, free 363.9 MB)\n",
      "[2019-08-17 03:16:34,359] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Added broadcast_54_piece0 in memory on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 (size: 60.2 KB, free: 365.7 MB)\n",
      "[2019-08-17 03:16:34,360] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  SparkContext:54 - Created broadcast 54 from broadcast at DAGScheduler.scala:1161\n",
      "[2019-08-17 03:16:34,362] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ShuffleMapStage 44 (MapPartitionsRDD[116] at mapPartitions at RandomForest.scala:538) (first 15 tasks are for partitions Vector(0))\n",
      "[2019-08-17 03:16:34,362] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  YarnScheduler:54 - Adding task set 44.0 with 1 tasks\n",
      "[2019-08-17 03:16:34,363] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  TaskSetManager:54 - Starting task 0.0 in stage 44.0 (TID 44, ip-172-31-29-159.ap-northeast-2.compute.internal, executor 2, partition 0, PROCESS_LOCAL, 8374 bytes)\n",
      "[2019-08-17 03:16:34,374] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Added broadcast_54_piece0 in memory on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 (size: 60.2 KB, free: 9.4 GB)\n",
      "[2019-08-17 03:16:34,399] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Added broadcast_53_piece0 in memory on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 (size: 897.0 B, free: 9.4 GB)\n",
      "[2019-08-17 03:16:34,424] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  TaskSetManager:54 - Finished task 0.0 in stage 44.0 (TID 44) in 61 ms on ip-172-31-29-159.ap-northeast-2.compute.internal (executor 2) (1/1)\n",
      "[2019-08-17 03:16:34,424] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  YarnScheduler:54 - Removed TaskSet 44.0, whose tasks have all completed, from pool\n",
      "[2019-08-17 03:16:34,425] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  DAGScheduler:54 - ShuffleMapStage 44 (mapPartitions at RandomForest.scala:538) finished in 0.076 s\n",
      "[2019-08-17 03:16:34,425] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  DAGScheduler:54 - looking for newly runnable stages\n",
      "[2019-08-17 03:16:34,425] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  DAGScheduler:54 - running: Set()\n",
      "[2019-08-17 03:16:34,425] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  DAGScheduler:54 - waiting: Set(ResultStage 45)\n",
      "[2019-08-17 03:16:34,425] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  DAGScheduler:54 - failed: Set()\n",
      "[2019-08-17 03:16:34,426] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  DAGScheduler:54 - Submitting ResultStage 45 (MapPartitionsRDD[118] at map at RandomForest.scala:557), which has no missing parents\n",
      "[2019-08-17 03:16:34,427] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  MemoryStore:54 - Block broadcast_55 stored as values in memory (estimated size 14.2 KB, free 363.9 MB)\n",
      "[2019-08-17 03:16:34,428] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  MemoryStore:54 - Block broadcast_55_piece0 stored as bytes in memory (estimated size 5.4 KB, free 363.9 MB)\n",
      "[2019-08-17 03:16:34,429] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Added broadcast_55_piece0 in memory on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 (size: 5.4 KB, free: 365.7 MB)\n",
      "[2019-08-17 03:16:34,429] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  SparkContext:54 - Created broadcast 55 from broadcast at DAGScheduler.scala:1161\n",
      "[2019-08-17 03:16:34,429] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 45 (MapPartitionsRDD[118] at map at RandomForest.scala:557) (first 15 tasks are for partitions Vector(0))\n",
      "[2019-08-17 03:16:34,429] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  YarnScheduler:54 - Adding task set 45.0 with 1 tasks\n",
      "[2019-08-17 03:16:34,430] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  TaskSetManager:54 - Starting task 0.0 in stage 45.0 (TID 45, ip-172-31-29-159.ap-northeast-2.compute.internal, executor 2, partition 0, NODE_LOCAL, 7673 bytes)\n",
      "[2019-08-17 03:16:34,440] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Added broadcast_55_piece0 in memory on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 (size: 5.4 KB, free: 9.4 GB)\n",
      "[2019-08-17 03:16:34,445] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  MapOutputTrackerMasterEndpoint:54 - Asked to send map output locations for shuffle 17 to 172.31.29.159:58394\n",
      "[2019-08-17 03:16:34,628] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  TaskSetManager:54 - Finished task 0.0 in stage 45.0 (TID 45) in 198 ms on ip-172-31-29-159.ap-northeast-2.compute.internal (executor 2) (1/1)\n",
      "[2019-08-17 03:16:34,629] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  YarnScheduler:54 - Removed TaskSet 45.0, whose tasks have all completed, from pool\n",
      "[2019-08-17 03:16:34,629] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  DAGScheduler:54 - ResultStage 45 (collectAsMap at RandomForest.scala:567) finished in 0.203 s\n",
      "[2019-08-17 03:16:34,634] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  DAGScheduler:54 - Job 27 finished: collectAsMap at RandomForest.scala:567, took 0.284322 s\n",
      "[2019-08-17 03:16:34,634] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  MemoryStore:54 - Block broadcast_56 stored as values in memory (estimated size 8.0 KB, free 363.9 MB)\n",
      "[2019-08-17 03:16:34,635] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  MemoryStore:54 - Block broadcast_56_piece0 stored as bytes in memory (estimated size 953.0 B, free 363.8 MB)\n",
      "[2019-08-17 03:16:34,636] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Added broadcast_56_piece0 in memory on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 (size: 953.0 B, free: 365.7 MB)\n",
      "[2019-08-17 03:16:34,636] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  SparkContext:54 - Created broadcast 56 from broadcast at RandomForest.scala:517\n",
      "[2019-08-17 03:16:34,654] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  SparkContext:54 - Starting job: collectAsMap at RandomForest.scala:567\n",
      "[2019-08-17 03:16:34,656] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  DAGScheduler:54 - Registering RDD 119 (mapPartitions at RandomForest.scala:538)\n",
      "[2019-08-17 03:16:34,656] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  DAGScheduler:54 - Got job 28 (collectAsMap at RandomForest.scala:567) with 1 output partitions\n",
      "[2019-08-17 03:16:34,656] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  DAGScheduler:54 - Final stage: ResultStage 47 (collectAsMap at RandomForest.scala:567)\n",
      "[2019-08-17 03:16:34,656] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  DAGScheduler:54 - Parents of final stage: List(ShuffleMapStage 46)\n",
      "[2019-08-17 03:16:34,656] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  DAGScheduler:54 - Missing parents: List(ShuffleMapStage 46)\n",
      "[2019-08-17 03:16:34,657] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  DAGScheduler:54 - Submitting ShuffleMapStage 46 (MapPartitionsRDD[119] at mapPartitions at RandomForest.scala:538), which has no missing parents\n",
      "[2019-08-17 03:16:34,662] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  MemoryStore:54 - Block broadcast_57 stored as values in memory (estimated size 189.1 KB, free 363.7 MB)\n",
      "[2019-08-17 03:16:34,673] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 998\n",
      "[2019-08-17 03:16:34,675] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned shuffle 14\n",
      "[2019-08-17 03:16:34,675] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 678\n",
      "[2019-08-17 03:16:34,676] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1071\n",
      "[2019-08-17 03:16:34,676] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 943\n",
      "[2019-08-17 03:16:34,676] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1105\n",
      "[2019-08-17 03:16:34,676] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 922\n",
      "[2019-08-17 03:16:34,676] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1190\n",
      "[2019-08-17 03:16:34,676] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 736\n",
      "[2019-08-17 03:16:34,676] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1065\n",
      "[2019-08-17 03:16:34,676] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 699\n",
      "[2019-08-17 03:16:34,676] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 969\n",
      "[2019-08-17 03:16:34,676] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1163\n",
      "[2019-08-17 03:16:34,676] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  MemoryStore:54 - Block broadcast_57_piece0 stored as bytes in memory (estimated size 65.8 KB, free 363.6 MB)\n",
      "[2019-08-17 03:16:34,677] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Added broadcast_57_piece0 in memory on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 (size: 65.8 KB, free: 365.6 MB)\n",
      "[2019-08-17 03:16:34,677] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  SparkContext:54 - Created broadcast 57 from broadcast at DAGScheduler.scala:1161\n",
      "[2019-08-17 03:16:34,678] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ShuffleMapStage 46 (MapPartitionsRDD[119] at mapPartitions at RandomForest.scala:538) (first 15 tasks are for partitions Vector(0))\n",
      "[2019-08-17 03:16:34,679] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  YarnScheduler:54 - Adding task set 46.0 with 1 tasks\n",
      "[2019-08-17 03:16:34,679] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Removed broadcast_35_piece0 on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 in memory (size: 510.0 B, free: 9.4 GB)\n",
      "[2019-08-17 03:16:34,679] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  TaskSetManager:54 - Starting task 0.0 in stage 46.0 (TID 46, ip-172-31-29-159.ap-northeast-2.compute.internal, executor 2, partition 0, PROCESS_LOCAL, 8374 bytes)\n",
      "[2019-08-17 03:16:34,683] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Removed broadcast_35_piece0 on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 in memory (size: 510.0 B, free: 365.6 MB)\n",
      "[2019-08-17 03:16:34,691] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Added broadcast_57_piece0 in memory on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 (size: 65.8 KB, free: 9.4 GB)\n",
      "[2019-08-17 03:16:34,692] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1080\n",
      "[2019-08-17 03:16:34,692] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 877\n",
      "[2019-08-17 03:16:34,692] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 964\n",
      "[2019-08-17 03:16:34,692] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 905\n",
      "[2019-08-17 03:16:34,692] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 855\n",
      "[2019-08-17 03:16:34,692] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 896\n",
      "[2019-08-17 03:16:34,692] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 938\n",
      "[2019-08-17 03:16:34,692] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 968\n",
      "[2019-08-17 03:16:34,692] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 796\n",
      "[2019-08-17 03:16:34,692] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 874\n",
      "[2019-08-17 03:16:34,692] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1008\n",
      "[2019-08-17 03:16:34,692] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 712\n",
      "[2019-08-17 03:16:34,693] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1149\n",
      "[2019-08-17 03:16:34,693] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 984\n",
      "[2019-08-17 03:16:34,693] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1170\n",
      "[2019-08-17 03:16:34,693] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1160\n",
      "[2019-08-17 03:16:34,693] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 816\n",
      "[2019-08-17 03:16:34,693] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1138\n",
      "[2019-08-17 03:16:34,693] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 864\n",
      "[2019-08-17 03:16:34,694] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Removed broadcast_26_piece0 on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 in memory (size: 30.8 KB, free: 365.7 MB)\n",
      "[2019-08-17 03:16:34,695] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Removed broadcast_26_piece0 on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 in memory (size: 30.8 KB, free: 9.4 GB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-08-17 03:16:34,701] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 965\r\n",
      "[2019-08-17 03:16:34,702] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 860\r\n",
      "[2019-08-17 03:16:34,702] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1055\r\n",
      "[2019-08-17 03:16:34,702] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1167\r\n",
      "[2019-08-17 03:16:34,702] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1001\r\n",
      "[2019-08-17 03:16:34,702] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 802\r\n",
      "[2019-08-17 03:16:34,702] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1123\r\n",
      "[2019-08-17 03:16:34,702] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 975\r\n",
      "[2019-08-17 03:16:34,702] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 883\r\n",
      "[2019-08-17 03:16:34,702] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1017\r\n",
      "[2019-08-17 03:16:34,702] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1025\r\n",
      "[2019-08-17 03:16:34,703] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Removed broadcast_50_piece0 on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 in memory (size: 863.0 B, free: 365.7 MB)\r\n",
      "[2019-08-17 03:16:34,708] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Removed broadcast_50_piece0 on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 in memory (size: 863.0 B, free: 9.4 GB)\r\n",
      "[2019-08-17 03:16:34,710] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1191\r\n",
      "[2019-08-17 03:16:34,710] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 920\r\n",
      "[2019-08-17 03:16:34,710] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1073\r\n",
      "[2019-08-17 03:16:34,710] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1112\r\n",
      "[2019-08-17 03:16:34,710] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1016\r\n",
      "[2019-08-17 03:16:34,710] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 813\r\n",
      "[2019-08-17 03:16:34,710] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 871\r\n",
      "[2019-08-17 03:16:34,710] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 681\r\n",
      "[2019-08-17 03:16:34,710] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 760\r\n",
      "[2019-08-17 03:16:34,710] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 976\r\n",
      "[2019-08-17 03:16:34,711] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 809\r\n",
      "[2019-08-17 03:16:34,711] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 659\r\n",
      "[2019-08-17 03:16:34,711] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Removed broadcast_51_piece0 on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 in memory (size: 55.3 KB, free: 365.7 MB)\r\n",
      "[2019-08-17 03:16:34,712] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Added broadcast_56_piece0 in memory on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 (size: 953.0 B, free: 9.4 GB)\r\n",
      "[2019-08-17 03:16:34,714] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Removed broadcast_51_piece0 on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 in memory (size: 55.3 KB, free: 9.4 GB)\r\n",
      "[2019-08-17 03:16:34,717] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 780\r\n",
      "[2019-08-17 03:16:34,717] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 672\r\n",
      "[2019-08-17 03:16:34,717] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 730\r\n",
      "[2019-08-17 03:16:34,717] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 732\r\n",
      "[2019-08-17 03:16:34,717] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1116\r\n",
      "[2019-08-17 03:16:34,717] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 768\r\n",
      "[2019-08-17 03:16:34,717] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 884\r\n",
      "[2019-08-17 03:16:34,718] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1084\r\n",
      "[2019-08-17 03:16:34,718] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1092\r\n",
      "[2019-08-17 03:16:34,718] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 989\r\n",
      "[2019-08-17 03:16:34,718] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1026\r\n",
      "[2019-08-17 03:16:34,718] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 849\r\n",
      "[2019-08-17 03:16:34,718] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1088\r\n",
      "[2019-08-17 03:16:34,718] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 888\r\n",
      "[2019-08-17 03:16:34,719] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 694\r\n",
      "[2019-08-17 03:16:34,719] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 875\r\n",
      "[2019-08-17 03:16:34,719] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Removed broadcast_52_piece0 on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 in memory (size: 5.4 KB, free: 365.7 MB)\r\n",
      "[2019-08-17 03:16:34,724] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Removed broadcast_52_piece0 on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 in memory (size: 5.4 KB, free: 9.4 GB)\r\n",
      "[2019-08-17 03:16:34,726] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 927\r\n",
      "[2019-08-17 03:16:34,726] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 777\r\n",
      "[2019-08-17 03:16:34,726] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1107\r\n",
      "[2019-08-17 03:16:34,726] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 740\r\n",
      "[2019-08-17 03:16:34,726] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 746\r\n",
      "[2019-08-17 03:16:34,726] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 820\r\n",
      "[2019-08-17 03:16:34,726] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 779\r\n",
      "[2019-08-17 03:16:34,726] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1175\r\n",
      "[2019-08-17 03:16:34,726] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 786\r\n",
      "[2019-08-17 03:16:34,726] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1053\r\n",
      "[2019-08-17 03:16:34,726] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1117\r\n",
      "[2019-08-17 03:16:34,726] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 967\r\n",
      "[2019-08-17 03:16:34,726] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1114\r\n",
      "[2019-08-17 03:16:34,727] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1049\r\n",
      "[2019-08-17 03:16:34,727] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1067\r\n",
      "[2019-08-17 03:16:34,727] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1038\r\n",
      "[2019-08-17 03:16:34,727] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1085\r\n",
      "[2019-08-17 03:16:34,727] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 852\r\n",
      "[2019-08-17 03:16:34,727] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Removed broadcast_41_piece0 on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 in memory (size: 697.0 B, free: 365.7 MB)\r\n",
      "[2019-08-17 03:16:34,729] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Removed broadcast_41_piece0 on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 in memory (size: 697.0 B, free: 9.4 GB)\r\n",
      "[2019-08-17 03:16:34,733] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 727\r\n",
      "[2019-08-17 03:16:34,733] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 945\r\n",
      "[2019-08-17 03:16:34,734] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1101\r\n",
      "[2019-08-17 03:16:34,734] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1041\r\n",
      "[2019-08-17 03:16:34,734] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1144\r\n",
      "[2019-08-17 03:16:34,734] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1161\r\n",
      "[2019-08-17 03:16:34,734] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1042\r\n",
      "[2019-08-17 03:16:34,734] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 723\r\n",
      "[2019-08-17 03:16:34,734] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 721\r\n",
      "[2019-08-17 03:16:34,734] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1185\r\n",
      "[2019-08-17 03:16:34,735] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 837\r\n",
      "[2019-08-17 03:16:34,735] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 935\r\n",
      "[2019-08-17 03:16:34,735] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 804\r\n",
      "[2019-08-17 03:16:34,735] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 680\r\n",
      "[2019-08-17 03:16:34,735] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 774\r\n",
      "[2019-08-17 03:16:34,735] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1189\r\n",
      "[2019-08-17 03:16:34,735] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 988\r\n",
      "[2019-08-17 03:16:34,735] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1148\r\n",
      "[2019-08-17 03:16:34,735] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 729\r\n",
      "[2019-08-17 03:16:34,735] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 839\r\n",
      "[2019-08-17 03:16:34,735] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1196\r\n",
      "[2019-08-17 03:16:34,735] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 785\r\n",
      "[2019-08-17 03:16:34,735] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 952\r\n",
      "[2019-08-17 03:16:34,735] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1060\r\n",
      "[2019-08-17 03:16:34,736] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 765\r\n",
      "[2019-08-17 03:16:34,736] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1056\r\n",
      "[2019-08-17 03:16:34,736] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 724\r\n",
      "[2019-08-17 03:16:34,736] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1151\r\n",
      "[2019-08-17 03:16:34,736] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1083\r\n",
      "[2019-08-17 03:16:34,736] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1157\r\n",
      "[2019-08-17 03:16:34,736] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 899\r\n",
      "[2019-08-17 03:16:34,736] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 869\r\n",
      "[2019-08-17 03:16:34,736] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 958\r\n",
      "[2019-08-17 03:16:34,736] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 716\r\n",
      "[2019-08-17 03:16:34,736] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 806\r\n",
      "[2019-08-17 03:16:34,736] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1082\r\n",
      "[2019-08-17 03:16:34,736] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 913\r\n",
      "[2019-08-17 03:16:34,736] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned shuffle 17\r\n",
      "[2019-08-17 03:16:34,736] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 972\r\n",
      "[2019-08-17 03:16:34,737] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 986\r\n",
      "[2019-08-17 03:16:34,737] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 691\r\n",
      "[2019-08-17 03:16:34,737] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 956\r\n",
      "[2019-08-17 03:16:34,737] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Removed broadcast_37_piece0 on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 in memory (size: 4.6 KB, free: 365.7 MB)\r\n",
      "[2019-08-17 03:16:34,744] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  TaskSetManager:54 - Finished task 0.0 in stage 46.0 (TID 46) in 60 ms on ip-172-31-29-159.ap-northeast-2.compute.internal (executor 2) (1/1)\r\n",
      "[2019-08-17 03:16:34,744] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  YarnScheduler:54 - Removed TaskSet 46.0, whose tasks have all completed, from pool\r\n",
      "[2019-08-17 03:16:34,745] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  DAGScheduler:54 - ShuffleMapStage 46 (mapPartitions at RandomForest.scala:538) finished in 0.083 s\r\n",
      "[2019-08-17 03:16:34,745] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  DAGScheduler:54 - looking for newly runnable stages\r\n",
      "[2019-08-17 03:16:34,745] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  DAGScheduler:54 - running: Set()\r\n",
      "[2019-08-17 03:16:34,745] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  DAGScheduler:54 - waiting: Set(ResultStage 47)\r\n",
      "[2019-08-17 03:16:34,746] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  DAGScheduler:54 - failed: Set()\r\n",
      "[2019-08-17 03:16:34,746] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  DAGScheduler:54 - Submitting ResultStage 47 (MapPartitionsRDD[121] at map at RandomForest.scala:557), which has no missing parents\r\n",
      "[2019-08-17 03:16:34,746] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  MemoryStore:54 - Block broadcast_58 stored as values in memory (estimated size 15.1 KB, free 363.9 MB)\r\n",
      "[2019-08-17 03:16:34,746] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  MemoryStore:54 - Block broadcast_58_piece0 stored as bytes in memory (estimated size 5.6 KB, free 363.9 MB)\r\n",
      "[2019-08-17 03:16:34,746] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Added broadcast_58_piece0 in memory on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 (size: 5.6 KB, free: 365.7 MB)\r\n",
      "[2019-08-17 03:16:34,746] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  SparkContext:54 - Created broadcast 58 from broadcast at DAGScheduler.scala:1161\r\n",
      "[2019-08-17 03:16:34,746] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 47 (MapPartitionsRDD[121] at map at RandomForest.scala:557) (first 15 tasks are for partitions Vector(0))\r\n",
      "[2019-08-17 03:16:34,746] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  YarnScheduler:54 - Adding task set 47.0 with 1 tasks\r\n",
      "[2019-08-17 03:16:34,746] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  TaskSetManager:54 - Starting task 0.0 in stage 47.0 (TID 47, ip-172-31-29-159.ap-northeast-2.compute.internal, executor 2, partition 0, NODE_LOCAL, 7673 bytes)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-08-17 03:16:34,751] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Removed broadcast_37_piece0 on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 in memory (size: 4.6 KB, free: 9.4 GB)\r\n",
      "[2019-08-17 03:16:34,755] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1030\r\n",
      "[2019-08-17 03:16:34,755] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 970\r\n",
      "[2019-08-17 03:16:34,756] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 741\r\n",
      "[2019-08-17 03:16:34,757] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 821\r\n",
      "[2019-08-17 03:16:34,757] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 819\r\n",
      "[2019-08-17 03:16:34,757] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Removed broadcast_40_piece0 on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 in memory (size: 4.8 KB, free: 365.7 MB)\r\n",
      "[2019-08-17 03:16:34,760] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Added broadcast_58_piece0 in memory on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 (size: 5.6 KB, free: 9.4 GB)\r\n",
      "[2019-08-17 03:16:34,760] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Removed broadcast_40_piece0 on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 in memory (size: 4.8 KB, free: 9.4 GB)\r\n",
      "[2019-08-17 03:16:34,765] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1128\r\n",
      "[2019-08-17 03:16:34,766] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 842\r\n",
      "[2019-08-17 03:16:34,766] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1169\r\n",
      "[2019-08-17 03:16:34,766] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 991\r\n",
      "[2019-08-17 03:16:34,766] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 772\r\n",
      "[2019-08-17 03:16:34,766] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 696\r\n",
      "[2019-08-17 03:16:34,766] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1100\r\n",
      "[2019-08-17 03:16:34,766] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 982\r\n",
      "[2019-08-17 03:16:34,766] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1125\r\n",
      "[2019-08-17 03:16:34,766] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 711\r\n",
      "[2019-08-17 03:16:34,766] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1164\r\n",
      "[2019-08-17 03:16:34,766] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1198\r\n",
      "[2019-08-17 03:16:34,767] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 698\r\n",
      "[2019-08-17 03:16:34,767] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1077\r\n",
      "[2019-08-17 03:16:34,767] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1121\r\n",
      "[2019-08-17 03:16:34,767] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1139\r\n",
      "[2019-08-17 03:16:34,767] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1122\r\n",
      "[2019-08-17 03:16:34,767] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1061\r\n",
      "[2019-08-17 03:16:34,767] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 908\r\n",
      "[2019-08-17 03:16:34,767] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 973\r\n",
      "[2019-08-17 03:16:34,767] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1168\r\n",
      "[2019-08-17 03:16:34,767] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1070\r\n",
      "[2019-08-17 03:16:34,767] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1155\r\n",
      "[2019-08-17 03:16:34,767] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 919\r\n",
      "[2019-08-17 03:16:34,767] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 705\r\n",
      "[2019-08-17 03:16:34,767] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1024\r\n",
      "[2019-08-17 03:16:34,767] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 801\r\n",
      "[2019-08-17 03:16:34,768] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 994\r\n",
      "[2019-08-17 03:16:34,768] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1048\r\n",
      "[2019-08-17 03:16:34,768] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1133\r\n",
      "[2019-08-17 03:16:34,768] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 895\r\n",
      "[2019-08-17 03:16:34,768] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 707\r\n",
      "[2019-08-17 03:16:34,768] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 728\r\n",
      "[2019-08-17 03:16:34,768] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1087\r\n",
      "[2019-08-17 03:16:34,768] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1181\r\n",
      "[2019-08-17 03:16:34,768] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1179\r\n",
      "[2019-08-17 03:16:34,768] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 654\r\n",
      "[2019-08-17 03:16:34,768] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1126\r\n",
      "[2019-08-17 03:16:34,768] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1089\r\n",
      "[2019-08-17 03:16:34,768] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 748\r\n",
      "[2019-08-17 03:16:34,768] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 788\r\n",
      "[2019-08-17 03:16:34,769] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 669\r\n",
      "[2019-08-17 03:16:34,769] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 745\r\n",
      "[2019-08-17 03:16:34,769] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 731\r\n",
      "[2019-08-17 03:16:34,769] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 758\r\n",
      "[2019-08-17 03:16:34,769] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 803\r\n",
      "[2019-08-17 03:16:34,769] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1031\r\n",
      "[2019-08-17 03:16:34,769] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 961\r\n",
      "[2019-08-17 03:16:34,769] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1174\r\n",
      "[2019-08-17 03:16:34,769] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 719\r\n",
      "[2019-08-17 03:16:34,769] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1150\r\n",
      "[2019-08-17 03:16:34,769] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1135\r\n",
      "[2019-08-17 03:16:34,769] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 853\r\n",
      "[2019-08-17 03:16:34,769] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1050\r\n",
      "[2019-08-17 03:16:34,769] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 817\r\n",
      "[2019-08-17 03:16:34,769] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1007\r\n",
      "[2019-08-17 03:16:34,770] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 752\r\n",
      "[2019-08-17 03:16:34,770] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1081\r\n",
      "[2019-08-17 03:16:34,770] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 996\r\n",
      "[2019-08-17 03:16:34,770] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 797\r\n",
      "[2019-08-17 03:16:34,770] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 781\r\n",
      "[2019-08-17 03:16:34,770] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 661\r\n",
      "[2019-08-17 03:16:34,770] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 889\r\n",
      "[2019-08-17 03:16:34,771] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 876\r\n",
      "[2019-08-17 03:16:34,771] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 907\r\n",
      "[2019-08-17 03:16:34,771] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 930\r\n",
      "[2019-08-17 03:16:34,771] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 861\r\n",
      "[2019-08-17 03:16:34,771] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 944\r\n",
      "[2019-08-17 03:16:34,771] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 720\r\n",
      "[2019-08-17 03:16:34,771] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1137\r\n",
      "[2019-08-17 03:16:34,772] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1199\r\n",
      "[2019-08-17 03:16:34,772] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 962\r\n",
      "[2019-08-17 03:16:34,772] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 674\r\n",
      "[2019-08-17 03:16:34,772] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 693\r\n",
      "[2019-08-17 03:16:34,772] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1119\r\n",
      "[2019-08-17 03:16:34,772] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Removed broadcast_45_piece0 on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 in memory (size: 47.0 KB, free: 365.8 MB)\r\n",
      "[2019-08-17 03:16:34,772] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  MapOutputTrackerMasterEndpoint:54 - Asked to send map output locations for shuffle 18 to 172.31.29.159:58394\r\n",
      "[2019-08-17 03:16:34,774] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Removed broadcast_45_piece0 on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 in memory (size: 47.0 KB, free: 9.4 GB)\r\n",
      "[2019-08-17 03:16:34,776] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 873\r\n",
      "[2019-08-17 03:16:34,777] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Removed broadcast_34_piece0 on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 in memory (size: 4.4 KB, free: 365.8 MB)\r\n",
      "[2019-08-17 03:16:34,780] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Removed broadcast_34_piece0 on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 in memory (size: 4.4 KB, free: 9.4 GB)\r\n",
      "[2019-08-17 03:16:34,782] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1086\r\n",
      "[2019-08-17 03:16:34,782] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1051\r\n",
      "[2019-08-17 03:16:34,782] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 990\r\n",
      "[2019-08-17 03:16:34,782] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 868\r\n",
      "[2019-08-17 03:16:34,782] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 666\r\n",
      "[2019-08-17 03:16:34,783] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Removed broadcast_49_piece0 on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 in memory (size: 5.3 KB, free: 365.8 MB)\r\n",
      "[2019-08-17 03:16:34,784] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Removed broadcast_49_piece0 on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 in memory (size: 5.3 KB, free: 9.4 GB)\r\n",
      "[2019-08-17 03:16:34,786] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 701\r\n",
      "[2019-08-17 03:16:34,786] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 725\r\n",
      "[2019-08-17 03:16:34,786] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 923\r\n",
      "[2019-08-17 03:16:34,786] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 898\r\n",
      "[2019-08-17 03:16:34,786] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 844\r\n",
      "[2019-08-17 03:16:34,787] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1044\r\n",
      "[2019-08-17 03:16:34,787] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 904\r\n",
      "[2019-08-17 03:16:34,787] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 737\r\n",
      "[2019-08-17 03:16:34,787] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 863\r\n",
      "[2019-08-17 03:16:34,787] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 718\r\n",
      "[2019-08-17 03:16:34,787] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 738\r\n",
      "[2019-08-17 03:16:34,787] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 650\r\n",
      "[2019-08-17 03:16:34,787] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 814\r\n",
      "[2019-08-17 03:16:34,787] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 890\r\n",
      "[2019-08-17 03:16:34,787] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 893\r\n",
      "[2019-08-17 03:16:34,787] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1036\r\n",
      "[2019-08-17 03:16:34,787] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1069\r\n",
      "[2019-08-17 03:16:34,787] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1186\r\n",
      "[2019-08-17 03:16:34,787] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 794\r\n",
      "[2019-08-17 03:16:34,788] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 747\r\n",
      "[2019-08-17 03:16:34,788] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 656\r\n",
      "[2019-08-17 03:16:34,788] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 668\r\n",
      "[2019-08-17 03:16:34,788] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 840\r\n",
      "[2019-08-17 03:16:34,788] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned shuffle 11\r\n",
      "[2019-08-17 03:16:34,788] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 845\r\n",
      "[2019-08-17 03:16:34,788] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1090\r\n",
      "[2019-08-17 03:16:34,788] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 714\r\n",
      "[2019-08-17 03:16:34,789] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 687\r\n",
      "[2019-08-17 03:16:34,789] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 703\r\n",
      "[2019-08-17 03:16:34,789] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1028\r\n",
      "[2019-08-17 03:16:34,789] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1029\r\n",
      "[2019-08-17 03:16:34,789] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Removed broadcast_53_piece0 on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 in memory (size: 897.0 B, free: 365.8 MB)\r\n",
      "[2019-08-17 03:16:34,791] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Removed broadcast_53_piece0 on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 in memory (size: 897.0 B, free: 9.4 GB)\r\n",
      "[2019-08-17 03:16:34,792] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 851\r\n",
      "[2019-08-17 03:16:34,792] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 858\r\n",
      "[2019-08-17 03:16:34,792] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 662\r\n",
      "[2019-08-17 03:16:34,792] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 751\r\n",
      "[2019-08-17 03:16:34,792] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 929\r\n",
      "[2019-08-17 03:16:34,793] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1143\r\n",
      "[2019-08-17 03:16:34,793] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 743\r\n",
      "[2019-08-17 03:16:34,793] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1004\r\n",
      "[2019-08-17 03:16:34,793] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 697\r\n",
      "[2019-08-17 03:16:34,793] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1006\r\n",
      "[2019-08-17 03:16:34,793] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 983\r\n",
      "[2019-08-17 03:16:34,793] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1099\r\n",
      "[2019-08-17 03:16:34,793] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1171\r\n",
      "[2019-08-17 03:16:34,793] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1079\r\n",
      "[2019-08-17 03:16:34,793] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 677\r\n",
      "[2019-08-17 03:16:34,793] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1113\r\n",
      "[2019-08-17 03:16:34,793] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 854\r\n",
      "[2019-08-17 03:16:34,793] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 993\r\n",
      "[2019-08-17 03:16:34,793] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1140\r\n",
      "[2019-08-17 03:16:34,793] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 655\r\n",
      "[2019-08-17 03:16:34,793] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 872\r\n",
      "[2019-08-17 03:16:34,793] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 791\r\n",
      "[2019-08-17 03:16:34,793] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 866\r\n",
      "[2019-08-17 03:16:34,794] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 857\r\n",
      "[2019-08-17 03:16:34,794] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 749\r\n",
      "[2019-08-17 03:16:34,794] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 776\r\n",
      "[2019-08-17 03:16:34,794] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 856\r\n",
      "[2019-08-17 03:16:34,794] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 921\r\n",
      "[2019-08-17 03:16:34,794] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned shuffle 16\r\n",
      "[2019-08-17 03:16:34,794] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1124\r\n",
      "[2019-08-17 03:16:34,794] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1052\r\n",
      "[2019-08-17 03:16:34,794] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 955\r\n",
      "[2019-08-17 03:16:34,794] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1146\r\n",
      "[2019-08-17 03:16:34,794] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 901\r\n",
      "[2019-08-17 03:16:34,794] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 663\r\n",
      "[2019-08-17 03:16:34,795] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 900\r\n",
      "[2019-08-17 03:16:34,795] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 985\r\n",
      "[2019-08-17 03:16:34,795] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 660\r\n",
      "[2019-08-17 03:16:34,795] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 832\r\n",
      "[2019-08-17 03:16:34,796] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 878\r\n",
      "[2019-08-17 03:16:34,796] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 722\r\n",
      "[2019-08-17 03:16:34,796] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 755\r\n",
      "[2019-08-17 03:16:34,796] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 915\r\n",
      "[2019-08-17 03:16:34,796] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 805\r\n",
      "[2019-08-17 03:16:34,796] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 778\r\n",
      "[2019-08-17 03:16:34,796] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 936\r\n",
      "[2019-08-17 03:16:34,796] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Removed broadcast_42_piece0 on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 in memory (size: 43.9 KB, free: 365.8 MB)\r\n",
      "[2019-08-17 03:16:34,797] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Removed broadcast_42_piece0 on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 in memory (size: 43.9 KB, free: 9.4 GB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-08-17 03:16:34,805] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Removed broadcast_44_piece0 on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 in memory (size: 756.0 B, free: 365.8 MB)\r\n",
      "[2019-08-17 03:16:34,809] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Removed broadcast_44_piece0 on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 in memory (size: 756.0 B, free: 9.4 GB)\r\n",
      "[2019-08-17 03:16:34,810] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1142\r\n",
      "[2019-08-17 03:16:34,810] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 946\r\n",
      "[2019-08-17 03:16:34,810] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 733\r\n",
      "[2019-08-17 03:16:34,810] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 710\r\n",
      "[2019-08-17 03:16:34,810] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 917\r\n",
      "[2019-08-17 03:16:34,811] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1066\r\n",
      "[2019-08-17 03:16:34,811] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 766\r\n",
      "[2019-08-17 03:16:34,811] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 679\r\n",
      "[2019-08-17 03:16:34,811] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 997\r\n",
      "[2019-08-17 03:16:34,811] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1000\r\n",
      "[2019-08-17 03:16:34,811] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 793\r\n",
      "[2019-08-17 03:16:34,811] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 978\r\n",
      "[2019-08-17 03:16:34,811] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 795\r\n",
      "[2019-08-17 03:16:34,811] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1010\r\n",
      "[2019-08-17 03:16:34,811] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 700\r\n",
      "[2019-08-17 03:16:34,811] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 742\r\n",
      "[2019-08-17 03:16:34,811] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 685\r\n",
      "[2019-08-17 03:16:34,811] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 932\r\n",
      "[2019-08-17 03:16:34,811] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 792\r\n",
      "[2019-08-17 03:16:34,811] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1147\r\n",
      "[2019-08-17 03:16:34,811] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 753\r\n",
      "[2019-08-17 03:16:34,811] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1063\r\n",
      "[2019-08-17 03:16:34,811] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 770\r\n",
      "[2019-08-17 03:16:34,811] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 887\r\n",
      "[2019-08-17 03:16:34,811] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1102\r\n",
      "[2019-08-17 03:16:34,812] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1184\r\n",
      "[2019-08-17 03:16:34,812] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 912\r\n",
      "[2019-08-17 03:16:34,812] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 686\r\n",
      "[2019-08-17 03:16:34,812] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 726\r\n",
      "[2019-08-17 03:16:34,812] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 827\r\n",
      "[2019-08-17 03:16:34,812] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 708\r\n",
      "[2019-08-17 03:16:34,812] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 670\r\n",
      "[2019-08-17 03:16:34,812] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 867\r\n",
      "[2019-08-17 03:16:34,812] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 910\r\n",
      "[2019-08-17 03:16:34,812] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1118\r\n",
      "[2019-08-17 03:16:34,812] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1165\r\n",
      "[2019-08-17 03:16:34,813] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Removed broadcast_27_piece0 on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 in memory (size: 32.4 KB, free: 365.9 MB)\r\n",
      "[2019-08-17 03:16:34,819] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Removed broadcast_27_piece0 on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 in memory (size: 32.4 KB, free: 9.4 GB)\r\n",
      "[2019-08-17 03:16:34,824] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 981\r\n",
      "[2019-08-17 03:16:34,824] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 822\r\n",
      "[2019-08-17 03:16:34,824] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1022\r\n",
      "[2019-08-17 03:16:34,824] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 652\r\n",
      "[2019-08-17 03:16:34,825] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 807\r\n",
      "[2019-08-17 03:16:34,825] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1018\r\n",
      "[2019-08-17 03:16:34,825] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 909\r\n",
      "[2019-08-17 03:16:34,825] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 977\r\n",
      "[2019-08-17 03:16:34,825] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Removed broadcast_33_piece0 on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 in memory (size: 36.5 KB, free: 365.9 MB)\r\n",
      "[2019-08-17 03:16:34,829] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Removed broadcast_33_piece0 on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 in memory (size: 36.5 KB, free: 9.4 GB)\r\n",
      "[2019-08-17 03:16:34,831] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Removed broadcast_43_piece0 on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 in memory (size: 5.0 KB, free: 365.9 MB)\r\n",
      "[2019-08-17 03:16:34,833] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Removed broadcast_43_piece0 on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 in memory (size: 5.0 KB, free: 9.4 GB)\r\n",
      "[2019-08-17 03:16:34,835] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 902\r\n",
      "[2019-08-17 03:16:34,835] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1062\r\n",
      "[2019-08-17 03:16:34,835] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1045\r\n",
      "[2019-08-17 03:16:34,835] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1110\r\n",
      "[2019-08-17 03:16:34,835] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1132\r\n",
      "[2019-08-17 03:16:34,835] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 665\r\n",
      "[2019-08-17 03:16:34,835] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1141\r\n",
      "[2019-08-17 03:16:34,836] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 971\r\n",
      "[2019-08-17 03:16:34,836] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 709\r\n",
      "[2019-08-17 03:16:34,836] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 715\r\n",
      "[2019-08-17 03:16:34,836] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 960\r\n",
      "[2019-08-17 03:16:34,836] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1127\r\n",
      "[2019-08-17 03:16:34,836] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 713\r\n",
      "[2019-08-17 03:16:34,836] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 879\r\n",
      "[2019-08-17 03:16:34,836] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1120\r\n",
      "[2019-08-17 03:16:34,836] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1130\r\n",
      "[2019-08-17 03:16:34,836] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 657\r\n",
      "[2019-08-17 03:16:34,836] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 894\r\n",
      "[2019-08-17 03:16:34,836] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1057\r\n",
      "[2019-08-17 03:16:34,836] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1192\r\n",
      "[2019-08-17 03:16:34,836] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 881\r\n",
      "[2019-08-17 03:16:34,836] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1033\r\n",
      "[2019-08-17 03:16:34,837] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Removed broadcast_36_piece0 on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 in memory (size: 37.9 KB, free: 365.9 MB)\r\n",
      "[2019-08-17 03:16:34,842] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Removed broadcast_36_piece0 on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 in memory (size: 37.9 KB, free: 9.4 GB)\r\n",
      "[2019-08-17 03:16:34,844] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1014\r\n",
      "[2019-08-17 03:16:34,844] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1015\r\n",
      "[2019-08-17 03:16:34,844] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1154\r\n",
      "[2019-08-17 03:16:34,844] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 848\r\n",
      "[2019-08-17 03:16:34,844] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1076\r\n",
      "[2019-08-17 03:16:34,844] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 831\r\n",
      "[2019-08-17 03:16:34,844] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1095\r\n",
      "[2019-08-17 03:16:34,844] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1193\r\n",
      "[2019-08-17 03:16:34,844] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1002\r\n",
      "[2019-08-17 03:16:34,844] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1003\r\n",
      "[2019-08-17 03:16:34,844] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 675\r\n",
      "[2019-08-17 03:16:34,845] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 744\r\n",
      "[2019-08-17 03:16:34,845] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 750\r\n",
      "[2019-08-17 03:16:34,845] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 834\r\n",
      "[2019-08-17 03:16:34,845] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1195\r\n",
      "[2019-08-17 03:16:34,845] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 974\r\n",
      "[2019-08-17 03:16:34,845] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Removed broadcast_39_piece0 on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 in memory (size: 40.5 KB, free: 366.0 MB)\r\n",
      "[2019-08-17 03:16:34,848] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Removed broadcast_39_piece0 on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 in memory (size: 40.5 KB, free: 9.4 GB)\r\n",
      "[2019-08-17 03:16:34,849] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 950\r\n",
      "[2019-08-17 03:16:34,849] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 815\r\n",
      "[2019-08-17 03:16:34,850] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1020\r\n",
      "[2019-08-17 03:16:34,850] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 825\r\n",
      "[2019-08-17 03:16:34,850] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 811\r\n",
      "[2019-08-17 03:16:34,850] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Removed broadcast_29_piece0 on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 in memory (size: 394.0 B, free: 366.0 MB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-08-17 03:16:34,852] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Removed broadcast_29_piece0 on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 in memory (size: 394.0 B, free: 9.4 GB)\r\n",
      "[2019-08-17 03:16:34,856] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1096\r\n",
      "[2019-08-17 03:16:34,856] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 891\r\n",
      "[2019-08-17 03:16:34,856] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 992\r\n",
      "[2019-08-17 03:16:34,856] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 769\r\n",
      "[2019-08-17 03:16:34,856] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 838\r\n",
      "[2019-08-17 03:16:34,857] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1111\r\n",
      "[2019-08-17 03:16:34,857] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 826\r\n",
      "[2019-08-17 03:16:34,858] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 931\r\n",
      "[2019-08-17 03:16:34,858] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 941\r\n",
      "[2019-08-17 03:16:34,858] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Removed broadcast_28_piece0 on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 in memory (size: 33.6 KB, free: 366.0 MB)\r\n",
      "[2019-08-17 03:16:34,859] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Removed broadcast_28_piece0 on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 in memory (size: 33.6 KB, free: 9.4 GB)\r\n",
      "[2019-08-17 03:16:34,866] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1136\r\n",
      "[2019-08-17 03:16:34,866] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned shuffle 13\r\n",
      "[2019-08-17 03:16:34,867] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 761\r\n",
      "[2019-08-17 03:16:34,867] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1059\r\n",
      "[2019-08-17 03:16:34,867] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1109\r\n",
      "[2019-08-17 03:16:34,867] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 782\r\n",
      "[2019-08-17 03:16:34,867] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1187\r\n",
      "[2019-08-17 03:16:34,867] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Removed broadcast_46_piece0 on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 in memory (size: 5.1 KB, free: 366.0 MB)\r\n",
      "[2019-08-17 03:16:34,871] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Removed broadcast_46_piece0 on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 in memory (size: 5.1 KB, free: 9.4 GB)\r\n",
      "[2019-08-17 03:16:34,873] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1104\r\n",
      "[2019-08-17 03:16:34,874] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 695\r\n",
      "[2019-08-17 03:16:34,874] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1034\r\n",
      "[2019-08-17 03:16:34,874] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Removed broadcast_47_piece0 on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 in memory (size: 868.0 B, free: 366.0 MB)\r\n",
      "[2019-08-17 03:16:34,877] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Removed broadcast_47_piece0 on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 in memory (size: 868.0 B, free: 9.4 GB)\r\n",
      "[2019-08-17 03:16:34,883] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 924\r\n",
      "[2019-08-17 03:16:34,884] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1039\r\n",
      "[2019-08-17 03:16:34,884] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 918\r\n",
      "[2019-08-17 03:16:34,884] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Removed broadcast_48_piece0 on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 in memory (size: 51.2 KB, free: 366.1 MB)\r\n",
      "[2019-08-17 03:16:34,890] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Removed broadcast_48_piece0 on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 in memory (size: 51.2 KB, free: 9.4 GB)\r\n",
      "[2019-08-17 03:16:34,893] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 651\r\n",
      "[2019-08-17 03:16:34,893] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 667\r\n",
      "[2019-08-17 03:16:34,893] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1064\r\n",
      "[2019-08-17 03:16:34,893] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1153\r\n",
      "[2019-08-17 03:16:34,893] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 933\r\n",
      "[2019-08-17 03:16:34,893] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1043\r\n",
      "[2019-08-17 03:16:34,893] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1180\r\n",
      "[2019-08-17 03:16:34,893] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1172\r\n",
      "[2019-08-17 03:16:34,893] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 810\r\n",
      "[2019-08-17 03:16:34,893] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 916\r\n",
      "[2019-08-17 03:16:34,893] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 903\r\n",
      "[2019-08-17 03:16:34,893] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 928\r\n",
      "[2019-08-17 03:16:34,893] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 763\r\n",
      "[2019-08-17 03:16:34,893] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1046\r\n",
      "[2019-08-17 03:16:34,893] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1058\r\n",
      "[2019-08-17 03:16:34,893] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 824\r\n",
      "[2019-08-17 03:16:34,894] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 963\r\n",
      "[2019-08-17 03:16:34,894] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 841\r\n",
      "[2019-08-17 03:16:34,894] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1197\r\n",
      "[2019-08-17 03:16:34,894] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1108\r\n",
      "[2019-08-17 03:16:34,894] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 882\r\n",
      "[2019-08-17 03:16:34,894] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned shuffle 12\r\n",
      "[2019-08-17 03:16:34,895] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 734\r\n",
      "[2019-08-17 03:16:34,895] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1129\r\n",
      "[2019-08-17 03:16:34,895] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1183\r\n",
      "[2019-08-17 03:16:34,895] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 937\r\n",
      "[2019-08-17 03:16:34,895] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 689\r\n",
      "[2019-08-17 03:16:34,895] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1097\r\n",
      "[2019-08-17 03:16:34,895] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 688\r\n",
      "[2019-08-17 03:16:34,895] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1106\r\n",
      "[2019-08-17 03:16:34,895] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 828\r\n",
      "[2019-08-17 03:16:34,895] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 735\r\n",
      "[2019-08-17 03:16:34,895] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 870\r\n",
      "[2019-08-17 03:16:34,895] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 959\r\n",
      "[2019-08-17 03:16:34,895] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 812\r\n",
      "[2019-08-17 03:16:34,895] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 885\r\n",
      "[2019-08-17 03:16:34,896] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1103\r\n",
      "[2019-08-17 03:16:34,896] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 934\r\n",
      "[2019-08-17 03:16:34,896] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 808\r\n",
      "[2019-08-17 03:16:34,896] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 865\r\n",
      "[2019-08-17 03:16:34,896] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 843\r\n",
      "[2019-08-17 03:16:34,896] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1093\r\n",
      "[2019-08-17 03:16:34,896] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1134\r\n",
      "[2019-08-17 03:16:34,896] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1162\r\n",
      "[2019-08-17 03:16:34,896] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 764\r\n",
      "[2019-08-17 03:16:34,896] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 690\r\n",
      "[2019-08-17 03:16:34,896] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1040\r\n",
      "[2019-08-17 03:16:34,896] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1166\r\n",
      "[2019-08-17 03:16:34,896] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 784\r\n",
      "[2019-08-17 03:16:34,896] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 798\r\n",
      "[2019-08-17 03:16:34,896] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 954\r\n",
      "[2019-08-17 03:16:34,897] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 692\r\n",
      "[2019-08-17 03:16:34,897] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 926\r\n",
      "[2019-08-17 03:16:34,897] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 940\r\n",
      "[2019-08-17 03:16:34,897] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1158\r\n",
      "[2019-08-17 03:16:34,897] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 767\r\n",
      "[2019-08-17 03:16:34,897] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 789\r\n",
      "[2019-08-17 03:16:34,897] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 880\r\n",
      "[2019-08-17 03:16:34,897] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 957\r\n",
      "[2019-08-17 03:16:34,897] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1054\r\n",
      "[2019-08-17 03:16:34,897] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1159\r\n",
      "[2019-08-17 03:16:34,898] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1152\r\n",
      "[2019-08-17 03:16:34,898] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 942\r\n",
      "[2019-08-17 03:16:34,898] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 762\r\n",
      "[2019-08-17 03:16:34,898] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 939\r\n",
      "[2019-08-17 03:16:34,898] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 966\r\n",
      "[2019-08-17 03:16:34,899] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 664\r\n",
      "[2019-08-17 03:16:34,899] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1012\r\n",
      "[2019-08-17 03:16:34,899] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 702\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-08-17 03:16:34,899] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned shuffle 15\r\n",
      "[2019-08-17 03:16:34,899] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 911\r\n",
      "[2019-08-17 03:16:34,899] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1013\r\n",
      "[2019-08-17 03:16:34,906] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1023\r\n",
      "[2019-08-17 03:16:34,906] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1019\r\n",
      "[2019-08-17 03:16:34,906] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 833\r\n",
      "[2019-08-17 03:16:34,907] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 846\r\n",
      "[2019-08-17 03:16:34,907] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1035\r\n",
      "[2019-08-17 03:16:34,907] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1091\r\n",
      "[2019-08-17 03:16:34,907] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1094\r\n",
      "[2019-08-17 03:16:34,907] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 892\r\n",
      "[2019-08-17 03:16:34,907] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Removed broadcast_54_piece0 on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 in memory (size: 60.2 KB, free: 366.1 MB)\r\n",
      "[2019-08-17 03:16:34,907] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  TaskSetManager:54 - Finished task 0.0 in stage 47.0 (TID 47) in 155 ms on ip-172-31-29-159.ap-northeast-2.compute.internal (executor 2) (1/1)\r\n",
      "[2019-08-17 03:16:34,907] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  YarnScheduler:54 - Removed TaskSet 47.0, whose tasks have all completed, from pool\r\n",
      "[2019-08-17 03:16:34,907] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Removed broadcast_54_piece0 on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 in memory (size: 60.2 KB, free: 9.4 GB)\r\n",
      "[2019-08-17 03:16:34,907] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  DAGScheduler:54 - ResultStage 47 (collectAsMap at RandomForest.scala:567) finished in 0.160 s\r\n",
      "[2019-08-17 03:16:34,907] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  DAGScheduler:54 - Job 28 finished: collectAsMap at RandomForest.scala:567, took 0.247099 s\r\n",
      "[2019-08-17 03:16:34,907] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  MapPartitionsRDD:54 - Removing RDD 91 from persistence list\r\n",
      "[2019-08-17 03:16:34,910] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1072\r\n",
      "[2019-08-17 03:16:34,911] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 653\r\n",
      "[2019-08-17 03:16:34,911] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 987\r\n",
      "[2019-08-17 03:16:34,919] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManager:54 - Removing RDD 91\r\n",
      "[2019-08-17 03:16:34,921] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Removed broadcast_32_piece0 on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 in memory (size: 440.0 B, free: 366.1 MB)\r\n",
      "[2019-08-17 03:16:34,922] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Removed broadcast_32_piece0 on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 in memory (size: 440.0 B, free: 9.4 GB)\r\n",
      "[2019-08-17 03:16:34,930] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  RandomForest:54 - Internal timing for DecisionTree:\r\n",
      "[2019-08-17 03:16:34,932] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  RandomForest:54 -   init: 1.521720015\r\n",
      "[2019-08-17 03:16:34,932] {bash_operator.py:127} INFO -   total: 4.438280571\r\n",
      "[2019-08-17 03:16:34,932] {bash_operator.py:127} INFO -   findSplits: 0.542281164\r\n",
      "[2019-08-17 03:16:34,932] {bash_operator.py:127} INFO -   findBestSplits: 2.873129519\r\n",
      "[2019-08-17 03:16:34,932] {bash_operator.py:127} INFO -   chooseSplits: 2.860076597\r\n",
      "[2019-08-17 03:16:34,932] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 683\r\n",
      "[2019-08-17 03:16:34,932] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 759\r\n",
      "[2019-08-17 03:16:34,932] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 671\r\n",
      "[2019-08-17 03:16:34,932] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 947\r\n",
      "[2019-08-17 03:16:34,933] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1068\r\n",
      "[2019-08-17 03:16:34,933] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 818\r\n",
      "[2019-08-17 03:16:34,933] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 773\r\n",
      "[2019-08-17 03:16:34,933] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 948\r\n",
      "[2019-08-17 03:16:34,934] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1176\r\n",
      "[2019-08-17 03:16:34,934] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 835\r\n",
      "[2019-08-17 03:16:34,934] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 706\r\n",
      "[2019-08-17 03:16:34,934] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1005\r\n",
      "[2019-08-17 03:16:34,935] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 897\r\n",
      "[2019-08-17 03:16:34,935] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 953\r\n",
      "[2019-08-17 03:16:34,935] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1011\r\n",
      "[2019-08-17 03:16:34,935] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1182\r\n",
      "[2019-08-17 03:16:34,935] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 757\r\n",
      "[2019-08-17 03:16:34,935] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned shuffle 8\r\n",
      "[2019-08-17 03:16:34,936] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned shuffle 9\r\n",
      "[2019-08-17 03:16:34,936] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 949\r\n",
      "[2019-08-17 03:16:34,936] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1098\r\n",
      "[2019-08-17 03:16:34,936] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 787\r\n",
      "[2019-08-17 03:16:34,936] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1074\r\n",
      "[2019-08-17 03:16:34,936] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1177\r\n",
      "[2019-08-17 03:16:34,936] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 658\r\n",
      "[2019-08-17 03:16:34,936] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1037\r\n",
      "[2019-08-17 03:16:34,936] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1173\r\n",
      "[2019-08-17 03:16:34,936] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Removed broadcast_55_piece0 on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 in memory (size: 5.4 KB, free: 366.1 MB)\r\n",
      "[2019-08-17 03:16:34,936] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Removed broadcast_55_piece0 on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 in memory (size: 5.4 KB, free: 9.4 GB)\r\n",
      "[2019-08-17 03:16:34,939] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 850\r\n",
      "[2019-08-17 03:16:34,939] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 684\r\n",
      "[2019-08-17 03:16:34,939] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1047\r\n",
      "[2019-08-17 03:16:34,939] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 754\r\n",
      "[2019-08-17 03:16:34,940] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 800\r\n",
      "[2019-08-17 03:16:34,940] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 673\r\n",
      "[2019-08-17 03:16:34,940] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 980\r\n",
      "[2019-08-17 03:16:34,940] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 859\r\n",
      "[2019-08-17 03:16:34,940] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 717\r\n",
      "[2019-08-17 03:16:34,940] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 790\r\n",
      "[2019-08-17 03:16:34,940] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 830\r\n",
      "[2019-08-17 03:16:34,940] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1009\r\n",
      "[2019-08-17 03:16:34,940] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1194\r\n",
      "[2019-08-17 03:16:34,940] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1131\r\n",
      "[2019-08-17 03:16:34,940] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1178\r\n",
      "[2019-08-17 03:16:34,940] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 676\r\n",
      "[2019-08-17 03:16:34,941] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 775\r\n",
      "[2019-08-17 03:16:34,941] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1188\r\n",
      "[2019-08-17 03:16:34,941] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 847\r\n",
      "[2019-08-17 03:16:34,941] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 682\r\n",
      "[2019-08-17 03:16:34,941] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 823\r\n",
      "[2019-08-17 03:16:34,941] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 995\r\n",
      "[2019-08-17 03:16:34,941] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1075\r\n",
      "[2019-08-17 03:16:34,941] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Removed broadcast_25_piece0 on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 in memory (size: 31.0 KB, free: 366.2 MB)\r\n",
      "[2019-08-17 03:16:34,941] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Removed broadcast_25_piece0 on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 in memory (size: 31.0 KB, free: 9.4 GB)\r\n",
      "[2019-08-17 03:16:34,942] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Removed broadcast_30_piece0 on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 in memory (size: 35.1 KB, free: 366.2 MB)\r\n",
      "[2019-08-17 03:16:34,944] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Removed broadcast_30_piece0 on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 in memory (size: 35.1 KB, free: 9.4 GB)\r\n",
      "[2019-08-17 03:16:34,945] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 739\r\n",
      "[2019-08-17 03:16:34,945] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 771\r\n",
      "[2019-08-17 03:16:34,946] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Removed broadcast_38_piece0 on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 in memory (size: 612.0 B, free: 366.2 MB)\r\n",
      "[2019-08-17 03:16:34,947] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Removed broadcast_38_piece0 on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 in memory (size: 612.0 B, free: 9.4 GB)\r\n",
      "[2019-08-17 03:16:34,949] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1032\r\n",
      "[2019-08-17 03:16:34,949] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 799\r\n",
      "[2019-08-17 03:16:34,950] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 925\r\n",
      "[2019-08-17 03:16:34,950] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned shuffle 10\r\n",
      "[2019-08-17 03:16:34,950] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 979\r\n",
      "[2019-08-17 03:16:34,950] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 862\r\n",
      "[2019-08-17 03:16:34,950] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1156\r\n",
      "[2019-08-17 03:16:34,950] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 756\r\n",
      "[2019-08-17 03:16:34,950] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 914\r\n",
      "[2019-08-17 03:16:34,950] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 951\r\n",
      "[2019-08-17 03:16:34,950] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Removed broadcast_31_piece0 on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 in memory (size: 3.7 KB, free: 366.2 MB)\r\n",
      "[2019-08-17 03:16:34,951] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Removed broadcast_31_piece0 on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 in memory (size: 3.7 KB, free: 9.4 GB)\r\n",
      "[2019-08-17 03:16:34,952] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 906\r\n",
      "[2019-08-17 03:16:34,952] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 829\r\n",
      "[2019-08-17 03:16:34,952] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1145\r\n",
      "[2019-08-17 03:16:34,953] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 704\r\n",
      "[2019-08-17 03:16:34,953] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 836\r\n",
      "[2019-08-17 03:16:34,953] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 886\r\n",
      "[2019-08-17 03:16:34,953] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1027\r\n",
      "[2019-08-17 03:16:34,953] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1021\r\n",
      "[2019-08-17 03:16:34,953] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1078\r\n",
      "[2019-08-17 03:16:34,953] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 1115\r\n",
      "[2019-08-17 03:16:34,953] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 783\r\n",
      "[2019-08-17 03:16:34,953] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  ContextCleaner:54 - Cleaned accumulator 999\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-08-17 03:16:34,963] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  SparkContext:54 - Starting job: first at RandomForestClassifier.scala:145\n",
      "[2019-08-17 03:16:34,963] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  DAGScheduler:54 - Got job 29 (first at RandomForestClassifier.scala:145) with 1 output partitions\n",
      "[2019-08-17 03:16:34,963] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  DAGScheduler:54 - Final stage: ResultStage 48 (first at RandomForestClassifier.scala:145)\n",
      "[2019-08-17 03:16:34,964] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  DAGScheduler:54 - Parents of final stage: List()\n",
      "[2019-08-17 03:16:34,965] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  DAGScheduler:54 - Missing parents: List()\n",
      "[2019-08-17 03:16:34,965] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  DAGScheduler:54 - Submitting ResultStage 48 (MapPartitionsRDD[83] at map at Classifier.scala:82), which has no missing parents\n",
      "[2019-08-17 03:16:34,966] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  MemoryStore:54 - Block broadcast_59 stored as values in memory (estimated size 74.0 KB, free 365.6 MB)\n",
      "[2019-08-17 03:16:34,967] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  MemoryStore:54 - Block broadcast_59_piece0 stored as bytes in memory (estimated size 30.8 KB, free 365.5 MB)\n",
      "[2019-08-17 03:16:34,968] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Added broadcast_59_piece0 in memory on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 (size: 30.8 KB, free: 366.2 MB)\n",
      "[2019-08-17 03:16:34,968] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  SparkContext:54 - Created broadcast 59 from broadcast at DAGScheduler.scala:1161\n",
      "[2019-08-17 03:16:34,968] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 48 (MapPartitionsRDD[83] at map at Classifier.scala:82) (first 15 tasks are for partitions Vector(0))\n",
      "[2019-08-17 03:16:34,969] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  YarnScheduler:54 - Adding task set 48.0 with 1 tasks\n",
      "[2019-08-17 03:16:34,970] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  TaskSetManager:54 - Starting task 0.0 in stage 48.0 (TID 48, ip-172-31-29-159.ap-northeast-2.compute.internal, executor 2, partition 0, NODE_LOCAL, 8385 bytes)\n",
      "[2019-08-17 03:16:34,978] {bash_operator.py:127} INFO - 2019-08-17 03:16:34 INFO  BlockManagerInfo:54 - Added broadcast_59_piece0 in memory on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 (size: 30.8 KB, free: 9.4 GB)\n",
      "[2019-08-17 03:16:35,199] {bash_operator.py:127} INFO - 2019-08-17 03:16:35 INFO  TaskSetManager:54 - Finished task 0.0 in stage 48.0 (TID 48) in 230 ms on ip-172-31-29-159.ap-northeast-2.compute.internal (executor 2) (1/1)\n",
      "[2019-08-17 03:16:35,199] {bash_operator.py:127} INFO - 2019-08-17 03:16:35 INFO  YarnScheduler:54 - Removed TaskSet 48.0, whose tasks have all completed, from pool\n",
      "[2019-08-17 03:16:35,200] {bash_operator.py:127} INFO - 2019-08-17 03:16:35 INFO  DAGScheduler:54 - ResultStage 48 (first at RandomForestClassifier.scala:145) finished in 0.236 s\n",
      "[2019-08-17 03:16:35,200] {bash_operator.py:127} INFO - 2019-08-17 03:16:35 INFO  DAGScheduler:54 - Job 29 finished: first at RandomForestClassifier.scala:145, took 0.237217 s\n",
      "[2019-08-17 03:16:35,202] {bash_operator.py:127} INFO - 2019-08-17 03:16:35 INFO  Instrumentation:54 - [a07343fc] {\"numClasses\":4}\n",
      "[2019-08-17 03:16:35,202] {bash_operator.py:127} INFO - 2019-08-17 03:16:35 INFO  Instrumentation:54 - [a07343fc] {\"numFeatures\":9}\n",
      "[2019-08-17 03:16:35,203] {bash_operator.py:127} INFO - 2019-08-17 03:16:35 INFO  Instrumentation:54 - [a07343fc] training finished\n",
      "[2019-08-17 03:16:35,224] {bash_operator.py:127} INFO - /models/spark_random_forest_classifier.flight_delays.5.0.bin\n",
      "[2019-08-17 03:16:35,248] {bash_operator.py:127} INFO - 2019-08-17 03:16:35 INFO  FileSystemOverwrite:54 - Path /models/spark_random_forest_classifier.flight_delays.5.0.bin already exists. It will be overwritten.\n",
      "[2019-08-17 03:16:35,258] {bash_operator.py:127} INFO - 2019-08-17 03:16:35 INFO  HadoopMapRedCommitProtocol:54 - Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "[2019-08-17 03:16:35,259] {bash_operator.py:127} INFO - 2019-08-17 03:16:35 INFO  FileOutputCommitter:123 - File Output Committer Algorithm version is 1\n",
      "[2019-08-17 03:16:35,259] {bash_operator.py:127} INFO - 2019-08-17 03:16:35 INFO  FileOutputCommitter:138 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "[2019-08-17 03:16:35,271] {bash_operator.py:127} INFO - 2019-08-17 03:16:35 INFO  SparkContext:54 - Starting job: runJob at SparkHadoopWriter.scala:78\n",
      "[2019-08-17 03:16:35,275] {bash_operator.py:127} INFO - 2019-08-17 03:16:35 INFO  DAGScheduler:54 - Got job 30 (runJob at SparkHadoopWriter.scala:78) with 1 output partitions\n",
      "[2019-08-17 03:16:35,276] {bash_operator.py:127} INFO - 2019-08-17 03:16:35 INFO  DAGScheduler:54 - Final stage: ResultStage 49 (runJob at SparkHadoopWriter.scala:78)\n",
      "[2019-08-17 03:16:35,276] {bash_operator.py:127} INFO - 2019-08-17 03:16:35 INFO  DAGScheduler:54 - Parents of final stage: List()\n",
      "[2019-08-17 03:16:35,276] {bash_operator.py:127} INFO - 2019-08-17 03:16:35 INFO  DAGScheduler:54 - Missing parents: List()\n",
      "[2019-08-17 03:16:35,276] {bash_operator.py:127} INFO - 2019-08-17 03:16:35 INFO  DAGScheduler:54 - Submitting ResultStage 49 (MapPartitionsRDD[123] at saveAsTextFile at ReadWrite.scala:441), which has no missing parents\n",
      "[2019-08-17 03:16:35,282] {bash_operator.py:127} INFO - 2019-08-17 03:16:35 INFO  MemoryStore:54 - Block broadcast_60 stored as values in memory (estimated size 86.1 KB, free 365.4 MB)\n",
      "[2019-08-17 03:16:35,284] {bash_operator.py:127} INFO - 2019-08-17 03:16:35 INFO  MemoryStore:54 - Block broadcast_60_piece0 stored as bytes in memory (estimated size 30.3 KB, free 365.4 MB)\n",
      "[2019-08-17 03:16:35,285] {bash_operator.py:127} INFO - 2019-08-17 03:16:35 INFO  BlockManagerInfo:54 - Added broadcast_60_piece0 in memory on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 (size: 30.3 KB, free: 366.1 MB)\n",
      "[2019-08-17 03:16:35,286] {bash_operator.py:127} INFO - 2019-08-17 03:16:35 INFO  SparkContext:54 - Created broadcast 60 from broadcast at DAGScheduler.scala:1161\n",
      "[2019-08-17 03:16:35,286] {bash_operator.py:127} INFO - 2019-08-17 03:16:35 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 49 (MapPartitionsRDD[123] at saveAsTextFile at ReadWrite.scala:441) (first 15 tasks are for partitions Vector(0))\n",
      "[2019-08-17 03:16:35,286] {bash_operator.py:127} INFO - 2019-08-17 03:16:35 INFO  YarnScheduler:54 - Adding task set 49.0 with 1 tasks\n",
      "[2019-08-17 03:16:35,288] {bash_operator.py:127} INFO - 2019-08-17 03:16:35 INFO  TaskSetManager:54 - Starting task 0.0 in stage 49.0 (TID 49, ip-172-31-18-11.ap-northeast-2.compute.internal, executor 1, partition 0, PROCESS_LOCAL, 8637 bytes)\n",
      "[2019-08-17 03:16:35,304] {bash_operator.py:127} INFO - 2019-08-17 03:16:35 INFO  BlockManagerInfo:54 - Added broadcast_60_piece0 in memory on ip-172-31-18-11.ap-northeast-2.compute.internal:38933 (size: 30.3 KB, free: 9.4 GB)\n",
      "[2019-08-17 03:16:35,361] {bash_operator.py:127} INFO - 2019-08-17 03:16:35 INFO  TaskSetManager:54 - Finished task 0.0 in stage 49.0 (TID 49) in 74 ms on ip-172-31-18-11.ap-northeast-2.compute.internal (executor 1) (1/1)\n",
      "[2019-08-17 03:16:35,361] {bash_operator.py:127} INFO - 2019-08-17 03:16:35 INFO  YarnScheduler:54 - Removed TaskSet 49.0, whose tasks have all completed, from pool\n",
      "[2019-08-17 03:16:35,363] {bash_operator.py:127} INFO - 2019-08-17 03:16:35 INFO  DAGScheduler:54 - ResultStage 49 (runJob at SparkHadoopWriter.scala:78) finished in 0.091 s\n",
      "[2019-08-17 03:16:35,364] {bash_operator.py:127} INFO - 2019-08-17 03:16:35 INFO  DAGScheduler:54 - Job 30 finished: runJob at SparkHadoopWriter.scala:78, took 0.092771 s\n",
      "[2019-08-17 03:16:35,378] {bash_operator.py:127} INFO - 2019-08-17 03:16:35 INFO  SparkHadoopWriter:54 - Job job_20190817031635_0123 committed.\n",
      "[2019-08-17 03:16:35,508] {bash_operator.py:127} INFO - 2019-08-17 03:16:35 INFO  ParquetFileFormat:54 - Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "[2019-08-17 03:16:35,510] {bash_operator.py:127} INFO - 2019-08-17 03:16:35 INFO  FileOutputCommitter:123 - File Output Committer Algorithm version is 1\n",
      "[2019-08-17 03:16:35,510] {bash_operator.py:127} INFO - 2019-08-17 03:16:35 INFO  FileOutputCommitter:138 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "[2019-08-17 03:16:35,511] {bash_operator.py:127} INFO - 2019-08-17 03:16:35 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "[2019-08-17 03:16:35,511] {bash_operator.py:127} INFO - 2019-08-17 03:16:35 INFO  FileOutputCommitter:123 - File Output Committer Algorithm version is 1\n",
      "[2019-08-17 03:16:35,512] {bash_operator.py:127} INFO - 2019-08-17 03:16:35 INFO  FileOutputCommitter:138 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "[2019-08-17 03:16:35,512] {bash_operator.py:127} INFO - 2019-08-17 03:16:35 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-08-17 03:16:35,537] {bash_operator.py:127} INFO - 2019-08-17 03:16:35 INFO  CodeGenerator:54 - Code generated in 17.113774 ms\n",
      "[2019-08-17 03:16:35,561] {bash_operator.py:127} INFO - 2019-08-17 03:16:35 INFO  SparkContext:54 - Starting job: parquet at treeModels.scala:402\n",
      "[2019-08-17 03:16:35,561] {bash_operator.py:127} INFO - 2019-08-17 03:16:35 INFO  DAGScheduler:54 - Got job 31 (parquet at treeModels.scala:402) with 10 output partitions\n",
      "[2019-08-17 03:16:35,562] {bash_operator.py:127} INFO - 2019-08-17 03:16:35 INFO  DAGScheduler:54 - Final stage: ResultStage 50 (parquet at treeModels.scala:402)\n",
      "[2019-08-17 03:16:35,562] {bash_operator.py:127} INFO - 2019-08-17 03:16:35 INFO  DAGScheduler:54 - Parents of final stage: List()\n",
      "[2019-08-17 03:16:35,562] {bash_operator.py:127} INFO - 2019-08-17 03:16:35 INFO  DAGScheduler:54 - Missing parents: List()\n",
      "[2019-08-17 03:16:35,563] {bash_operator.py:127} INFO - 2019-08-17 03:16:35 INFO  DAGScheduler:54 - Submitting ResultStage 50 (MapPartitionsRDD[125] at parquet at treeModels.scala:402), which has no missing parents\n",
      "[2019-08-17 03:16:35,579] {bash_operator.py:127} INFO - 2019-08-17 03:16:35 INFO  MemoryStore:54 - Block broadcast_61 stored as values in memory (estimated size 203.4 KB, free 365.2 MB)\n",
      "[2019-08-17 03:16:35,581] {bash_operator.py:127} INFO - 2019-08-17 03:16:35 INFO  MemoryStore:54 - Block broadcast_61_piece0 stored as bytes in memory (estimated size 69.5 KB, free 365.1 MB)\n",
      "[2019-08-17 03:16:35,581] {bash_operator.py:127} INFO - 2019-08-17 03:16:35 INFO  BlockManagerInfo:54 - Added broadcast_61_piece0 in memory on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 (size: 69.5 KB, free: 366.1 MB)\n",
      "[2019-08-17 03:16:35,582] {bash_operator.py:127} INFO - 2019-08-17 03:16:35 INFO  SparkContext:54 - Created broadcast 61 from broadcast at DAGScheduler.scala:1161\n",
      "[2019-08-17 03:16:35,582] {bash_operator.py:127} INFO - 2019-08-17 03:16:35 INFO  DAGScheduler:54 - Submitting 10 missing tasks from ResultStage 50 (MapPartitionsRDD[125] at parquet at treeModels.scala:402) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))\n",
      "[2019-08-17 03:16:35,582] {bash_operator.py:127} INFO - 2019-08-17 03:16:35 INFO  YarnScheduler:54 - Adding task set 50.0 with 10 tasks\n",
      "[2019-08-17 03:16:35,584] {bash_operator.py:127} INFO - 2019-08-17 03:16:35 INFO  TaskSetManager:54 - Starting task 0.0 in stage 50.0 (TID 50, ip-172-31-18-11.ap-northeast-2.compute.internal, executor 1, partition 0, PROCESS_LOCAL, 8570 bytes)\n",
      "[2019-08-17 03:16:35,584] {bash_operator.py:127} INFO - 2019-08-17 03:16:35 INFO  TaskSetManager:54 - Starting task 1.0 in stage 50.0 (TID 51, ip-172-31-29-159.ap-northeast-2.compute.internal, executor 2, partition 1, PROCESS_LOCAL, 8570 bytes)\n",
      "[2019-08-17 03:16:35,584] {bash_operator.py:127} INFO - 2019-08-17 03:16:35 INFO  TaskSetManager:54 - Starting task 2.0 in stage 50.0 (TID 52, ip-172-31-18-11.ap-northeast-2.compute.internal, executor 1, partition 2, PROCESS_LOCAL, 8570 bytes)\n",
      "[2019-08-17 03:16:35,586] {bash_operator.py:127} INFO - 2019-08-17 03:16:35 INFO  TaskSetManager:54 - Starting task 3.0 in stage 50.0 (TID 53, ip-172-31-29-159.ap-northeast-2.compute.internal, executor 2, partition 3, PROCESS_LOCAL, 8570 bytes)\n",
      "[2019-08-17 03:16:35,586] {bash_operator.py:127} INFO - 2019-08-17 03:16:35 INFO  TaskSetManager:54 - Starting task 4.0 in stage 50.0 (TID 54, ip-172-31-18-11.ap-northeast-2.compute.internal, executor 1, partition 4, PROCESS_LOCAL, 8570 bytes)\n",
      "[2019-08-17 03:16:35,586] {bash_operator.py:127} INFO - 2019-08-17 03:16:35 INFO  TaskSetManager:54 - Starting task 5.0 in stage 50.0 (TID 55, ip-172-31-29-159.ap-northeast-2.compute.internal, executor 2, partition 5, PROCESS_LOCAL, 8570 bytes)\n",
      "[2019-08-17 03:16:35,587] {bash_operator.py:127} INFO - 2019-08-17 03:16:35 INFO  TaskSetManager:54 - Starting task 6.0 in stage 50.0 (TID 56, ip-172-31-18-11.ap-northeast-2.compute.internal, executor 1, partition 6, PROCESS_LOCAL, 8570 bytes)\n",
      "[2019-08-17 03:16:35,587] {bash_operator.py:127} INFO - 2019-08-17 03:16:35 INFO  TaskSetManager:54 - Starting task 7.0 in stage 50.0 (TID 57, ip-172-31-29-159.ap-northeast-2.compute.internal, executor 2, partition 7, PROCESS_LOCAL, 8570 bytes)\n",
      "[2019-08-17 03:16:35,587] {bash_operator.py:127} INFO - 2019-08-17 03:16:35 INFO  TaskSetManager:54 - Starting task 8.0 in stage 50.0 (TID 58, ip-172-31-18-11.ap-northeast-2.compute.internal, executor 1, partition 8, PROCESS_LOCAL, 8570 bytes)\n",
      "[2019-08-17 03:16:35,587] {bash_operator.py:127} INFO - 2019-08-17 03:16:35 INFO  TaskSetManager:54 - Starting task 9.0 in stage 50.0 (TID 59, ip-172-31-29-159.ap-northeast-2.compute.internal, executor 2, partition 9, PROCESS_LOCAL, 8570 bytes)\n",
      "[2019-08-17 03:16:35,599] {bash_operator.py:127} INFO - 2019-08-17 03:16:35 INFO  BlockManagerInfo:54 - Added broadcast_61_piece0 in memory on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 (size: 69.5 KB, free: 9.4 GB)\n",
      "[2019-08-17 03:16:35,684] {bash_operator.py:127} INFO - 2019-08-17 03:16:35 INFO  BlockManagerInfo:54 - Added broadcast_61_piece0 in memory on ip-172-31-18-11.ap-northeast-2.compute.internal:38933 (size: 69.5 KB, free: 9.4 GB)\n",
      "[2019-08-17 03:16:35,753] {bash_operator.py:127} INFO - 2019-08-17 03:16:35 INFO  TaskSetManager:54 - Finished task 3.0 in stage 50.0 (TID 53) in 169 ms on ip-172-31-29-159.ap-northeast-2.compute.internal (executor 2) (1/10)\n",
      "[2019-08-17 03:16:35,769] {bash_operator.py:127} INFO - 2019-08-17 03:16:35 INFO  TaskSetManager:54 - Finished task 7.0 in stage 50.0 (TID 57) in 184 ms on ip-172-31-29-159.ap-northeast-2.compute.internal (executor 2) (2/10)\n",
      "[2019-08-17 03:16:35,770] {bash_operator.py:127} INFO - 2019-08-17 03:16:35 INFO  TaskSetManager:54 - Finished task 9.0 in stage 50.0 (TID 59) in 185 ms on ip-172-31-29-159.ap-northeast-2.compute.internal (executor 2) (3/10)\n",
      "[2019-08-17 03:16:35,772] {bash_operator.py:127} INFO - 2019-08-17 03:16:35 INFO  TaskSetManager:54 - Finished task 1.0 in stage 50.0 (TID 51) in 189 ms on ip-172-31-29-159.ap-northeast-2.compute.internal (executor 2) (4/10)\n",
      "[2019-08-17 03:16:35,780] {bash_operator.py:127} INFO - 2019-08-17 03:16:35 INFO  TaskSetManager:54 - Finished task 5.0 in stage 50.0 (TID 55) in 195 ms on ip-172-31-29-159.ap-northeast-2.compute.internal (executor 2) (5/10)\n",
      "[2019-08-17 03:16:36,395] {bash_operator.py:127} INFO - 2019-08-17 03:16:36 INFO  TaskSetManager:54 - Finished task 0.0 in stage 50.0 (TID 50) in 812 ms on ip-172-31-18-11.ap-northeast-2.compute.internal (executor 1) (6/10)\n",
      "[2019-08-17 03:16:36,396] {bash_operator.py:127} INFO - 2019-08-17 03:16:36 INFO  TaskSetManager:54 - Finished task 8.0 in stage 50.0 (TID 58) in 810 ms on ip-172-31-18-11.ap-northeast-2.compute.internal (executor 1) (7/10)\n",
      "[2019-08-17 03:16:36,397] {bash_operator.py:127} INFO - 2019-08-17 03:16:36 INFO  TaskSetManager:54 - Finished task 2.0 in stage 50.0 (TID 52) in 814 ms on ip-172-31-18-11.ap-northeast-2.compute.internal (executor 1) (8/10)\n",
      "[2019-08-17 03:16:36,398] {bash_operator.py:127} INFO - 2019-08-17 03:16:36 INFO  TaskSetManager:54 - Finished task 6.0 in stage 50.0 (TID 56) in 813 ms on ip-172-31-18-11.ap-northeast-2.compute.internal (executor 1) (9/10)\n",
      "[2019-08-17 03:16:36,398] {bash_operator.py:127} INFO - 2019-08-17 03:16:36 INFO  TaskSetManager:54 - Finished task 4.0 in stage 50.0 (TID 54) in 813 ms on ip-172-31-18-11.ap-northeast-2.compute.internal (executor 1) (10/10)\n",
      "[2019-08-17 03:16:36,398] {bash_operator.py:127} INFO - 2019-08-17 03:16:36 INFO  YarnScheduler:54 - Removed TaskSet 50.0, whose tasks have all completed, from pool\n",
      "[2019-08-17 03:16:36,399] {bash_operator.py:127} INFO - 2019-08-17 03:16:36 INFO  DAGScheduler:54 - ResultStage 50 (parquet at treeModels.scala:402) finished in 0.837 s\n",
      "[2019-08-17 03:16:36,399] {bash_operator.py:127} INFO - 2019-08-17 03:16:36 INFO  DAGScheduler:54 - Job 31 finished: parquet at treeModels.scala:402, took 0.838290 s\n",
      "[2019-08-17 03:16:36,435] {bash_operator.py:127} INFO - 2019-08-17 03:16:36 INFO  FileFormatWriter:54 - Write Job 95116f85-d3e0-46a5-85f2-b7b462995c46 committed.\n",
      "[2019-08-17 03:16:36,436] {bash_operator.py:127} INFO - 2019-08-17 03:16:36 INFO  FileFormatWriter:54 - Finished processing stats for write job 95116f85-d3e0-46a5-85f2-b7b462995c46.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-08-17 03:16:36,777] {bash_operator.py:127} INFO - 2019-08-17 03:16:36 INFO  ParquetFileFormat:54 - Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "[2019-08-17 03:16:36,779] {bash_operator.py:127} INFO - 2019-08-17 03:16:36 INFO  FileOutputCommitter:123 - File Output Committer Algorithm version is 1\n",
      "[2019-08-17 03:16:36,779] {bash_operator.py:127} INFO - 2019-08-17 03:16:36 INFO  FileOutputCommitter:138 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "[2019-08-17 03:16:36,780] {bash_operator.py:127} INFO - 2019-08-17 03:16:36 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "[2019-08-17 03:16:36,780] {bash_operator.py:127} INFO - 2019-08-17 03:16:36 INFO  FileOutputCommitter:123 - File Output Committer Algorithm version is 1\n",
      "[2019-08-17 03:16:36,780] {bash_operator.py:127} INFO - 2019-08-17 03:16:36 INFO  FileOutputCommitter:138 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "[2019-08-17 03:16:36,781] {bash_operator.py:127} INFO - 2019-08-17 03:16:36 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "[2019-08-17 03:16:36,886] {bash_operator.py:127} INFO - 2019-08-17 03:16:36 INFO  CodeGenerator:54 - Code generated in 78.548369 ms\n",
      "[2019-08-17 03:16:36,908] {bash_operator.py:127} INFO - 2019-08-17 03:16:36 INFO  SparkContext:54 - Starting job: parquet at treeModels.scala:407\n",
      "[2019-08-17 03:16:36,910] {bash_operator.py:127} INFO - 2019-08-17 03:16:36 INFO  DAGScheduler:54 - Got job 32 (parquet at treeModels.scala:407) with 10 output partitions\n",
      "[2019-08-17 03:16:36,914] {bash_operator.py:127} INFO - 2019-08-17 03:16:36 INFO  DAGScheduler:54 - Final stage: ResultStage 51 (parquet at treeModels.scala:407)\n",
      "[2019-08-17 03:16:36,914] {bash_operator.py:127} INFO - 2019-08-17 03:16:36 INFO  DAGScheduler:54 - Parents of final stage: List()\n",
      "[2019-08-17 03:16:36,914] {bash_operator.py:127} INFO - 2019-08-17 03:16:36 INFO  DAGScheduler:54 - Missing parents: List()\n",
      "[2019-08-17 03:16:36,914] {bash_operator.py:127} INFO - 2019-08-17 03:16:36 INFO  DAGScheduler:54 - Submitting ResultStage 51 (MapPartitionsRDD[130] at parquet at treeModels.scala:407), which has no missing parents\n",
      "[2019-08-17 03:16:36,935] {bash_operator.py:127} INFO - 2019-08-17 03:16:36 INFO  MemoryStore:54 - Block broadcast_62 stored as values in memory (estimated size 240.2 KB, free 364.9 MB)\n",
      "[2019-08-17 03:16:36,939] {bash_operator.py:127} INFO - 2019-08-17 03:16:36 INFO  MemoryStore:54 - Block broadcast_62_piece0 stored as bytes in memory (estimated size 76.8 KB, free 364.8 MB)\n",
      "[2019-08-17 03:16:36,940] {bash_operator.py:127} INFO - 2019-08-17 03:16:36 INFO  BlockManagerInfo:54 - Added broadcast_62_piece0 in memory on ip-172-31-30-20.ap-northeast-2.compute.internal:34313 (size: 76.8 KB, free: 366.0 MB)\n",
      "[2019-08-17 03:16:36,940] {bash_operator.py:127} INFO - 2019-08-17 03:16:36 INFO  SparkContext:54 - Created broadcast 62 from broadcast at DAGScheduler.scala:1161\n",
      "[2019-08-17 03:16:36,941] {bash_operator.py:127} INFO - 2019-08-17 03:16:36 INFO  DAGScheduler:54 - Submitting 10 missing tasks from ResultStage 51 (MapPartitionsRDD[130] at parquet at treeModels.scala:407) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))\n",
      "[2019-08-17 03:16:36,941] {bash_operator.py:127} INFO - 2019-08-17 03:16:36 INFO  YarnScheduler:54 - Adding task set 51.0 with 10 tasks\n",
      "[2019-08-17 03:16:36,953] {bash_operator.py:127} INFO - 2019-08-17 03:16:36 INFO  TaskSetManager:54 - Starting task 0.0 in stage 51.0 (TID 60, ip-172-31-18-11.ap-northeast-2.compute.internal, executor 1, partition 0, PROCESS_LOCAL, 13697 bytes)\n",
      "[2019-08-17 03:16:36,955] {bash_operator.py:127} INFO - 2019-08-17 03:16:36 INFO  TaskSetManager:54 - Starting task 1.0 in stage 51.0 (TID 61, ip-172-31-29-159.ap-northeast-2.compute.internal, executor 2, partition 1, PROCESS_LOCAL, 16699 bytes)\n",
      "[2019-08-17 03:16:36,956] {bash_operator.py:127} INFO - 2019-08-17 03:16:36 INFO  TaskSetManager:54 - Starting task 2.0 in stage 51.0 (TID 62, ip-172-31-18-11.ap-northeast-2.compute.internal, executor 1, partition 2, PROCESS_LOCAL, 25537 bytes)\n",
      "[2019-08-17 03:16:36,957] {bash_operator.py:127} INFO - 2019-08-17 03:16:36 INFO  TaskSetManager:54 - Starting task 3.0 in stage 51.0 (TID 63, ip-172-31-29-159.ap-northeast-2.compute.internal, executor 2, partition 3, PROCESS_LOCAL, 27637 bytes)\n",
      "[2019-08-17 03:16:36,958] {bash_operator.py:127} INFO - 2019-08-17 03:16:36 INFO  TaskSetManager:54 - Starting task 4.0 in stage 51.0 (TID 64, ip-172-31-18-11.ap-northeast-2.compute.internal, executor 1, partition 4, PROCESS_LOCAL, 19622 bytes)\n",
      "[2019-08-17 03:16:36,960] {bash_operator.py:127} INFO - 2019-08-17 03:16:36 INFO  TaskSetManager:54 - Starting task 5.0 in stage 51.0 (TID 65, ip-172-31-29-159.ap-northeast-2.compute.internal, executor 2, partition 5, PROCESS_LOCAL, 25066 bytes)\n",
      "[2019-08-17 03:16:36,961] {bash_operator.py:127} INFO - 2019-08-17 03:16:36 INFO  TaskSetManager:54 - Starting task 6.0 in stage 51.0 (TID 66, ip-172-31-18-11.ap-northeast-2.compute.internal, executor 1, partition 6, PROCESS_LOCAL, 28061 bytes)\n",
      "[2019-08-17 03:16:36,962] {bash_operator.py:127} INFO - 2019-08-17 03:16:36 INFO  TaskSetManager:54 - Starting task 7.0 in stage 51.0 (TID 67, ip-172-31-29-159.ap-northeast-2.compute.internal, executor 2, partition 7, PROCESS_LOCAL, 14123 bytes)\n",
      "[2019-08-17 03:16:36,963] {bash_operator.py:127} INFO - 2019-08-17 03:16:36 INFO  TaskSetManager:54 - Starting task 8.0 in stage 51.0 (TID 68, ip-172-31-18-11.ap-northeast-2.compute.internal, executor 1, partition 8, PROCESS_LOCAL, 26303 bytes)\n",
      "[2019-08-17 03:16:36,964] {bash_operator.py:127} INFO - 2019-08-17 03:16:36 INFO  TaskSetManager:54 - Starting task 9.0 in stage 51.0 (TID 69, ip-172-31-29-159.ap-northeast-2.compute.internal, executor 2, partition 9, PROCESS_LOCAL, 28918 bytes)\n",
      "[2019-08-17 03:16:37,009] {bash_operator.py:127} INFO - 2019-08-17 03:16:37 INFO  BlockManagerInfo:54 - Added broadcast_62_piece0 in memory on ip-172-31-29-159.ap-northeast-2.compute.internal:35703 (size: 76.8 KB, free: 9.4 GB)\n",
      "[2019-08-17 03:16:37,043] {bash_operator.py:127} INFO - 2019-08-17 03:16:37 INFO  BlockManagerInfo:54 - Added broadcast_62_piece0 in memory on ip-172-31-18-11.ap-northeast-2.compute.internal:38933 (size: 76.8 KB, free: 9.4 GB)\n",
      "[2019-08-17 03:16:37,331] {bash_operator.py:127} INFO - 2019-08-17 03:16:37 INFO  TaskSetManager:54 - Finished task 5.0 in stage 51.0 (TID 65) in 371 ms on ip-172-31-29-159.ap-northeast-2.compute.internal (executor 2) (1/10)\n",
      "[2019-08-17 03:16:37,333] {bash_operator.py:127} INFO - 2019-08-17 03:16:37 INFO  TaskSetManager:54 - Finished task 7.0 in stage 51.0 (TID 67) in 372 ms on ip-172-31-29-159.ap-northeast-2.compute.internal (executor 2) (2/10)\n",
      "[2019-08-17 03:16:37,350] {bash_operator.py:127} INFO - 2019-08-17 03:16:37 INFO  TaskSetManager:54 - Finished task 1.0 in stage 51.0 (TID 61) in 395 ms on ip-172-31-29-159.ap-northeast-2.compute.internal (executor 2) (3/10)\n",
      "[2019-08-17 03:16:37,351] {bash_operator.py:127} INFO - 2019-08-17 03:16:37 INFO  TaskSetManager:54 - Finished task 9.0 in stage 51.0 (TID 69) in 387 ms on ip-172-31-29-159.ap-northeast-2.compute.internal (executor 2) (4/10)\n",
      "[2019-08-17 03:16:37,356] {bash_operator.py:127} INFO - 2019-08-17 03:16:37 INFO  TaskSetManager:54 - Finished task 3.0 in stage 51.0 (TID 63) in 400 ms on ip-172-31-29-159.ap-northeast-2.compute.internal (executor 2) (5/10)\n",
      "[2019-08-17 03:16:37,835] {bash_operator.py:127} INFO - 2019-08-17 03:16:37 INFO  TaskSetManager:54 - Finished task 0.0 in stage 51.0 (TID 60) in 891 ms on ip-172-31-18-11.ap-northeast-2.compute.internal (executor 1) (6/10)\n",
      "[2019-08-17 03:16:37,873] {bash_operator.py:127} INFO - 2019-08-17 03:16:37 INFO  TaskSetManager:54 - Finished task 4.0 in stage 51.0 (TID 64) in 915 ms on ip-172-31-18-11.ap-northeast-2.compute.internal (executor 1) (7/10)\n",
      "[2019-08-17 03:16:37,913] {bash_operator.py:127} INFO - 2019-08-17 03:16:37 INFO  TaskSetManager:54 - Finished task 6.0 in stage 51.0 (TID 66) in 953 ms on ip-172-31-18-11.ap-northeast-2.compute.internal (executor 1) (8/10)\n",
      "[2019-08-17 03:16:37,939] {bash_operator.py:127} INFO - 2019-08-17 03:16:37 INFO  TaskSetManager:54 - Finished task 2.0 in stage 51.0 (TID 62) in 983 ms on ip-172-31-18-11.ap-northeast-2.compute.internal (executor 1) (9/10)\n",
      "[2019-08-17 03:16:37,943] {bash_operator.py:127} INFO - 2019-08-17 03:16:37 INFO  TaskSetManager:54 - Finished task 8.0 in stage 51.0 (TID 68) in 980 ms on ip-172-31-18-11.ap-northeast-2.compute.internal (executor 1) (10/10)\n",
      "[2019-08-17 03:16:37,943] {bash_operator.py:127} INFO - 2019-08-17 03:16:37 INFO  YarnScheduler:54 - Removed TaskSet 51.0, whose tasks have all completed, from pool\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-08-17 03:16:37,944] {bash_operator.py:127} INFO - 2019-08-17 03:16:37 INFO  DAGScheduler:54 - ResultStage 51 (parquet at treeModels.scala:407) finished in 1.029 s\n",
      "[2019-08-17 03:16:37,944] {bash_operator.py:127} INFO - 2019-08-17 03:16:37 INFO  DAGScheduler:54 - Job 32 finished: parquet at treeModels.scala:407, took 1.035364 s\n",
      "[2019-08-17 03:16:37,981] {bash_operator.py:127} INFO - 2019-08-17 03:16:37 INFO  FileFormatWriter:54 - Write Job ffa2811e-25e8-4eb4-8b2f-2f0f3dccc208 committed.\n",
      "[2019-08-17 03:16:37,983] {bash_operator.py:127} INFO - 2019-08-17 03:16:37 INFO  FileFormatWriter:54 - Finished processing stats for write job ffa2811e-25e8-4eb4-8b2f-2f0f3dccc208.\n",
      "[2019-08-17 03:16:38,086] {bash_operator.py:127} INFO - 2019-08-17 03:16:38 INFO  SparkContext:54 - Invoking stop() from shutdown hook\n",
      "[2019-08-17 03:16:38,092] {bash_operator.py:127} INFO - 2019-08-17 03:16:38 INFO  AbstractConnector:318 - Stopped Spark@73b2dc29{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n",
      "[2019-08-17 03:16:38,095] {bash_operator.py:127} INFO - 2019-08-17 03:16:38 INFO  SparkUI:54 - Stopped Spark web UI at http://ip-172-31-30-20.ap-northeast-2.compute.internal:4040\n",
      "[2019-08-17 03:16:38,100] {bash_operator.py:127} INFO - 2019-08-17 03:16:38 INFO  YarnClientSchedulerBackend:54 - Interrupting monitor thread\n",
      "[2019-08-17 03:16:38,119] {bash_operator.py:127} INFO - 2019-08-17 03:16:38 INFO  YarnClientSchedulerBackend:54 - Shutting down all executors\n",
      "[2019-08-17 03:16:38,120] {bash_operator.py:127} INFO - 2019-08-17 03:16:38 INFO  YarnSchedulerBackend$YarnDriverEndpoint:54 - Asking each executor to shut down\n",
      "[2019-08-17 03:16:38,124] {bash_operator.py:127} INFO - 2019-08-17 03:16:38 INFO  SchedulerExtensionServices:54 - Stopping SchedulerExtensionServices\n",
      "[2019-08-17 03:16:38,124] {bash_operator.py:127} INFO - (serviceOption=None,\n",
      "[2019-08-17 03:16:38,124] {bash_operator.py:127} INFO -  services=List(),\n",
      "[2019-08-17 03:16:38,124] {bash_operator.py:127} INFO -  started=false)\n",
      "[2019-08-17 03:16:38,126] {bash_operator.py:127} INFO - 2019-08-17 03:16:38 INFO  YarnClientSchedulerBackend:54 - Stopped\n",
      "[2019-08-17 03:16:38,134] {bash_operator.py:127} INFO - 2019-08-17 03:16:38 INFO  MapOutputTrackerMasterEndpoint:54 - MapOutputTrackerMasterEndpoint stopped!\n",
      "[2019-08-17 03:16:38,164] {bash_operator.py:127} INFO - 2019-08-17 03:16:38 INFO  MemoryStore:54 - MemoryStore cleared\n",
      "[2019-08-17 03:16:38,164] {bash_operator.py:127} INFO - 2019-08-17 03:16:38 INFO  BlockManager:54 - BlockManager stopped\n",
      "[2019-08-17 03:16:38,175] {bash_operator.py:127} INFO - 2019-08-17 03:16:38 INFO  BlockManagerMaster:54 - BlockManagerMaster stopped\n",
      "[2019-08-17 03:16:38,179] {bash_operator.py:127} INFO - 2019-08-17 03:16:38 INFO  OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54 - OutputCommitCoordinator stopped!\n",
      "[2019-08-17 03:16:38,187] {bash_operator.py:127} INFO - 2019-08-17 03:16:38 INFO  SparkContext:54 - Successfully stopped SparkContext\n",
      "[2019-08-17 03:16:38,187] {bash_operator.py:127} INFO - 2019-08-17 03:16:38 INFO  ShutdownHookManager:54 - Shutdown hook called\n",
      "[2019-08-17 03:16:38,188] {bash_operator.py:127} INFO - 2019-08-17 03:16:38 INFO  ShutdownHookManager:54 - Deleting directory /tmp/spark-6b419b56-5731-41fc-b728-a5d6d14382a9\n",
      "[2019-08-17 03:16:38,191] {bash_operator.py:127} INFO - 2019-08-17 03:16:38 INFO  ShutdownHookManager:54 - Deleting directory /tmp/spark-6b419b56-5731-41fc-b728-a5d6d14382a9/pyspark-18354a3e-40ac-4bc7-b2a6-40a6ad9e2147\n",
      "[2019-08-17 03:16:38,193] {bash_operator.py:127} INFO - 2019-08-17 03:16:38 INFO  ShutdownHookManager:54 - Deleting directory /tmp/spark-6c0721c3-f787-48ef-90e1-3ec94cde547a\n",
      "[2019-08-17 03:16:38,544] {bash_operator.py:131} INFO - Command exited with return code 0\n"
     ]
    }
   ],
   "source": [
    "!airflow test DAG_model_training training_model 2019-08-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.6/site-packages/airflow/configuration.py:590: DeprecationWarning: You have two airflow.cfg files: /home/ubuntu/airflow/airflow.cfg and /home/ubuntu/project1/airflow/airflow.cfg. Airflow used to look at ~/airflow/airflow.cfg, even when AIRFLOW_HOME was set to a different value. Airflow will now only read /home/ubuntu/project1/airflow/airflow.cfg, and you should remove the other file\n",
      "  category=DeprecationWarning,\n",
      "[2019-08-08 09:49:35,751] {__init__.py:51} INFO - Using executor SequentialExecutor\n",
      "[2019-08-08 09:49:35,971] {__init__.py:305} INFO - Filling up the DagBag from /home/ubuntu/project1/airflow/dags\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:387: DeprecationWarning: Importing 'SSHHook' directly from 'airflow.contrib.hooks' has been deprecated. Please import from 'airflow.contrib.hooks.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.\n",
      "  DeprecationWarning)\n",
      "[2019-08-08 09:49:36,031] {__init__.py:1139} INFO - Dependencies all met for <TaskInstance: DAG_model_training.trained_model_hdfs_to_s3 2019-08-01T00:00:00+00:00 [None]>\n",
      "[2019-08-08 09:49:36,035] {__init__.py:1139} INFO - Dependencies all met for <TaskInstance: DAG_model_training.trained_model_hdfs_to_s3 2019-08-01T00:00:00+00:00 [None]>\n",
      "[2019-08-08 09:49:36,035] {__init__.py:1353} INFO - \n",
      "--------------------------------------------------------------------------------\n",
      "[2019-08-08 09:49:36,036] {__init__.py:1354} INFO - Starting attempt 1 of 1\n",
      "[2019-08-08 09:49:36,036] {__init__.py:1355} INFO - \n",
      "--------------------------------------------------------------------------------\n",
      "[2019-08-08 09:49:36,036] {__init__.py:1374} INFO - Executing <Task(SSHOperator): trained_model_hdfs_to_s3> on 2019-08-01T00:00:00+00:00\n",
      "[2019-08-08 09:49:36,053] {ssh_hook.py:154} WARNING - Remote Identification Change is not verified. This wont protect against Man-In-The-Middle attacks\n",
      "[2019-08-08 09:49:36,054] {ssh_hook.py:158} WARNING - No Host Key Verification. This wont protect against Man-In-The-Middle attacks\n",
      "[2019-08-08 09:49:36,064] {transport.py:1819} INFO - Connected (version 2.0, client OpenSSH_7.4)\n",
      "[2019-08-08 09:49:36,188] {transport.py:1819} INFO - Authentication (password) successful!\n",
      "[2019-08-08 09:49:36,834] {ssh_operator.py:138} WARNING - 19/08/08 09:49:36 INFO s3distcp.S3DistCp: Running with args: -libjars /usr/share/aws/emr/s3-dist-cp/lib/commons-httpclient-3.1.jar,/usr/share/aws/emr/s3-dist-cp/lib/commons-logging-1.0.4.jar,/usr/share/aws/emr/s3-dist-cp/lib/guava-18.0.jar,/usr/share/aws/emr/s3-dist-cp/lib/s3-dist-cp-2.11.0.jar,/usr/share/aws/emr/s3-dist-cp/lib/s3-dist-cp.jar --src hdfs://ip-172-31-20-77.ap-northeast-2.compute.internal:8020/models/ --dest s3://jhw620/models/2019-07-24/ \n",
      "[2019-08-08 09:49:37,871] {ssh_operator.py:138} WARNING - 19/08/08 09:49:37 INFO s3distcp.S3DistCp: S3DistCp args: --src hdfs://ip-172-31-20-77.ap-northeast-2.compute.internal:8020/models/ --dest s3://jhw620/models/2019-07-24/ \n",
      "[2019-08-08 09:49:37,880] {ssh_operator.py:138} WARNING - 19/08/08 09:49:37 INFO s3distcp.S3DistCp: Using output path 'hdfs:/tmp/ee7270da-36a5-4f8b-9a7e-f8e58fb85725/output'\n",
      "[2019-08-08 09:49:37,960] {ssh_operator.py:138} WARNING - 19/08/08 09:49:37 INFO s3distcp.S3DistCp: GET http://169.254.169.254/latest/meta-data/placement/availability-zone result: ap-northeast-2c\n",
      "[2019-08-08 09:49:38,550] {ssh_operator.py:138} WARNING - 19/08/08 09:49:38 INFO s3distcp.FileInfoListing: Opening new file: hdfs:/tmp/ee7270da-36a5-4f8b-9a7e-f8e58fb85725/files/1\n",
      "[2019-08-08 09:49:38,637] {ssh_operator.py:138} WARNING - 19/08/08 09:49:38 INFO s3distcp.S3DistCp: Created 1 files to copy 44 files \n",
      "[2019-08-08 09:49:41,584] {ssh_operator.py:138} WARNING - 19/08/08 09:49:41 INFO s3distcp.S3DistCp: Reducer number: 7\n",
      "[2019-08-08 09:49:41,746] {ssh_operator.py:138} WARNING - 19/08/08 09:49:41 INFO client.RMProxy: Connecting to ResourceManager at ip-172-31-18-159.ap-northeast-2.compute.internal/172.31.18.159:8032\n",
      "[2019-08-08 09:49:42,124] {ssh_operator.py:138} WARNING - 19/08/08 09:49:42 INFO input.FileInputFormat: Total input files to process : 1\n",
      "[2019-08-08 09:49:42,150] {ssh_operator.py:138} WARNING - 19/08/08 09:49:42 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "[2019-08-08 09:49:42,280] {ssh_operator.py:138} WARNING - 19/08/08 09:49:42 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1565248245831_0020\n",
      "[2019-08-08 09:49:42,502] {ssh_operator.py:138} WARNING - 19/08/08 09:49:42 INFO impl.YarnClientImpl: Submitted application application_1565248245831_0020\n",
      "[2019-08-08 09:49:42,564] {ssh_operator.py:138} WARNING - 19/08/08 09:49:42 INFO mapreduce.Job: The url to track the job: http://ip-172-31-18-159.ap-northeast-2.compute.internal:20888/proxy/application_1565248245831_0020/\n",
      "[2019-08-08 09:49:42,564] {ssh_operator.py:138} WARNING - 19/08/08 09:49:42 INFO mapreduce.Job: Running job: job_1565248245831_0020\n",
      "[2019-08-08 09:49:49,654] {ssh_operator.py:138} WARNING - 19/08/08 09:49:49 INFO mapreduce.Job: Job job_1565248245831_0020 running in uber mode : false\n",
      "[2019-08-08 09:49:49,655] {ssh_operator.py:138} WARNING - 19/08/08 09:49:49 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "[2019-08-08 09:49:54,698] {ssh_operator.py:138} WARNING - 19/08/08 09:49:54 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "[2019-08-08 09:50:07,775] {ssh_operator.py:138} WARNING - 19/08/08 09:50:07 INFO mapreduce.Job:  map 100% reduce 14%\n",
      "[2019-08-08 09:50:09,787] {ssh_operator.py:138} WARNING - 19/08/08 09:50:09 INFO mapreduce.Job:  map 100% reduce 29%\n",
      "[2019-08-08 09:50:10,791] {ssh_operator.py:138} WARNING - 19/08/08 09:50:10 INFO mapreduce.Job:  map 100% reduce 57%\n",
      "[2019-08-08 09:50:13,803] {ssh_operator.py:138} WARNING - 19/08/08 09:50:13 INFO mapreduce.Job:  map 100% reduce 71%\n",
      "[2019-08-08 09:50:14,810] {ssh_operator.py:138} WARNING - 19/08/08 09:50:14 INFO mapreduce.Job:  map 100% reduce 86%\n",
      "[2019-08-08 09:50:15,814] {ssh_operator.py:138} WARNING - 19/08/08 09:50:15 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "[2019-08-08 09:50:15,820] {ssh_operator.py:138} WARNING - 19/08/08 09:50:15 INFO mapreduce.Job: Job job_1565248245831_0020 completed successfully\n",
      "[2019-08-08 09:50:15,933] {ssh_operator.py:138} WARNING - 19/08/08 09:50:15 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=3726\n",
      "\t\tFILE: Number of bytes written=1411367\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=130199\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of read operations=69\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=14\n",
      "\t\tS3: Number of bytes read=0\n",
      "\t\tS3: Number of bytes written=115893\n",
      "\t\tS3: Number of read operations=0\n",
      "\t\tS3: Number of large read operations=0\n",
      "\t\tS3: Number of write operations=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=1\n",
      "\t\tLaunched reduce tasks=7\n",
      "\t\tRack-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=295412\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=16344279\n",
      "\t\tTotal time spent by all map tasks (ms)=3211\n",
      "\t\tTotal time spent by all reduce tasks (ms)=89313\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=3211\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=89313\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=9401808\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=523016928\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=44\n",
      "\t\tMap output records=44\n",
      "\t\tMap output bytes=17663\n",
      "\t\tMap output materialized bytes=3698\n",
      "\t\tInput split bytes=174\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=44\n",
      "\t\tReduce shuffle bytes=3698\n",
      "\t\tReduce input records=44\n",
      "\t\tReduce output records=0\n",
      "\t\tSpilled Records=88\n",
      "\t\tShuffled Maps =7\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=7\n",
      "\t\tGC time elapsed (ms)=2416\n",
      "\t\tCPU time spent (ms)=64200\n",
      "\t\tPhysical memory (bytes) snapshot=3701633024\n",
      "\t\tVirtual memory (bytes) snapshot=54332391424\n",
      "\t\tTotal committed heap usage (bytes)=4794613760\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=14132\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=0\n",
      "[2019-08-08 09:50:15,934] {ssh_operator.py:138} WARNING - 19/08/08 09:50:15 INFO s3distcp.S3DistCp: Try to recursively delete hdfs:/tmp/ee7270da-36a5-4f8b-9a7e-f8e58fb85725\n"
     ]
    }
   ],
   "source": [
    "!airflow test DAG_model_training trained_model_hdfs_to_s3 2019-08-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-14T03:37:55.427508Z",
     "start_time": "2022-07-14T03:37:52.827260Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.6/site-packages/airflow/configuration.py:590: DeprecationWarning: You have two airflow.cfg files: /home/ubuntu/airflow/airflow.cfg and /home/ubuntu/project1/airflow/airflow.cfg. Airflow used to look at ~/airflow/airflow.cfg, even when AIRFLOW_HOME was set to a different value. Airflow will now only read /home/ubuntu/project1/airflow/airflow.cfg, and you should remove the other file\n",
      "  category=DeprecationWarning,\n",
      "[2022-07-14 03:37:53,648] {__init__.py:51} INFO - Using executor SequentialExecutor\n",
      "[2022-07-14 03:37:53,865] {__init__.py:305} INFO - Filling up the DagBag from /home/ubuntu/project1/airflow/dags\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:387: DeprecationWarning: Importing 'SSHHook' directly from 'airflow.contrib.hooks' has been deprecated. Please import from 'airflow.contrib.hooks.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.\n",
      "  DeprecationWarning)\n",
      "[2022-07-14 03:37:53,920] {__init__.py:1139} INFO - Dependencies all met for <TaskInstance: DAG_model_training.trained_model_s3_to_server 2019-08-01T00:00:00+00:00 [success]>\n",
      "[2022-07-14 03:37:53,924] {__init__.py:1139} INFO - Dependencies all met for <TaskInstance: DAG_model_training.trained_model_s3_to_server 2019-08-01T00:00:00+00:00 [success]>\n",
      "[2022-07-14 03:37:53,924] {__init__.py:1353} INFO - \n",
      "--------------------------------------------------------------------------------\n",
      "[2022-07-14 03:37:53,924] {__init__.py:1354} INFO - Starting attempt 3 of 2\n",
      "[2022-07-14 03:37:53,924] {__init__.py:1355} INFO - \n",
      "--------------------------------------------------------------------------------\n",
      "[2022-07-14 03:37:53,924] {__init__.py:1374} INFO - Executing <Task(BashOperator): trained_model_s3_to_server> on 2019-08-01T00:00:00+00:00\n",
      "[2022-07-14 03:37:53,939] {bash_operator.py:81} INFO - Tmp dir root location: \n",
      " /tmp\n",
      "[2022-07-14 03:37:53,940] {bash_operator.py:90} INFO - Exporting the following env vars:\n",
      "AIRFLOW_CTX_DAG_ID=DAG_model_training\n",
      "AIRFLOW_CTX_TASK_ID=trained_model_s3_to_server\n",
      "AIRFLOW_CTX_EXECUTION_DATE=2019-08-01T00:00:00+00:00\n",
      "AIRFLOW_CTX_DAG_RUN_ID=backfill_2019-08-01T00:00:00+00:00\n",
      "[2022-07-14 03:37:53,940] {bash_operator.py:104} INFO - Temporary script location: /tmp/airflowtmpdclwk7gl/trained_model_s3_to_serverh2iam372\n",
      "[2022-07-14 03:37:53,940] {bash_operator.py:114} INFO - Running command: aws s3 cp s3://jhw620/models/2019-07-24/ ~/Project/03_PredictionModel_RealTime_Processing/models1/ --recursive\n",
      "[2022-07-14 03:37:53,945] {bash_operator.py:123} INFO - Output:\n",
      "[2022-07-14 03:37:54,620] {bash_operator.py:127} INFO - download: s3://jhw620/models/2019-07-24/arrival_bucketizer_2.0.bin/metadata/_SUCCESS to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/arrival_bucketizer_2.0.bin/metadata/_SUCCESS\n",
      "download: s3://jhw620/models/2019-07-24/spark_random_forest_classifier.flight_delays.5.0.bin/data/part-00000-b76c5248-8ab2-4f73-b19f-7370b11fe1e7-c000.snappy.parquet to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/spark_random_forest_classifier.flight_delays.5.0.bin/data/part-00000-b76c5248-8ab2-4f73-b19f-7370b11fe1e7-c000.snappy.parquet\n",
      "download: s3://jhw620/models/2019-07-24/numeric_vector_assembler.bin/metadata/_SUCCESS to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/numeric_vector_assembler.bin/metadata/_SUCCESS\n",
      "download: s3://jhw620/models/2019-07-24/arrival_bucketizer_2.0.bin/metadata/part-00000 to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/arrival_bucketizer_2.0.bin/metadata/part-00000\n",
      "download: s3://jhw620/models/2019-07-24/spark_random_forest_classifier.flight_delays.5.0.bin/data/_SUCCESS to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/spark_random_forest_classifier.flight_delays.5.0.bin/data/_SUCCESS\n",
      "download: s3://jhw620/models/2019-07-24/numeric_vector_assembler.bin/metadata/part-00000 to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/numeric_vector_assembler.bin/metadata/part-00000\n",
      "download: s3://jhw620/models/2019-07-24/spark_random_forest_classifier.flight_delays.5.0.bin/data/part-00001-b76c5248-8ab2-4f73-b19f-7370b11fe1e7-c000.snappy.parquet to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/spark_random_forest_classifier.flight_delays.5.0.bin/data/part-00001-b76c5248-8ab2-4f73-b19f-7370b11fe1e7-c000.snappy.parquet\n",
      "download: s3://jhw620/models/2019-07-24/spark_random_forest_classifier.flight_delays.5.0.bin/data/part-00001-f6fa9a28-9f69-46c7-86bd-0e8d70a36f70-c000.snappy.parquet to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/spark_random_forest_classifier.flight_delays.5.0.bin/data/part-00001-f6fa9a28-9f69-46c7-86bd-0e8d70a36f70-c000.snappy.parquet\n",
      "download: s3://jhw620/models/2019-07-24/spark_random_forest_classifier.flight_delays.5.0.bin/data/part-00003-b76c5248-8ab2-4f73-b19f-7370b11fe1e7-c000.snappy.parquet to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/spark_random_forest_classifier.flight_delays.5.0.bin/data/part-00003-b76c5248-8ab2-4f73-b19f-7370b11fe1e7-c000.snappy.parquet\n",
      "download: s3://jhw620/models/2019-07-24/spark_random_forest_classifier.flight_delays.5.0.bin/data/part-00000-6319c52d-d45f-4f86-bc33-9e15f7c7e054-c000.snappy.parquet to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/spark_random_forest_classifier.flight_delays.5.0.bin/data/part-00000-6319c52d-d45f-4f86-bc33-9e15f7c7e054-c000.snappy.parquet\n",
      "download: s3://jhw620/models/2019-07-24/spark_random_forest_classifier.flight_delays.5.0.bin/data/part-00003-f6fa9a28-9f69-46c7-86bd-0e8d70a36f70-c000.snappy.parquet to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/spark_random_forest_classifier.flight_delays.5.0.bin/data/part-00003-f6fa9a28-9f69-46c7-86bd-0e8d70a36f70-c000.snappy.parquet\n",
      "download: s3://jhw620/models/2019-07-24/spark_random_forest_classifier.flight_delays.5.0.bin/data/part-00002-6319c52d-d45f-4f86-bc33-9e15f7c7e054-c000.snappy.parquet to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/spark_random_forest_classifier.flight_delays.5.0.bin/data/part-00002-6319c52d-d45f-4f86-bc33-9e15f7c7e054-c000.snappy.parquet\n",
      "download: s3://jhw620/models/2019-07-24/spark_random_forest_classifier.flight_delays.5.0.bin/data/part-00000-f6fa9a28-9f69-46c7-86bd-0e8d70a36f70-c000.snappy.parquet to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/spark_random_forest_classifier.flight_delays.5.0.bin/data/part-00000-f6fa9a28-9f69-46c7-86bd-0e8d70a36f70-c000.snappy.parquet\n",
      "download: s3://jhw620/models/2019-07-24/spark_random_forest_classifier.flight_delays.5.0.bin/data/part-00004-6319c52d-d45f-4f86-bc33-9e15f7c7e054-c000.snappy.parquet to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/spark_random_forest_classifier.flight_delays.5.0.bin/data/part-00004-6319c52d-d45f-4f86-bc33-9e15f7c7e054-c000.snappy.parquet\n",
      "download: s3://jhw620/models/2019-07-24/spark_random_forest_classifier.flight_delays.5.0.bin/data/part-00006-6319c52d-d45f-4f86-bc33-9e15f7c7e054-c000.snappy.parquet to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/spark_random_forest_classifier.flight_delays.5.0.bin/data/part-00006-6319c52d-d45f-4f86-bc33-9e15f7c7e054-c000.snappy.parquet\n",
      "download: s3://jhw620/models/2019-07-24/spark_random_forest_classifier.flight_delays.5.0.bin/data/part-00002-f6fa9a28-9f69-46c7-86bd-0e8d70a36f70-c000.snappy.parquet to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/spark_random_forest_classifier.flight_delays.5.0.bin/data/part-00002-f6fa9a28-9f69-46c7-86bd-0e8d70a36f70-c000.snappy.parquet\n",
      "download: s3://jhw620/models/2019-07-24/spark_random_forest_classifier.flight_delays.5.0.bin/data/part-00005-b76c5248-8ab2-4f73-b19f-7370b11fe1e7-c000.snappy.parquet to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/spark_random_forest_classifier.flight_delays.5.0.bin/data/part-00005-b76c5248-8ab2-4f73-b19f-7370b11fe1e7-c000.snappy.parquet\n",
      "download: s3://jhw620/models/2019-07-24/spark_random_forest_classifier.flight_delays.5.0.bin/data/part-00005-6319c52d-d45f-4f86-bc33-9e15f7c7e054-c000.snappy.parquet to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/spark_random_forest_classifier.flight_delays.5.0.bin/data/part-00005-6319c52d-d45f-4f86-bc33-9e15f7c7e054-c000.snappy.parquet\n",
      "download: s3://jhw620/models/2019-07-24/spark_random_forest_classifier.flight_delays.5.0.bin/data/part-00001-6319c52d-d45f-4f86-bc33-9e15f7c7e054-c000.snappy.parquet to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/spark_random_forest_classifier.flight_delays.5.0.bin/data/part-00001-6319c52d-d45f-4f86-bc33-9e15f7c7e054-c000.snappy.parquet\n",
      "download: s3://jhw620/models/2019-07-24/spark_random_forest_classifier.flight_delays.5.0.bin/data/part-00002-b76c5248-8ab2-4f73-b19f-7370b11fe1e7-c000.snappy.parquet to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/spark_random_forest_classifier.flight_delays.5.0.bin/data/part-00002-b76c5248-8ab2-4f73-b19f-7370b11fe1e7-c000.snappy.parquet\n",
      "download: s3://jhw620/models/2019-07-24/spark_random_forest_classifier.flight_delays.5.0.bin/data/part-00003-6319c52d-d45f-4f86-bc33-9e15f7c7e054-c000.snappy.parquet to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/spark_random_forest_classifier.flight_delays.5.0.bin/data/part-00003-6319c52d-d45f-4f86-bc33-9e15f7c7e054-c000.snappy.parquet\n",
      "download: s3://jhw620/models/2019-07-24/spark_random_forest_classifier.flight_delays.5.0.bin/data/part-00005-f6fa9a28-9f69-46c7-86bd-0e8d70a36f70-c000.snappy.parquet to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/spark_random_forest_classifier.flight_delays.5.0.bin/data/part-00005-f6fa9a28-9f69-46c7-86bd-0e8d70a36f70-c000.snappy.parquet\n",
      "download: s3://jhw620/models/2019-07-24/spark_random_forest_classifier.flight_delays.5.0.bin/data/part-00004-b76c5248-8ab2-4f73-b19f-7370b11fe1e7-c000.snappy.parquet to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/spark_random_forest_classifier.flight_delays.5.0.bin/data/part-00004-b76c5248-8ab2-4f73-b19f-7370b11fe1e7-c000.snappy.parquet\n",
      "download: s3://jhw620/models/2019-07-24/spark_random_forest_classifier.flight_delays.5.0.bin/data/part-00004-f6fa9a28-9f69-46c7-86bd-0e8d70a36f70-c000.snappy.parquet to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/spark_random_forest_classifier.flight_delays.5.0.bin/data/part-00004-f6fa9a28-9f69-46c7-86bd-0e8d70a36f70-c000.snappy.parquet\n",
      "download: s3://jhw620/models/2019-07-24/spark_random_forest_classifier.flight_delays.5.0.bin/data/part-00006-b76c5248-8ab2-4f73-b19f-7370b11fe1e7-c000.snappy.parquet to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/spark_random_forest_classifier.flight_delays.5.0.bin/data/part-00006-b76c5248-8ab2-4f73-b19f-7370b11fe1e7-c000.snappy.parquet\n",
      "download: s3://jhw620/models/2019-07-24/spark_random_forest_classifier.flight_delays.5.0.bin/data/part-00006-f6fa9a28-9f69-46c7-86bd-0e8d70a36f70-c000.snappy.parquet to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/spark_random_forest_classifier.flight_delays.5.0.bin/data/part-00006-f6fa9a28-9f69-46c7-86bd-0e8d70a36f70-c000.snappy.parquet\n",
      "download: s3://jhw620/models/2019-07-24/spark_random_forest_classifier.flight_delays.5.0.bin/data/part-00007-6319c52d-d45f-4f86-bc33-9e15f7c7e054-c000.snappy.parquet to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/spark_random_forest_classifier.flight_delays.5.0.bin/data/part-00007-6319c52d-d45f-4f86-bc33-9e15f7c7e054-c000.snappy.parquet\n",
      "download: s3://jhw620/models/2019-07-24/spark_random_forest_classifier.flight_delays.5.0.bin/data/part-00009-6319c52d-d45f-4f86-bc33-9e15f7c7e054-c000.snappy.parquet to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/spark_random_forest_classifier.flight_delays.5.0.bin/data/part-00009-6319c52d-d45f-4f86-bc33-9e15f7c7e054-c000.snappy.parquet\n",
      "download: s3://jhw620/models/2019-07-24/spark_random_forest_classifier.flight_delays.5.0.bin/data/part-00009-f6fa9a28-9f69-46c7-86bd-0e8d70a36f70-c000.snappy.parquet to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/spark_random_forest_classifier.flight_delays.5.0.bin/data/part-00009-f6fa9a28-9f69-46c7-86bd-0e8d70a36f70-c000.snappy.parquet\n",
      "download: s3://jhw620/models/2019-07-24/spark_random_forest_classifier.flight_delays.5.0.bin/metadata/_SUCCESS to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/spark_random_forest_classifier.flight_delays.5.0.bin/metadata/_SUCCESS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://jhw620/models/2019-07-24/spark_random_forest_classifier.flight_delays.5.0.bin/data/part-00007-b76c5248-8ab2-4f73-b19f-7370b11fe1e7-c000.snappy.parquet to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/spark_random_forest_classifier.flight_delays.5.0.bin/data/part-00007-b76c5248-8ab2-4f73-b19f-7370b11fe1e7-c000.snappy.parquet\n",
      "download: s3://jhw620/models/2019-07-24/spark_random_forest_classifier.flight_delays.5.0.bin/data/part-00007-f6fa9a28-9f69-46c7-86bd-0e8d70a36f70-c000.snappy.parquet to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/spark_random_forest_classifier.flight_delays.5.0.bin/data/part-00007-f6fa9a28-9f69-46c7-86bd-0e8d70a36f70-c000.snappy.parquet\n",
      "download: s3://jhw620/models/2019-07-24/spark_random_forest_classifier.flight_delays.5.0.bin/metadata/part-00000 to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/spark_random_forest_classifier.flight_delays.5.0.bin/metadata/part-00000\n",
      "download: s3://jhw620/models/2019-07-24/spark_random_forest_classifier.flight_delays.5.0.bin/data/part-00008-6319c52d-d45f-4f86-bc33-9e15f7c7e054-c000.snappy.parquet to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/spark_random_forest_classifier.flight_delays.5.0.bin/data/part-00008-6319c52d-d45f-4f86-bc33-9e15f7c7e054-c000.snappy.parquet\n",
      "download: s3://jhw620/models/2019-07-24/spark_random_forest_classifier.flight_delays.5.0.bin/treesMetadata/part-00000-1bf029d3-6ae3-42b1-8adc-d761a49923f5-c000.snappy.parquet to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/spark_random_forest_classifier.flight_delays.5.0.bin/treesMetadata/part-00000-1bf029d3-6ae3-42b1-8adc-d761a49923f5-c000.snappy.parquet\n",
      "download: s3://jhw620/models/2019-07-24/spark_random_forest_classifier.flight_delays.5.0.bin/treesMetadata/_SUCCESS to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/spark_random_forest_classifier.flight_delays.5.0.bin/treesMetadata/_SUCCESS\n",
      "download: s3://jhw620/models/2019-07-24/spark_random_forest_classifier.flight_delays.5.0.bin/treesMetadata/part-00000-47071f84-cb7a-4fbc-8a1c-879bceeccac1-c000.snappy.parquet to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/spark_random_forest_classifier.flight_delays.5.0.bin/treesMetadata/part-00000-47071f84-cb7a-4fbc-8a1c-879bceeccac1-c000.snappy.parquet\n",
      "download: s3://jhw620/models/2019-07-24/spark_random_forest_classifier.flight_delays.5.0.bin/treesMetadata/part-00001-47071f84-cb7a-4fbc-8a1c-879bceeccac1-c000.snappy.parquet to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/spark_random_forest_classifier.flight_delays.5.0.bin/treesMetadata/part-00001-47071f84-cb7a-4fbc-8a1c-879bceeccac1-c000.snappy.parquet\n",
      "download: s3://jhw620/models/2019-07-24/spark_random_forest_classifier.flight_delays.5.0.bin/treesMetadata/part-00001-d1ca02bb-3f18-4776-b61b-5c1494b8c4a1-c000.snappy.parquet to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/spark_random_forest_classifier.flight_delays.5.0.bin/treesMetadata/part-00001-d1ca02bb-3f18-4776-b61b-5c1494b8c4a1-c000.snappy.parquet\n",
      "download: s3://jhw620/models/2019-07-24/spark_random_forest_classifier.flight_delays.5.0.bin/treesMetadata/part-00002-1bf029d3-6ae3-42b1-8adc-d761a49923f5-c000.snappy.parquet to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/spark_random_forest_classifier.flight_delays.5.0.bin/treesMetadata/part-00002-1bf029d3-6ae3-42b1-8adc-d761a49923f5-c000.snappy.parquet\n",
      "download: s3://jhw620/models/2019-07-24/spark_random_forest_classifier.flight_delays.5.0.bin/treesMetadata/part-00000-d1ca02bb-3f18-4776-b61b-5c1494b8c4a1-c000.snappy.parquet to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/spark_random_forest_classifier.flight_delays.5.0.bin/treesMetadata/part-00000-d1ca02bb-3f18-4776-b61b-5c1494b8c4a1-c000.snappy.parquet\n",
      "download: s3://jhw620/models/2019-07-24/spark_random_forest_classifier.flight_delays.5.0.bin/data/part-00008-f6fa9a28-9f69-46c7-86bd-0e8d70a36f70-c000.snappy.parquet to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/spark_random_forest_classifier.flight_delays.5.0.bin/data/part-00008-f6fa9a28-9f69-46c7-86bd-0e8d70a36f70-c000.snappy.parquet\n",
      "download: s3://jhw620/models/2019-07-24/spark_random_forest_classifier.flight_delays.5.0.bin/treesMetadata/part-00003-d1ca02bb-3f18-4776-b61b-5c1494b8c4a1-c000.snappy.parquet to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/spark_random_forest_classifier.flight_delays.5.0.bin/treesMetadata/part-00003-d1ca02bb-3f18-4776-b61b-5c1494b8c4a1-c000.snappy.parquet\n",
      "download: s3://jhw620/models/2019-07-24/spark_random_forest_classifier.flight_delays.5.0.bin/treesMetadata/part-00005-1bf029d3-6ae3-42b1-8adc-d761a49923f5-c000.snappy.parquet to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/spark_random_forest_classifier.flight_delays.5.0.bin/treesMetadata/part-00005-1bf029d3-6ae3-42b1-8adc-d761a49923f5-c000.snappy.parquet\n",
      "download: s3://jhw620/models/2019-07-24/spark_random_forest_classifier.flight_delays.5.0.bin/treesMetadata/part-00002-d1ca02bb-3f18-4776-b61b-5c1494b8c4a1-c000.snappy.parquet to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/spark_random_forest_classifier.flight_delays.5.0.bin/treesMetadata/part-00002-d1ca02bb-3f18-4776-b61b-5c1494b8c4a1-c000.snappy.parquet\n",
      "download: s3://jhw620/models/2019-07-24/spark_random_forest_classifier.flight_delays.5.0.bin/treesMetadata/part-00001-1bf029d3-6ae3-42b1-8adc-d761a49923f5-c000.snappy.parquet to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/spark_random_forest_classifier.flight_delays.5.0.bin/treesMetadata/part-00001-1bf029d3-6ae3-42b1-8adc-d761a49923f5-c000.snappy.parquet\n",
      "download: s3://jhw620/models/2019-07-24/spark_random_forest_classifier.flight_delays.5.0.bin/treesMetadata/part-00004-d1ca02bb-3f18-4776-b61b-5c1494b8c4a1-c000.snappy.parquet to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/spark_random_forest_classifier.flight_delays.5.0.bin/treesMetadata/part-00004-d1ca02bb-3f18-4776-b61b-5c1494b8c4a1-c000.snappy.parquet\n",
      "download: s3://jhw620/models/2019-07-24/spark_random_forest_classifier.flight_delays.5.0.bin/treesMetadata/part-00005-47071f84-cb7a-4fbc-8a1c-879bceeccac1-c000.snappy.parquet to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/spark_random_forest_classifier.flight_delays.5.0.bin/treesMetadata/part-00005-47071f84-cb7a-4fbc-8a1c-879bceeccac1-c000.snappy.parquet\n",
      "download: s3://jhw620/models/2019-07-24/spark_random_forest_classifier.flight_delays.5.0.bin/treesMetadata/part-00003-47071f84-cb7a-4fbc-8a1c-879bceeccac1-c000.snappy.parquet to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/spark_random_forest_classifier.flight_delays.5.0.bin/treesMetadata/part-00003-47071f84-cb7a-4fbc-8a1c-879bceeccac1-c000.snappy.parquet\n",
      "download: s3://jhw620/models/2019-07-24/spark_random_forest_classifier.flight_delays.5.0.bin/treesMetadata/part-00006-47071f84-cb7a-4fbc-8a1c-879bceeccac1-c000.snappy.parquet to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/spark_random_forest_classifier.flight_delays.5.0.bin/treesMetadata/part-00006-47071f84-cb7a-4fbc-8a1c-879bceeccac1-c000.snappy.parquet\n",
      "download: s3://jhw620/models/2019-07-24/spark_random_forest_classifier.flight_delays.5.0.bin/treesMetadata/part-00006-1bf029d3-6ae3-42b1-8adc-d761a49923f5-c000.snappy.parquet to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/spark_random_forest_classifier.flight_delays.5.0.bin/treesMetadata/part-00006-1bf029d3-6ae3-42b1-8adc-d761a49923f5-c000.snappy.parquet\n",
      "download: s3://jhw620/models/2019-07-24/spark_random_forest_classifier.flight_delays.5.0.bin/treesMetadata/part-00005-d1ca02bb-3f18-4776-b61b-5c1494b8c4a1-c000.snappy.parquet to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/spark_random_forest_classifier.flight_delays.5.0.bin/treesMetadata/part-00005-d1ca02bb-3f18-4776-b61b-5c1494b8c4a1-c000.snappy.parquet\n",
      "download: s3://jhw620/models/2019-07-24/spark_random_forest_classifier.flight_delays.5.0.bin/treesMetadata/part-00002-47071f84-cb7a-4fbc-8a1c-879bceeccac1-c000.snappy.parquet to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/spark_random_forest_classifier.flight_delays.5.0.bin/treesMetadata/part-00002-47071f84-cb7a-4fbc-8a1c-879bceeccac1-c000.snappy.parquet\n",
      "download: s3://jhw620/models/2019-07-24/spark_random_forest_classifier.flight_delays.5.0.bin/treesMetadata/part-00004-47071f84-cb7a-4fbc-8a1c-879bceeccac1-c000.snappy.parquet to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/spark_random_forest_classifier.flight_delays.5.0.bin/treesMetadata/part-00004-47071f84-cb7a-4fbc-8a1c-879bceeccac1-c000.snappy.parquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://jhw620/models/2019-07-24/spark_random_forest_classifier.flight_delays.5.0.bin/treesMetadata/part-00004-1bf029d3-6ae3-42b1-8adc-d761a49923f5-c000.snappy.parquet to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/spark_random_forest_classifier.flight_delays.5.0.bin/treesMetadata/part-00004-1bf029d3-6ae3-42b1-8adc-d761a49923f5-c000.snappy.parquet\n",
      "download: s3://jhw620/models/2019-07-24/spark_random_forest_classifier.flight_delays.5.0.bin/treesMetadata/part-00003-1bf029d3-6ae3-42b1-8adc-d761a49923f5-c000.snappy.parquet to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/spark_random_forest_classifier.flight_delays.5.0.bin/treesMetadata/part-00003-1bf029d3-6ae3-42b1-8adc-d761a49923f5-c000.snappy.parquet\n",
      "download: s3://jhw620/models/2019-07-24/spark_random_forest_classifier.flight_delays.5.0.bin/treesMetadata/part-00006-d1ca02bb-3f18-4776-b61b-5c1494b8c4a1-c000.snappy.parquet to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/spark_random_forest_classifier.flight_delays.5.0.bin/treesMetadata/part-00006-d1ca02bb-3f18-4776-b61b-5c1494b8c4a1-c000.snappy.parquet\n",
      "download: s3://jhw620/models/2019-07-24/spark_random_forest_classifier.flight_delays.5.0.bin/treesMetadata/part-00007-d1ca02bb-3f18-4776-b61b-5c1494b8c4a1-c000.snappy.parquet to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/spark_random_forest_classifier.flight_delays.5.0.bin/treesMetadata/part-00007-d1ca02bb-3f18-4776-b61b-5c1494b8c4a1-c000.snappy.parquet\n",
      "download: s3://jhw620/models/2019-07-24/spark_random_forest_classifier.flight_delays.5.0.bin/treesMetadata/part-00007-47071f84-cb7a-4fbc-8a1c-879bceeccac1-c000.snappy.parquet to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/spark_random_forest_classifier.flight_delays.5.0.bin/treesMetadata/part-00007-47071f84-cb7a-4fbc-8a1c-879bceeccac1-c000.snappy.parquet\n",
      "download: s3://jhw620/models/2019-07-24/spark_random_forest_classifier.flight_delays.5.0.bin/treesMetadata/part-00007-1bf029d3-6ae3-42b1-8adc-d761a49923f5-c000.snappy.parquet to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/spark_random_forest_classifier.flight_delays.5.0.bin/treesMetadata/part-00007-1bf029d3-6ae3-42b1-8adc-d761a49923f5-c000.snappy.parquet\n",
      "download: s3://jhw620/models/2019-07-24/string_indexer_model_Carrier.bin/data/_SUCCESS to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/string_indexer_model_Carrier.bin/data/_SUCCESS\n",
      "download: s3://jhw620/models/2019-07-24/spark_random_forest_classifier.flight_delays.5.0.bin/treesMetadata/part-00009-47071f84-cb7a-4fbc-8a1c-879bceeccac1-c000.snappy.parquet to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/spark_random_forest_classifier.flight_delays.5.0.bin/treesMetadata/part-00009-47071f84-cb7a-4fbc-8a1c-879bceeccac1-c000.snappy.parquet\n",
      "download: s3://jhw620/models/2019-07-24/spark_random_forest_classifier.flight_delays.5.0.bin/treesMetadata/part-00008-d1ca02bb-3f18-4776-b61b-5c1494b8c4a1-c000.snappy.parquet to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/spark_random_forest_classifier.flight_delays.5.0.bin/treesMetadata/part-00008-d1ca02bb-3f18-4776-b61b-5c1494b8c4a1-c000.snappy.parquet\n",
      "download: s3://jhw620/models/2019-07-24/string_indexer_model_Carrier.bin/metadata/_SUCCESS to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/string_indexer_model_Carrier.bin/metadata/_SUCCESS\n",
      "download: s3://jhw620/models/2019-07-24/spark_random_forest_classifier.flight_delays.5.0.bin/treesMetadata/part-00008-47071f84-cb7a-4fbc-8a1c-879bceeccac1-c000.snappy.parquet to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/spark_random_forest_classifier.flight_delays.5.0.bin/treesMetadata/part-00008-47071f84-cb7a-4fbc-8a1c-879bceeccac1-c000.snappy.parquet\n",
      "download: s3://jhw620/models/2019-07-24/spark_random_forest_classifier.flight_delays.5.0.bin/treesMetadata/part-00009-d1ca02bb-3f18-4776-b61b-5c1494b8c4a1-c000.snappy.parquet to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/spark_random_forest_classifier.flight_delays.5.0.bin/treesMetadata/part-00009-d1ca02bb-3f18-4776-b61b-5c1494b8c4a1-c000.snappy.parquet\n",
      "download: s3://jhw620/models/2019-07-24/string_indexer_model_Dest.bin/data/_SUCCESS to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/string_indexer_model_Dest.bin/data/_SUCCESS\n",
      "download: s3://jhw620/models/2019-07-24/string_indexer_model_Carrier.bin/data/part-00000-9d236781-7347-472c-afcb-a013a982d3f2-c000.snappy.parquet to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/string_indexer_model_Carrier.bin/data/part-00000-9d236781-7347-472c-afcb-a013a982d3f2-c000.snappy.parquet\n",
      "download: s3://jhw620/models/2019-07-24/string_indexer_model_Carrier.bin/data/part-00000-e0c0b22d-e652-45a8-af9a-efc98d767d0e-c000.snappy.parquet to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/string_indexer_model_Carrier.bin/data/part-00000-e0c0b22d-e652-45a8-af9a-efc98d767d0e-c000.snappy.parquet\n",
      "download: s3://jhw620/models/2019-07-24/string_indexer_model_Dest.bin/metadata/_SUCCESS to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/string_indexer_model_Dest.bin/metadata/_SUCCESS\n",
      "download: s3://jhw620/models/2019-07-24/string_indexer_model_Carrier.bin/data/part-00000-e4c1c2ab-56d5-4593-a4fd-79ec8fa01987-c000.snappy.parquet to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/string_indexer_model_Carrier.bin/data/part-00000-e4c1c2ab-56d5-4593-a4fd-79ec8fa01987-c000.snappy.parquet\n",
      "download: s3://jhw620/models/2019-07-24/string_indexer_model_Carrier.bin/metadata/part-00000 to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/string_indexer_model_Carrier.bin/metadata/part-00000\n",
      "download: s3://jhw620/models/2019-07-24/string_indexer_model_Dest.bin/metadata/part-00000 to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/string_indexer_model_Dest.bin/metadata/part-00000\n",
      "download: s3://jhw620/models/2019-07-24/string_indexer_model_Dest.bin/data/part-00000-6ed1745a-0ae0-44b7-8791-aa07f00df19f-c000.snappy.parquet to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/string_indexer_model_Dest.bin/data/part-00000-6ed1745a-0ae0-44b7-8791-aa07f00df19f-c000.snappy.parquet\n",
      "download: s3://jhw620/models/2019-07-24/string_indexer_model_Origin.bin/data/_SUCCESS to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/string_indexer_model_Origin.bin/data/_SUCCESS\n",
      "download: s3://jhw620/models/2019-07-24/string_indexer_model_Origin.bin/data/part-00000-4cd24001-cb16-4d93-bbbc-71cf858cfc75-c000.snappy.parquet to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/string_indexer_model_Origin.bin/data/part-00000-4cd24001-cb16-4d93-bbbc-71cf858cfc75-c000.snappy.parquet\n",
      "download: s3://jhw620/models/2019-07-24/string_indexer_model_Dest.bin/data/part-00000-e6d2af51-d75d-48f7-8bd6-347e52cdb6f0-c000.snappy.parquet to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/string_indexer_model_Dest.bin/data/part-00000-e6d2af51-d75d-48f7-8bd6-347e52cdb6f0-c000.snappy.parquet\n",
      "download: s3://jhw620/models/2019-07-24/string_indexer_model_Origin.bin/metadata/_SUCCESS to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/string_indexer_model_Origin.bin/metadata/_SUCCESS\n",
      "download: s3://jhw620/models/2019-07-24/string_indexer_model_Route.bin/data/_SUCCESS to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/string_indexer_model_Route.bin/data/_SUCCESS\n",
      "download: s3://jhw620/models/2019-07-24/string_indexer_model_Origin.bin/data/part-00000-50eb857f-0359-4dde-ab1e-2cad6205e1d5-c000.snappy.parquet to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/string_indexer_model_Origin.bin/data/part-00000-50eb857f-0359-4dde-ab1e-2cad6205e1d5-c000.snappy.parquet\n",
      "download: s3://jhw620/models/2019-07-24/string_indexer_model_Origin.bin/metadata/part-00000 to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/string_indexer_model_Origin.bin/metadata/part-00000\n",
      "download: s3://jhw620/models/2019-07-24/string_indexer_model_Route.bin/data/part-00000-3254cbcb-1e44-4606-83e8-82f593408c23-c000.snappy.parquet to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/string_indexer_model_Route.bin/data/part-00000-3254cbcb-1e44-4606-83e8-82f593408c23-c000.snappy.parquet\n",
      "download: s3://jhw620/models/2019-07-24/string_indexer_model_Route.bin/metadata/_SUCCESS to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/string_indexer_model_Route.bin/metadata/_SUCCESS\n",
      "download: s3://jhw620/models/2019-07-24/string_indexer_model_Route.bin/data/part-00000-4cc851da-035d-4c42-ae8c-2114becdd4d9-c000.snappy.parquet to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/string_indexer_model_Route.bin/data/part-00000-4cc851da-035d-4c42-ae8c-2114becdd4d9-c000.snappy.parquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://jhw620/models/2019-07-24/string_indexer_model_Dest.bin/data/part-00000-9e9193a3-65c7-4ef6-84dc-dfe807cfdec0-c000.snappy.parquet to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/string_indexer_model_Dest.bin/data/part-00000-9e9193a3-65c7-4ef6-84dc-dfe807cfdec0-c000.snappy.parquet\n",
      "download: s3://jhw620/models/2019-07-24/string_indexer_model_Origin.bin/data/part-00000-92958a3e-ba66-4662-8730-ffead430c6c1-c000.snappy.parquet to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/string_indexer_model_Origin.bin/data/part-00000-92958a3e-ba66-4662-8730-ffead430c6c1-c000.snappy.parquet\n",
      "download: s3://jhw620/models/2019-07-24/string_indexer_model_Route.bin/data/part-00000-2483b63f-d285-42df-82f7-38004b736c4e-c000.snappy.parquet to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/string_indexer_model_Route.bin/data/part-00000-2483b63f-d285-42df-82f7-38004b736c4e-c000.snappy.parquet\n",
      "download: s3://jhw620/models/2019-07-24/string_indexer_model_Route.bin/metadata/part-00000 to ../../home/ubuntu/Project/03_PredictionModel_RealTime_Processing/models1/string_indexer_model_Route.bin/metadata/part-00000\n",
      "[2022-07-14 03:37:55,135] {bash_operator.py:131} INFO - Command exited with return code 0\n"
     ]
    }
   ],
   "source": [
    "!airflow test DAG_model_training trained_model_s3_to_server 2019-08-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.6/site-packages/airflow/configuration.py:590: DeprecationWarning: You have two airflow.cfg files: /home/ubuntu/airflow/airflow.cfg and /home/ubuntu/project1/airflow/airflow.cfg. Airflow used to look at ~/airflow/airflow.cfg, even when AIRFLOW_HOME was set to a different value. Airflow will now only read /home/ubuntu/project1/airflow/airflow.cfg, and you should remove the other file\n",
      "  category=DeprecationWarning,\n",
      "[2019-08-08 09:53:18,301] {__init__.py:51} INFO - Using executor SequentialExecutor\n",
      "[2019-08-08 09:53:18,516] {__init__.py:305} INFO - Filling up the DagBag from /home/ubuntu/project1/airflow/dags\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:387: DeprecationWarning: Importing 'SSHHook' directly from 'airflow.contrib.hooks' has been deprecated. Please import from 'airflow.contrib.hooks.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.\n",
      "  DeprecationWarning)\n",
      "[2019-08-08 09:53:18,571] {__init__.py:1139} INFO - Dependencies all met for <TaskInstance: DAG_model_training.emr_cluster_close 2019-08-01T00:00:00+00:00 [None]>\n",
      "[2019-08-08 09:53:18,575] {__init__.py:1139} INFO - Dependencies all met for <TaskInstance: DAG_model_training.emr_cluster_close 2019-08-01T00:00:00+00:00 [None]>\n",
      "[2019-08-08 09:53:18,575] {__init__.py:1353} INFO - \n",
      "--------------------------------------------------------------------------------\n",
      "[2019-08-08 09:53:18,575] {__init__.py:1354} INFO - Starting attempt 1 of 1\n",
      "[2019-08-08 09:53:18,575] {__init__.py:1355} INFO - \n",
      "--------------------------------------------------------------------------------\n",
      "[2019-08-08 09:53:18,575] {__init__.py:1374} INFO - Executing <Task(BashOperator): emr_cluster_close> on 2019-08-01T00:00:00+00:00\n",
      "[2019-08-08 09:53:18,595] {bash_operator.py:81} INFO - Tmp dir root location: \n",
      " /tmp\n",
      "[2019-08-08 09:53:18,595] {bash_operator.py:90} INFO - Exporting the following env vars:\n",
      "AIRFLOW_CTX_DAG_ID=DAG_model_training\n",
      "AIRFLOW_CTX_TASK_ID=emr_cluster_close\n",
      "AIRFLOW_CTX_EXECUTION_DATE=2019-08-01T00:00:00+00:00\n",
      "[2019-08-08 09:53:18,596] {bash_operator.py:104} INFO - Temporary script location: /tmp/airflowtmpqhpfh4ka/emr_cluster_closertlicypy\n",
      "[2019-08-08 09:53:18,596] {bash_operator.py:114} INFO - Running command: aws emr terminate-clusters --cluster-ids j-2N9KVTJGNP9B0\n",
      "[2019-08-08 09:53:18,600] {bash_operator.py:123} INFO - Output:\n",
      "[2019-08-08 09:53:19,448] {bash_operator.py:131} INFO - Command exited with return code 0\n"
     ]
    }
   ],
   "source": [
    "!airflow test DAG_model_training emr_cluster_close 2019-08-01"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
