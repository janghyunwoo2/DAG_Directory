{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType, IntegerType, FloatType, DoubleType, DateType, TimestampType\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark\n",
    "import pyspark.sql\n",
    "from datetime import datetime,timedelta\n",
    "\n",
    "import os, sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_home = '..'\n",
    "\n",
    "# 스파크 객체 생성\n",
    "conf = pyspark.SparkConf().setAll([('spark.driver.memory', '2g')])\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = pyspark.sql.SparkSession(sc).builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Year='2015', Quarter='2', Month='5', DayofMonth='9', DayOfWeek='6', FlightDate='2015-05-09', Carrier='WN', TailNum='N8309C', FlightNum='64', Origin='PIT', OriginCityName='Pittsburgh, PA', OriginState='PA', Dest='FLL', DestCityName='Fort Lauderdale, FL', DestState='FL', DepTime='0559', DepDelay=9.0, DepDelayMinutes=9, TaxiOut=11.0, TaxiIn=4.0, WheelsOff='0610', WheelsOn='0816', ArrTime='0820', ArrDelay=-15.0, ArrDelayMinutes=0.0, Cancelled=0, Diverted=0, ActualElapsedTime=141.0, AirTime=126.0, Flights=1, Distance=994.0, CRSDepTime='0550', CRSArrTime='0835')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#데이터 가져오기\n",
    "refined_data = spark.read.parquet(\"{}/data/Refined_Data.parquet\".format(project_home))\n",
    "\n",
    "refined_data = refined_data.sample(False, 0.1)\n",
    "\n",
    "# 테이블 등록\n",
    "refined_data.registerTempTable(\"Refined_Data\")\n",
    "refined_data.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+---------+----------+---------+-------+------+----+--------+--------+--------+----------+----------+\n",
      "|FlightNum|FlightDate|DayOfWeek|DayOfMonth|DayOfYear|Carrier|Origin|Dest|Distance|DepDelay|ArrDelay|CRSDepTime|CRSArrTime|\n",
      "+---------+----------+---------+----------+---------+-------+------+----+--------+--------+--------+----------+----------+\n",
      "|       64|2015-05-09|        6|         9|      5-9|     WN|   PIT| FLL|   994.0|     9.0|   -15.0|      0550|      0835|\n",
      "|     2188|2015-05-09|        6|         9|      5-9|     WN|   SMF| DEN|   909.0|     4.0|     6.0|      0640|      1005|\n",
      "+---------+----------+---------+----------+---------+-------+------+----+--------+--------+--------+----------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 모델 훈련에 쓰일 데이터 생성\n",
    "training_data = spark.sql(\"\"\"\n",
    "SELECT\n",
    "  FlightNum,\n",
    "  FlightDate,\n",
    "  DayOfWeek,\n",
    "  DayofMonth AS DayOfMonth,\n",
    "  CONCAT(Month, '-',  DayofMonth) AS DayOfYear,\n",
    "  Carrier,\n",
    "  Origin,\n",
    "  Dest,\n",
    "  Distance,\n",
    "  DepDelay,\n",
    "  ArrDelay,\n",
    "  CRSDepTime,\n",
    "  CRSArrTime\n",
    "FROM Refined_Data\n",
    "\"\"\")\n",
    "training_data.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.6/site-packages/pyspark/sql/session.py:366: UserWarning: Using RDD of dict to inferSchema is deprecated. Use pyspark.sql.Row instead\n",
      "  warnings.warn(\"Using RDD of dict to inferSchema is deprecated. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-------------------+-------+----------+---------+---------+--------+----+--------+-------------------+---------+------+\n",
      "|ArrDelay|         CRSArrTime|         CRSDepTime|Carrier|DayOfMonth|DayOfWeek|DayOfYear|DepDelay|Dest|Distance|         FlightDate|FlightNum|Origin|\n",
      "+--------+-------------------+-------------------+-------+----------+---------+---------+--------+----+--------+-------------------+---------+------+\n",
      "|   -15.0|2015-05-09 08:35:00|2015-05-09 05:50:00|     WN|         9|        6|      129|     9.0| FLL|   994.0|2015-05-09 00:00:00|       64|   PIT|\n",
      "|     6.0|2015-05-09 10:05:00|2015-05-09 06:40:00|     WN|         9|        6|      129|     4.0| DEN|   909.0|2015-05-09 00:00:00|     2188|   SMF|\n",
      "+--------+-------------------+-------------------+-------+----------+---------+---------+--------+----+--------+-------------------+---------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(\"../lib\")\n",
    "import date_util\n",
    "\n",
    "# SparkContext에 모듈 등록\n",
    "sc.addPyFile('../lib/date_util.py')\n",
    "\n",
    "# date_util.alter_feature_datetimes : 날짜 파싱\n",
    "training_data=training_data.rdd.map(date_util.alter_feature_datetimes).toDF()\n",
    "training_data.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-------+\n",
      "|Origin|Dest|  Route|\n",
      "+------+----+-------+\n",
      "|   PIT| FLL|PIT-FLL|\n",
      "|   SMF| DEN|SMF-DEN|\n",
      "|   MCI| LAS|MCI-LAS|\n",
      "|   CMH| TPA|CMH-TPA|\n",
      "|   DTW| DEN|DTW-DEN|\n",
      "+------+----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 항공편 번호를 운항 경로로 대체하기\n",
    "from pyspark.sql.functions import lit, concat\n",
    "\n",
    "features_with_route = training_data.withColumn(\n",
    "  'Route',\n",
    "  concat(\n",
    "    training_data.Origin,\n",
    "    lit('-'),\n",
    "    training_data.Dest\n",
    "  )\n",
    ")\n",
    "features_with_route.select(\"Origin\", \"Dest\", \"Route\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../models/arrival_bucketizer_2.0.bin\n",
      "+--------+--------------+\n",
      "|ArrDelay|ArrDelayBucket|\n",
      "+--------+--------------+\n",
      "|   -15.0|           1.0|\n",
      "|     6.0|           2.0|\n",
      "|    -1.0|           1.0|\n",
      "|   -20.0|           0.0|\n",
      "|     4.0|           2.0|\n",
      "+--------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### Bucketizer:목표변수 분류 클래스 나누기 ####\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "\n",
    "\n",
    "splits = [-float(\"inf\"), -15.0, 0, 30.0, float(\"inf\")]\n",
    "bucketizer = Bucketizer(\n",
    "  splits=splits,\n",
    "  inputCol=\"ArrDelay\", #원시 목표변수\n",
    "  outputCol=\"ArrDelayBucket\" #클래스 나뉜 목표변수\n",
    ")\n",
    "\n",
    "# Bucketizer 객체 저장\n",
    "bucketizer_path = \"./models/arrival_bucketizer_2.0.bin\"\n",
    "print(bucketizer_path)\n",
    "bucketizer.write().overwrite().save(bucketizer_path)\n",
    "\n",
    "# Bucketizer로 데이터 변환\n",
    "ml_bucketized_features = bucketizer.transform(features_with_route)\n",
    "ml_bucketized_features.select(\"ArrDelay\", \"ArrDelayBucket\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../models/string_indexer_model_Carrier.bin\n",
      "../models/string_indexer_model_Origin.bin\n",
      "../models/string_indexer_model_Dest.bin\n",
      "../models/string_indexer_model_Route.bin\n"
     ]
    }
   ],
   "source": [
    "#### StringIndexer : String 타입의 범주 값을 해당 값의 정수 번호로 변환 ####\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "for column in [\"Carrier\", \"Origin\", \"Dest\", \"Route\"]:\n",
    "    string_indexer = StringIndexer(\n",
    "    inputCol=column,\n",
    "    outputCol=column + \"_index\"\n",
    "    )\n",
    "    string_indexer_model = string_indexer.fit(ml_bucketized_features)\n",
    "    ml_bucketized_features = string_indexer_model.transform(ml_bucketized_features)\n",
    "\n",
    "    ml_bucketized_features = ml_bucketized_features.drop(column)\n",
    "\n",
    "    # StringIndexer 객체 저장\n",
    "    string_indexer_output_path = \"./models/string_indexer_model_{}.bin\".format(\n",
    "      column\n",
    "    )\n",
    "    print(string_indexer_output_path)\n",
    "    string_indexer_model.write().overwrite().save(string_indexer_output_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../models/numeric_vector_assembler.bin\n",
      "+--------+-------------------+-------------------+----------+---------+---------+--------+--------+-------------------+---------+--------------+--------------------+\n",
      "|ArrDelay|         CRSArrTime|         CRSDepTime|DayOfMonth|DayOfWeek|DayOfYear|DepDelay|Distance|         FlightDate|FlightNum|ArrDelayBucket|        Features_vec|\n",
      "+--------+-------------------+-------------------+----------+---------+---------+--------+--------+-------------------+---------+--------------+--------------------+\n",
      "|   -15.0|2015-05-09 08:35:00|2015-05-09 05:50:00|         9|        6|      129|     9.0|   994.0|2015-05-09 00:00:00|       64|           1.0|[9.0,994.0,9.0,6....|\n",
      "|     6.0|2015-05-09 10:05:00|2015-05-09 06:40:00|         9|        6|      129|     4.0|   909.0|2015-05-09 00:00:00|     2188|           2.0|[4.0,909.0,9.0,6....|\n",
      "+--------+-------------------+-------------------+----------+---------+---------+--------+--------+-------------------+---------+--------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### VectorAssembler: 데이터를 벡터화 하기 ####\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "numeric_columns = [\"DepDelay\", \"Distance\",\n",
    "    \"DayOfMonth\", \"DayOfWeek\",\n",
    "    \"DayOfYear\"]\n",
    "index_columns = [\"Carrier_index\", \"Origin_index\",\n",
    "                   \"Dest_index\", \"Route_index\"]\n",
    "vector_assembler = VectorAssembler(\n",
    "  inputCols=numeric_columns + index_columns,\n",
    "  outputCol=\"Features_vec\"\n",
    ")\n",
    "final_vectorized_features = vector_assembler.transform(ml_bucketized_features)\n",
    "\n",
    "# VectorAssembler 객체 저장\n",
    "vector_assembler_path = \"./models/numeric_vector_assembler.bin\"\n",
    "print(vector_assembler_path)\n",
    "vector_assembler.write().overwrite().save(vector_assembler_path)\n",
    "\n",
    "# 필요없는 컬럼 제거\n",
    "for column in index_columns:\n",
    "    final_vectorized_features = final_vectorized_features.drop(column)\n",
    "final_vectorized_features.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련/검증 데이터 나누기\n",
    "training_data, test_data = final_vectorized_features.randomSplit([0.8, 0.2])\n",
    "\n",
    "# 모델 : 랜덤포레스트\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(\n",
    "  featuresCol=\"Features_vec\",\n",
    "  labelCol=\"ArrDelayBucket\",\n",
    "  maxBins=4657,\n",
    "  maxMemoryInMB=1024,\n",
    "  numTrees = 10,\n",
    "  maxDepth = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../models/spark_random_forest_classifier.flight_delays.5.0.bin\n"
     ]
    }
   ],
   "source": [
    "# 훈련시작\n",
    "model = rfc.fit(training_data)\n",
    "\n",
    "# 모델 객체 저장\n",
    "model_output_path = \"./models/spark_random_forest_classifier.flight_delays.5.0.bin\"\n",
    "print(model_output_path)\n",
    "model.write().overwrite().save(model_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.5936564878817044\n"
     ]
    }
   ],
   "source": [
    "# 검증하기(ArrDelayBucket 가 목표변수)\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"ArrDelayBucket\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy = {}\".format(accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
